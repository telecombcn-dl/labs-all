{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JxrgpKtTNK9r"
            },
            "source": [
                "# Imagenet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-5fxOK1gkTJ6"
            },
            "source": [
                "Notebook created in PyTorch by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the UPC School (2020), and updated by [Pol Caselles](https://www.linkedin.com/in/pcaselles/) (2022) and [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) (2022)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ubMG1ZMcTmcM"
            },
            "source": [
                "For this and many of the following labs we will use [Torchvision](https://pytorch.org/vision/stable/index.html), a library with computer vision datasets, pretrained models and useful methods made to work with PyTorch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import ast\n",
                "import shutil\n",
                "import urllib\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "import torch.nn.functional as F\n",
                "import torchvision.models as models\n",
                "import torchvision.datasets as datasets\n",
                "import torchvision.transforms as transforms\n",
                "from torchvision.transforms.functional import to_pil_image\n",
                "\n",
                "from typing import Tuple, List\n",
                "\n",
                "from matplotlib.pyplot import imshow\n",
                "torch.set_grad_enabled(True)\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "KzKHk7TcXfEm"
            },
            "source": [
                "To ensure reproducibility of the experiments, we can set the seed to a fixed number."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 123\n",
                "np.random.seed(seed)\n",
                "_ = torch.manual_seed(seed)\n",
                "_ = torch.cuda.manual_seed(seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we select to work on GPU\n",
                "\n",
                "if not torch.cuda.is_available():\n",
                "       raise RuntimeError(\"You should enable GPU runtime!!\")\n",
                "device = torch.device(\"cuda\")\n",
                "\n",
                "# whenever we send something to the selected device (X.to(device)) we already use\n",
                "# either CPU or CUDA (GPU). Importantly...\n",
                "# The .to() operation is in-place for nn.Module's, so network.to(device) suffices\n",
                "# The .to() operation is NOT in.place for tensors, so we must assign the result\n",
                "# to some tensor, like: X = X.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "wuSbcGObNFVw"
            },
            "source": [
                "# Training on Imagenet"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DC-eohx4NTaV"
            },
            "source": [
                "Since Imagenet is a really big dataset, we cannot train a model on it (it would take too much time in a single GPU). Anyway, we can take a look at how this training is done in PyTorch. We will go through a very similar version to the official code that was used to train the models from [Torchvision](https://pytorch.org/docs/stable/torchvision/index.html). The code of this notebook can also serve as a template for training your own models."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NUu1aNPES8EG"
            },
            "source": [
                "## Utils"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CzK4BudGRT3I"
            },
            "source": [
                "First, we will define some useful functions. Make sure you understand what they do and how they work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def adjust_learning_rate(\n",
                "        optimizer: torch.optim, \n",
                "        epoch: int, \n",
                "        original_lr: float\n",
                "        ) -> None:\n",
                "    \n",
                "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
                "    lr = original_lr * (0.1 ** (epoch // 30))\n",
                "    # For some models, different parameters are in different groups with different lr\n",
                "    for param_group in optimizer.param_groups:\n",
                "        param_group['lr'] = lr\n",
                "\n",
                "\n",
                "def accuracy(\n",
                "        output: torch.tensor, \n",
                "        target: torch.tensor, \n",
                "        topk: tuple =(1,)\n",
                "        ) -> 'list[torch.tensor]':\n",
                "    \n",
                "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
                "    with torch.no_grad():\n",
                "        maxk = max(topk)\n",
                "        batch_size = target.size(0)\n",
                "\n",
                "        _, pred = output.topk(maxk, 1, True, True)\n",
                "        pred = pred.t()\n",
                "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
                "\n",
                "        res = []\n",
                "        for k in topk:\n",
                "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
                "            res.append(correct_k.mul_(100.0 / batch_size))\n",
                "        return res  # res is a list of all the top k accuracies\n",
                "\n",
                "def save_checkpoint(\n",
                "        state: 'dict', \n",
                "        is_best: bool, \n",
                "        filename: str = 'checkpoint.pth.tar'\n",
                "        ) -> None:\n",
                "    \n",
                "    torch.save(state, filename)\n",
                "    \n",
                "    # save an extra copy if it is the best model yet\n",
                "    if is_best:\n",
                "        shutil.copyfile(filename, 'model_best.pth.tar')  "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oLJhM-nsS-YK"
            },
            "source": [
                "## Model and optimizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AXsmFFRZTD56"
            },
            "source": [
                "Here we will define the model. In our case, we will just take a predifined architecture from `torchvision.models`. You can take a look at the available models in: https://pytorch.org/vision/stable/models.html#classification"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Exo8FFS8UnZm"
            },
            "source": [
                "### Exercise 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RfWnvmgXUxwp"
            },
            "source": [
                "Set up the model and optimizer. Take a ResNet34 from `torchvision.models` with 100 classes (we will not train on the full ImageNet) and make sure it is on GPU. Note that you can set the number of classes with the parameter [`num_classes`](https://github.com/pytorch/vision/blob/0dceac025615a1c2df6ec1675d8f9d7757432a49/torchvision/models/resnet.py#L171). Then, declare an Adam optimizer with learning rate 0.001. You should also define the loss function for classification. The input of the loss will be the raw, unnormalized scores for each class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: take the ResNet34 model\n",
                "model = ...\n",
                "\n",
                "# TODO: declare Adam optimizer\n",
                "optimizer = ...\n",
                "# TODO: define the loss function\n",
                "criterion = ...\n",
                "\n",
                "try:\n",
                "    assert isinstance(model, models.ResNet)\n",
                "except AssertionError:\n",
                "    raise Exception(\"Did you get an instance of a resnet34?\")\n",
                "\n",
                "try:\n",
                "    assert model.fc.weight.shape[0] == 100\n",
                "except AssertionError:\n",
                "    raise Exception(\"Did you set the number of classes to 100?\")\n",
                "\n",
                "try:\n",
                "    assert next(model.parameters()).is_cuda\n",
                "except AssertionError:\n",
                "    pass\n",
                "    # raise Exception(\"Did you forget to move the model to GPU?\")\n",
                "\n",
                "print(\"Well done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rIK-9fC1WsOc"
            },
            "source": [
                "## Data loading"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "42_xbERPXGP0"
            },
            "source": [
                "Now we will prepare the dataset. A very important part of training on ImageNet is the data augmentation. The `torchvision.transforms` package has the transformations we need: https://pytorch.org/vision/stable/transforms.html \n",
                "\n",
                "More specifically, for training, we will:\n",
                "* Do a RandomResizedCrop of size 224. This means, randomly resize the Image and then crop a square of size 224.\n",
                "* Randomly flip the image horizontally.\n",
                "* Converting our image to a torch tensor \n",
                "* Normalizing the values of the pixels with Imagenet mean and standard deviation for each channel.\n",
                "\n",
                "And for validation we will:\n",
                "* Resize our image to size 256 (this means, the smalles side will be 256)\n",
                "* Cropping the image at the center with size 224\n",
                "* Converting our image to a torch tensor \n",
                "* Normalizing the values of the pixels with Imagenet mean and standard deviation for each channel.\n",
                "\n",
                "The ImageNet mean and std values are: `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "72dg-57lZzCp"
            },
            "source": [
                "### Exercise 2\n",
                "Complete the code of the data augmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: define the composed transformation for training\n",
                "train_transforms = transforms.Compose([\n",
                "    ...,\n",
                "    ...,\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(...),\n",
                "])\n",
                "\n",
                "# TODO: define the composed transformation for validation\n",
                "val_transforms = transforms.Compose([\n",
                "    ...,\n",
                "    ...,\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(...) ,\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8ir3Qczna22-"
            },
            "source": [
                "Now we can download the data and declare our dataloaders. We will use CIFAR100 to mimic ImageNet, since ImageNet has over 14 million images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = datasets.CIFAR100(\n",
                "    root='data', \n",
                "    train=True, \n",
                "    download=True, \n",
                "    transform=train_transforms,\n",
                ")\n",
                "\n",
                "val_dataset = datasets.CIFAR100(\n",
                "    root='data', \n",
                "    train=False, \n",
                "    download=True, \n",
                "    transform=val_transforms,\n",
                ")\n",
                "\n",
                "train_loader = torch.utils.data.DataLoader(\n",
                "    train_dataset, \n",
                "    batch_size=64, \n",
                "    shuffle=True,\n",
                "    num_workers=2, \n",
                "    pin_memory=True,\n",
                ")\n",
                "\n",
                "val_loader = torch.utils.data.DataLoader(\n",
                "    val_dataset,\n",
                "    batch_size=64, \n",
                "    shuffle=False, \n",
                "    num_workers=2,\n",
                ")  # There is no need to shuffle data in validation\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Jv9ZGMiVZkJZ"
            },
            "source": [
                "## Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "N1xJ-uePZqfM"
            },
            "source": [
                "First we will define our train and validate functions. Once they are done, we can do the training loop."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5RDmQ86odW85"
            },
            "source": [
                "### Exercise 3\n",
                "Complete the necessary code for the train and validate functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(\n",
                "        train_loader: torch.utils.data.DataLoader, \n",
                "        model: torch.nn.Module, \n",
                "        criterion: torch.nn, \n",
                "        optimizer: torch.optim, \n",
                "        device: torch.device\n",
                "        ) -> None:\n",
                "\n",
                "    # TODO: switch to train mode\n",
                "    ...\n",
                "\n",
                "    for i, (images, target) in enumerate(train_loader):\n",
                "        \n",
                "        # Set network gradients to 0.\n",
                "        # TODO:  reset gradients\n",
                "        optimizer.\n",
                "\n",
                "        # move images to gpu\n",
                "        images = images.to(device)\n",
                "        target = target.to(device)\n",
                "\n",
                "        # Forward batch of images through the network\n",
                "        # TODO: compute output\n",
                "        output = ...\n",
                "\n",
                "        # TODO: loss\n",
                "        loss = ...\n",
                "\n",
                "        # TODO: compute gradient and do optimization step\n",
                "        loss.\n",
                "        optimizer.\n",
                "        \n",
                "        # measure accuracy\n",
                "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
                "\n",
                "        if (i+1) % 50 == 0:\n",
                "            print(f'TRAIN [{i+1}/{len(train_loader)}] Acc@1 {acc1.item():.3f} Acc@5 {acc5.item():.3f} Loss {loss.item():.3f}')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate(\n",
                "        val_loader: torch.utils.data.DataLoader, \n",
                "        model: torch.nn.Module, \n",
                "        criterion: torch.nn, \n",
                "        device: torch.device\n",
                "        ) -> None:\n",
                "\n",
                "    # TODO: switch to evaluate mode\n",
                "    ...\n",
                "\n",
                "    # We will save the values of the accuracies in this list to return the mean of the whole dataset at the end\n",
                "    top1_scores = []  \n",
                "\n",
                "    with torch.no_grad():  # We do not need to compute gradients\n",
                "        for i, (images, target) in enumerate(val_loader):\n",
                "            \n",
                "            # move images to gpu\n",
                "            images = images.to(device)\n",
                "            target = target.to(device)\n",
                "\n",
                "            # Forward batch of images through the network\n",
                "            # TODO: compute output\n",
                "            output = ...\n",
                "\n",
                "            # TODO: loss\n",
                "            loss = ...\n",
                "\n",
                "            # measure accuracy\n",
                "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
                "            top1_scores.append(acc1.item()*len(target))\n",
                "\n",
                "            if (i+1) % 50 == 0:\n",
                "                print(f'VAL [{i+1}/{len(val_loader)}] Acc@1 {acc1.item():.3f} Acc@5 {acc5.item():.3f} Loss {loss.item():.3f}')\n",
                "    \n",
                "    return sum(top1_scores)/len(val_loader.dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e_r3PZ6hfPPH"
            },
            "source": [
                "Finally, we are ready to train. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mjJAqW-CgjE9"
            },
            "source": [
                "### Exercise 4\n",
                "Complete the training loop using the train and validate functions. You do not need to finish the training (even on CIFAR100 it can take hours)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "epochs = 1\n",
                "best_acc1 = 0.\n",
                "for epoch in range(epochs):\n",
                "    adjust_learning_rate(optimizer, epoch, 0.001)\n",
                "\n",
                "    # TODO: train for one epoch\n",
                "    train(...)\n",
                "\n",
                "    # TODO: evaluate on validation set\n",
                "    acc1 = validate(...)\n",
                "\n",
                "    # remember best acc@1 and save checkpoint\n",
                "    is_best = acc1 > best_acc1\n",
                "    best_acc1 = max(acc1, best_acc1)\n",
                "\n",
                "    save_checkpoint({\n",
                "        'epoch': epoch + 1,\n",
                "        'arch': \"resnet34\",\n",
                "        'state_dict': model.state_dict(),\n",
                "        'best_acc1': best_acc1,\n",
                "        'optimizer' : optimizer.state_dict(),\n",
                "    }, is_best)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "al_cBVIkhiLQ"
            },
            "source": [
                "Good job! Now you know how ImageNet models are trained. Now you will learn how to use the models that are already pretrained in `torchvision.models`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TE2AORG9qk7F"
            },
            "source": [
                "# Pretrained imagenet models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Disable gradient computation for this second half of the notebook\n",
                "_ = torch.set_grad_enabled(False);"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2SVCtmKgqwzr"
            },
            "source": [
                "In Torchvision there are many models pretrained with Imagenet. You can see the lists [here](https://pytorch.org/vision/stable/models.html#table-of-all-available-classification-weights).\n",
                "We will begin by seeing how to use these already trained networks to predict images taken from the internet. We will start by using a VGG16. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "d9q34b-Vzqus"
            },
            "source": [
                "### Exercise 5\n",
                "Load a pretrained VGG16 model from PyTorch. Use the `weights` variable defined below. Remember to set validation mode calling `.eval()` and move the model to GPU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights = models.VGG16_Weights.DEFAULT\n",
                "# TODO : load a pretrained vgg16\n",
                "model = ...\n",
                "\n",
                "try:\n",
                "    assert next(model.parameters()).is_cuda\n",
                "except AssertionError:\n",
                "    raise Exception(\"Did you forget to move the model to GPU?\")\n",
                "\n",
                "try:\n",
                "    assert not model.training\n",
                "except AssertionError:\n",
                "    raise Exception(\"Did you forget set validation mode?\")\n",
                "\n",
                "print(\"Well done!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "60paNsDSz1CW"
            },
            "source": [
                "We will also load the 1000 imagenet labels in a Python dictionary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "urllib.request.urlretrieve(\"https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt\", \"labels.json\")\n",
                "with open(\"labels.json\") as f:\n",
                "    labels = ast.literal_eval(f.read()) # ast evaluates the string Python code\n",
                "print(f\"We have {len(labels)} labels.\")\n",
                "print(labels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "36qxc1rD0CnQ"
            },
            "source": [
                "We can now download a sample image to test our network. When working with PyTorch, we usually use PIL or PILLOW (Python Image Library), which is the standard Pythonic way of working with images. `Image.open(\"\\path\\to\\image.jpg\")` returns an Image object, wich then can be converted to a Numpy or PyTorch tensor. We can look at the images using the `imshow` method from `matplotlib`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_image(pil_image):\n",
                "    imshow(np.asarray(pil_image))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "urllib.request.urlretrieve(\"https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2018/2-dog.jpg\", \"dog_image.jpg\")\n",
                "%ls\n",
                "\n",
                "pil_image = Image.open(\"dog_image.jpg\")\n",
                "show_image(pil_image)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "UGjzIncp0PVx"
            },
            "source": [
                "As you know, when the network was trained, ImageNet images went through some preprocessing transformations. Before feeding an image to the network, we should do the same transformations. \n",
                "\n",
                "As in validation, we will:\n",
                "* Resize our image to size 256 (this means, the smallest side will be 256)\n",
                "* Cropping the image at the center with size 224\n",
                "* Converting our image to a torch tensor \n",
                "* Normalizing the values of the pixels with Imagenet mean and standard deviation for each channel. These are values are: `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`\n",
                "\n",
                "We will use `transforms.Compose` to compose all these transformations in a single method."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hqUpYInLi0yM"
            },
            "source": [
                "### Exercise 6\n",
                "\n",
                "Define the preprocessing transformations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO : Define the preprocessing transformations defined above\n",
                "preprocess = transforms.Compose([...])\n",
                "\n",
                "torch_image = preprocess(pil_image)\n",
                "try:\n",
                "    assert isinstance(torch_image, torch.Tensor)\n",
                "    assert list(torch_image.shape) == [3, 224, 224]\n",
                "    print(\"Well done!\")\n",
                "except Exception:\n",
                "    raise Exception(\"Did you do the 4 required transformations?\")\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JIzhrG2uzZd8"
            },
            "source": [
                "We can take a look at the image that will go through the network."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def show_torch_image(torch_image: torch.Tensor) -> None:\n",
                "    img = (torch_image-torch_image.min()) / (torch_image.max() - torch_image.min())\n",
                "    show_image(to_pil_image(img))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "show_torch_image(torch_image)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "P4xNWAHO1Dry"
            },
            "source": [
                "Now we can get the predictions of our network. Note that our model expects a batch dimension at the beginning, so we should add it with the method `.unsqueeze()`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "z-m7wKUEkOam"
            },
            "source": [
                "### Exercise 7\n",
                "Complete the predict function. Remember to move the images to GPU, as well as adding the batch dimension."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_image(\n",
                "        torch_image: torch.tensor, \n",
                "        model: torch.nn.Module, \n",
                "        topk: int = 3,\n",
                "        ) -> List[Tuple[str, float]]:\n",
                "    \n",
                "    x = torch_image.to(device) # move image to GPU\n",
                "    x = x.unsqueeze(0) # add batch dimension\n",
                "    \n",
                "    # TODO: predict raw outputs\n",
                "    output = ...\n",
                "\n",
                "    output = torch.softmax(output, dim=1)  #Compute the softmax to get probabilities\n",
                "    probs, idxs = output.topk(topk)  # Get the top k predicitons\n",
                "    return [(labels[i.item()], p.item()*100) for p, i in zip(probs[0], idxs[0])]\n",
                "\n",
                "print(predict_image(torch_image, model))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RDYhmZ9rEl2V"
            },
            "source": [
                "### Exercise 8\n",
                "Do the prediction with other images using different networks and compare the results.  You can use the function following function predict_from_url. The list of pretrained PyTorch networks can be found [here](https://pytorch.org/vision/stable/models.html#classification). When trying different models remember to move them to GPU and set evaluation mode."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_from_url(\n",
                "        url: str, \n",
                "        model: torch.nn.Module,\n",
                "        ) -> None:\n",
                "    urllib.request.urlretrieve(url, \"image.jpg\")\n",
                "    pil_image = Image.open(\"image.jpg\")\n",
                "    show_image(pil_image)\n",
                "    torch_image = preprocess(pil_image)\n",
                "    print(predict_image(torch_image, model))\n",
                "\n",
                "predict_from_url(\"https://icatcare.org/app/uploads/2018/07/Thinking-of-getting-a-cat.png\", model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#new_model = models.vgg16(weights='DEFAULT').to(device).eval()\n",
                "#new_model = models.ResNet(weights='DEFAULT').to(device).eval()\n",
                "\n",
                "resnet18 = models.resnet18(weights='DEFAULT').to(device).eval()\n",
                "#alexnet = models.alexnet(weights='DEFAULT')\n",
                "#vgg16 = models.vgg16(weights='DEFAULT')\n",
                "#squeezenet = models.squeezenet1_0(weights='DEFAULT')\n",
                "#densenet = models.densenet161(weights='DEFAULT')\n",
                "#inception = models.inception_v3(weights='DEFAULT')\n",
                "#googlenet = models.googlenet(weights='DEFAULT')\n",
                "#shufflenet = models.shufflenet_v2_x1_0(weights='DEFAULT')\n",
                "#mobilenet_v2 = models.mobilenet_v2(weights='DEFAULT')\n",
                "#mobilenet_v3_large = models.mobilenet_v3_large(weights='DEFAULT')\n",
                "#mobilenet_v3_small = models.mobilenet_v3_small(weights='DEFAULT')\n",
                "#resnext50_32x4d = models.resnext50_32x4d(weights='DEFAULT')\n",
                "#wide_resnet50_2 = models.wide_resnet50_2(weights='DEFAULT')\n",
                "#mnasnet = models.mnasnet1_0(weights='DEFAULT')\n",
                "\n",
                "predict_from_url(\"https://icatcare.org/app/uploads/2018/07/Thinking-of-getting-a-cat.png\", resnet18)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "gpuClass": "standard",
        "interpreter": {
            "hash": "0c3d4c834946bf2705ac1ec09f8a66e453f50361eed1d0fb26200a7575f77ccd"
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.1"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "10c10e7c2d674bc380e42337207431a9": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_49c9be81291643b289797e9d93c7e6e6",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_af22af5cb29443918725b356b0e27526",
                        "value": " 528M/528M [00:02&lt;00:00, 233MB/s]"
                    }
                },
                "1c451072689e40668b9e2093fda4a935": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "1ca0ffa11c2e4cf785744c7dac5e65bd": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "1eb2a7a1b8c74e2fa8a6ac601ce8a06a": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "2149382839cb47b1956812ef99b5bdd8": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_56a5fd6de2b345858a857fc27a07a724",
                            "IPY_MODEL_b5b4fd12c32648dfa6a7e037b55e1291",
                            "IPY_MODEL_557cf5a0b16641b599fff39dacedfd7b"
                        ],
                        "layout": "IPY_MODEL_506b41e456f34a7ea3a8e95511eae874"
                    }
                },
                "2fa35279a0a74b6fa0b714f5f3ea873b": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "33b073cbcfce462986cf9b52ace51068": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "39750d145bd44fb3883ac9fa1be72825": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "4453f7e1d35849689fe732c947a83e63": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "49c9be81291643b289797e9d93c7e6e6": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "506b41e456f34a7ea3a8e95511eae874": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "557cf5a0b16641b599fff39dacedfd7b": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_64b377b968024f36b972c4165368d9ae",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_33b073cbcfce462986cf9b52ace51068",
                        "value": " 169001437/169001437 [00:13&lt;00:00, 13546709.06it/s]"
                    }
                },
                "56a5fd6de2b345858a857fc27a07a724": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_cf65001dcfb249d6ae589b585d67ba9c",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_1c451072689e40668b9e2093fda4a935",
                        "value": "100%"
                    }
                },
                "64b377b968024f36b972c4165368d9ae": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "650fdc373f404766bbc6adb26a46da7f": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_86357474150e481d9880c43a2bbfff54",
                            "IPY_MODEL_81f927b7613f42ffa1c2b23c2dd96a5f",
                            "IPY_MODEL_10c10e7c2d674bc380e42337207431a9"
                        ],
                        "layout": "IPY_MODEL_39750d145bd44fb3883ac9fa1be72825"
                    }
                },
                "81f927b7613f42ffa1c2b23c2dd96a5f": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_1ca0ffa11c2e4cf785744c7dac5e65bd",
                        "max": 553433881,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_1eb2a7a1b8c74e2fa8a6ac601ce8a06a",
                        "value": 553433881
                    }
                },
                "86357474150e481d9880c43a2bbfff54": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_98f1936b0a3e49b680b13608919f607e",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_4453f7e1d35849689fe732c947a83e63",
                        "value": "100%"
                    }
                },
                "98f1936b0a3e49b680b13608919f607e": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "af22af5cb29443918725b356b0e27526": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "b5b4fd12c32648dfa6a7e037b55e1291": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_d8afea3caf7a424a83b29ea961aa66d4",
                        "max": 169001437,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_2fa35279a0a74b6fa0b714f5f3ea873b",
                        "value": 169001437
                    }
                },
                "cf65001dcfb249d6ae589b585d67ba9c": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "d8afea3caf7a424a83b29ea961aa66d4": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
