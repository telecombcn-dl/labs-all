{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "w3O99Tszc1eu"
            },
            "source": [
                "# Sentiment Analysis: Data preprocessing\n",
                "\n",
                "In this notebook we are going to compare different approaches of data preprocessing applied to a real world task, analysing movie reviews given by IMBD users. Each review can be classified in two different classes, positive, if the user likes the movie and negative otherwise. This tutorial is inspired by: https://towardsdatascience.com/sentiment-analysis-with-python-part-1-5ce197074184"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IC9er8QPdGXZ"
            },
            "source": [
                "## Data preprocessing\n",
                "\n",
                "Our first step will be to load and prepare all the our data to perform the experiments. In this case we are going to employ IMBD reviews data available at The Training Dataset used is stored in the zipped folder: aclImbdb.tar file. This can also be downloaded from: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
                "The dataset consists in 50.000 sentences splitted in two sets, train and test of 25.000 sentences each.\n",
                "\n",
                "For our experiments the corpus will be splitted in the following way:\n",
                "* Train_split: 17500 sentences extracted from the original training data.\n",
                "* Validation split: 7500 sentences extracted from the original training data.\n",
                "* Test split: 25000 sentences that form the original test data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8lD88efeYFwS"
            },
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torchdata\n",
                "!pip install 'portalocker>=2.0.0'\n",
                "!pip install datasets\n",
                "import nltk\n",
                "nltk.download('omw-1.4')\n",
                "nltk.download('punkt_tab')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "\n",
                "# Load dataset\n",
                "dataset = load_dataset(\"imdb\")\n",
                "import random\n",
                "import itertools\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "\n",
                "SEED = 0\n",
                "\n",
                "train_data = dataset[\"train\"]\n",
                "test_data = dataset[\"test\"]\n",
                "train_split = list(train_data)\n",
                "test_split = list(test_data)\n",
                "train_sents = [t[\"text\"].split() for t in train_split]\n",
                "test_sents = [t[\"text\"].split() for t in test_split]\n",
                "\n",
                "train_targets = [t[\"label\"] for t in train_split]\n",
                "test_targets = [t[\"label\"] for t in test_split]\n",
                "\n",
                "\n",
                "train_sents, valid_sents, train_targets, valid_targets = train_test_split(train_sents,\n",
                "                                                                          train_targets,\n",
                "                                                                          test_size=0.25,\n",
                "                                                                          random_state=SEED)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ej5mLvMzEI5h"
            },
            "source": [
                "In order to feed our data into a model that is able to predict the sentiment of our movie reviews we should first create a vocabulary of all the words that appear in our training data.\n",
                "\n",
                "A measure of quality of our vocabulary can be its coverage over the data. We can define the vocabulary coverage ans the percentage of tokens from our data that are found in our vocabulary.\n",
                "\n",
                "As we will see later vocabulary size is a paremeter that can be tuned and this coverage can serve as guidande.\n",
                "\n",
                "**Compute the vocabulary as a dictionary with words as keys and the number of times the word appears as value.**\n",
                "\n",
                "**Compute the coverage over the data given our vocabulary.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def compute_vocabulary(train_sents):\n",
                "  vocabulary = {'<unk>':99999999}\n",
                "  for sent in train_sents:\n",
                "    for token in sent:\n",
                "      if token in vocabulary:\n",
                "        vocabulary[token] += 1\n",
                "      else:\n",
                "        vocabulary[token] = 1\n",
                "\n",
                "  return vocabulary\n",
                "\n",
                "def coverage(split,voc):\n",
                "  total = 0.0\n",
                "  unk = 0.0\n",
                "  for sent in split:\n",
                "    for token in sent:\n",
                "      if not  token in voc:\n",
                "          unk += 1.0\n",
                "      total += 1.0\n",
                "  return 1.0 - (unk/total)\n",
                "\n",
                "\n",
                "def voc_stats(split,voc):\n",
                "  print('**** VOCABULARY ***')\n",
                "  print('* Unique words', len(voc))\n",
                "  print('* Coverage', coverage(split,voc))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "voc = compute_vocabulary(train_sents)\n",
                "voc_stats(valid_sents, voc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "V3bU72nvO6Kl"
            },
            "source": [
                "Our first proposed preprocess is word level tokenization. Tokenization consists in separating all words or particles that are attached to the words in the text. In this case, all word will be splitted and stop-words will be separated. In this case we are going to employ 'nltk' library for this task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('punkt')\n",
                "\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem.porter import PorterStemmer\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from nltk.tokenize import TweetTokenizer\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "stop_words = list(stopwords.words('english')) #About 150 stopwords\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "def tokenize_sentence(sentence):\n",
                "    return  nltk.word_tokenize(sentence)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DIvqs7LPw92B"
            },
            "source": [
                "**Compute the tokenization and vocabulary given the function avobe. How the vocabulary size and the coverage have changed?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_tok_sents = [tokenize_sentence(' '.join(s)) for s in train_sents]\n",
                "valid_tok_sents = [tokenize_sentence(' '.join(s)) for s in valid_sents]\n",
                "test_tok_sents = [tokenize_sentence(' '.join(s)) for s in test_sents]\n",
                "\n",
                "tok_voc = compute_vocabulary(train_tok_sents)\n",
                "voc_stats(valid_tok_sents, tok_voc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "M-3x7JSVjw21"
            },
            "source": [
                "\n",
                "In addition to the previous steps we are going further preprocess our data.\n",
                "\n",
                "For each review in the corpus we apply a preprocess consisting in:\n",
                "\n",
                "- Each word is replaced by its lemma form. Lemmatization reduces vocabulary size as all different forms of a word are grouped in a single lemma.\n",
                "- Remove casing from words, this helps reduce ambiguity as upper and lower cased appearences of a word are selected as different in the vocabulary.\n",
                "- Remove stop words. In order to reduce sentence length and represent the words that carry the meaning of the sentence, we remove all stop words.\n",
                "\n",
                "**Apply the preprocess and compute its new vocabulary. How did stats change?***\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_sentence(sentence):\n",
                "    return  [lemmatizer.lemmatize(word.lower()) for word in sentence if not word in stop_words]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_tok_sents = [preprocess_sentence(s) for s in train_tok_sents]\n",
                "# TODO Complete sentence tokenizer for validation set and testset\n",
                "\n",
                "# TODO: Make the tokenized vocabulary and calculate the stats\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OAtcsI1KGX63"
            },
            "source": [
                "The previous preprocess helps to reduce the ambiguity produced by the different\n",
                "form of the same word or stop-words included in other tokens, but the original tokens are mostly unchanged. In the following cases we will explore other levels of tokenization, characters and subwords."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "UKX1tr7leglW"
            },
            "source": [
                "In both cases vocabularies are too big to be handled in this task as a lot of words only appear a few times in the dataset of some ambiguity is still present in the data. A common resourse is removing the less frequent tokens in our vocabulary until ensuring that models can be trained with the resources available.\n",
                "\n",
                "**Create a 5000 token vocabulary that only include the most frequent tokens in the dataset. How is the new coverage?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def reduce_vocabulary(voc,size=5000):\n",
                "  # TODO: Create a 5000 token vocabulary that only include the most frequent tokens in the dataset\n",
                "\n",
                "SIZE_5000 = 2000\n",
                "# TODO: Apply the reduced vocabulary for both tokenized and not tokenized vocabulary (e.g. tok_voc = reduce_vocabulary(...)) and calculate the new vocabulary\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PY1cBAJdGF2N"
            },
            "source": [
                "\n",
                "\n",
                "Let's start with characters, where all words in the dataset are stripped into its individual characters, which has several new characterisitics:\n",
                "* Vocabularies are orders of magnitude smaller that its word counterparts, as all words share the same script that uses a limited set of them.\n",
                "* Each token does not provide a lot of information about the sentence. Individual words include semantic and morpghological information.\n",
                "* Character's use is more ambiguious, words are used generally on the same context consistently during the dataset while characters can belong to a great number of words. Also, individual characters have much less meaning information than words or\n",
                "\n",
                "**Based on the previous tokenization compute a character vocabulary over the training, and measure its size and coverage of the validation data**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_char_sents = [' '.join(s).strip() for s in train_tok_sents]\n",
                "# TODO: Complete sentence char for validation set and test set\n",
                "\n",
                "# TODO: Make the character level vocabulary and vocabulary\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7BIFW2qZRjQj"
            },
            "source": [
                "Character level may be too extreme for some tasks, but it provides a great coverage of the dataset. In those cases a great alternative e to apply is subword tokenization.\n",
                "\n",
                "In this case words are splitted in pieces which lenght depends on how common they are in the dataset. This way long pieces that are really common will be maintained, for example lemmas of common words in the data, while for not common words or affixes the vocabulary includes smaller pieces until arriving to  individual characters.\n",
                "\n",
                "This tokenizations allows:\n",
                "* A great coverage of the data, as only words including characters not present in the training data will not be recognized.\n",
                "* A parametrized vocabulary size. The number of tokens in the vocabulary is set when the tokenization is computed and can be tuned to improve the models performance.\n",
                "\n",
                "The example shows a call to Byte-Pair-Encoding tokenization using the standard subword-nmt library."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "! pip install subword-nmt\n",
                "\n",
                "# Write the data splits into files to compute the Byte-Pair Encoding (BPE)\n",
                "with open('train.txt','w+') as tr:\n",
                "  for s in train_tok_sents:\n",
                "    print(' '.join(s),file=tr)\n",
                "\n",
                "with open('valid.txt','w+') as vl:\n",
                "  for s in valid_tok_sents:\n",
                "    print(' '.join(s),file=vl)\n",
                "\n",
                "with open('test.txt','w+') as ts:\n",
                "  for s in test_tok_sents:\n",
                "    print(' '.join(s),file=ts)\n",
                "\n",
                "\n",
                "# Compute BPE codes and apply to the data splits\n",
                "! subword-nmt learn-bpe -s 1000 < train.txt > bpe1000.codes\n",
                "! subword-nmt apply-bpe -c bpe1000.codes < train.txt > train.bpe.txt\n",
                "! subword-nmt apply-bpe -c bpe1000.codes < valid.txt > valid.bpe.txt\n",
                "! subword-nmt apply-bpe -c bpe1000.codes < test.txt > test.bpe.txt\n",
                "\n",
                "train_bpe_sents = []\n",
                "with open('train.bpe.txt') as tr:\n",
                "  for line in tr.readlines():\n",
                "    train_bpe_sents.append(line.replace('\\n','').split())\n",
                "\n",
                "valid_bpe_sents = []\n",
                "with open('valid.bpe.txt') as tr:\n",
                "  for line in tr.readlines():\n",
                "    valid_bpe_sents.append(line.replace('\\n','').split())\n",
                "\n",
                "test_bpe_sents = []\n",
                "with open('test.bpe.txt') as tr:\n",
                "  for line in tr.readlines():\n",
                "    test_bpe_sents.append(line.replace('\\n','').split())\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r_MkG_YRoKhM"
            },
            "source": [
                "**Compute the coverage and vocabulary size of the subword tokenization**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Make the bpe level vocabulary and caculate the coverage and stats\n",
                "bpe_voc = ...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2IJKP3w4sf1T"
            },
            "source": [
                "**Pretrained Tokenizers from Language Models:**\n",
                "\n",
                "Pretrained tokenizers are components of large, pretrained language models (like BERT, GPT, or RoBERTa). These tokenizers come with a pre-built vocabulary and a set of rules for splitting text into tokens (e.g., words or subwords). They have been trained on massive amounts of text data and are designed to work seamlessly with their corresponding language models.\n",
                "\n",
                "Using a pretrained tokenizer has two main advantages:\n",
                "\n",
                "\n",
                "*   Consistency: The tokenizer\u2019s vocabulary and rules match those used during the\n",
                "\n",
                "*   model's training, ensuring that your text is processed in the same way.\n",
                "Convenience: Instead of building your own tokenizer and managing a vocabulary manually, you can simply load an existing one using libraries such as Hugging Face Transformers.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoTokenizer\n",
                "\n",
                "def tokenize_with_pretrained(model_name, sentences):\n",
                "    # Load the tokenizer corresponding to the given model name.\n",
                "    # This retrieves a pretrained tokenizer with its associated vocabulary and tokenization rules.\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "\n",
                "    # Check if the sentences are provided as lists of tokens.\n",
                "    # If so, join them into raw text strings (because the tokenizer expects plain text).\n",
                "    if isinstance(sentences[0], list):\n",
                "        sentences = [\" \".join(s) for s in sentences]\n",
                "\n",
                "    # Tokenize the sentences:\n",
                "    # - 'padding=True' ensures all sequences are padded to the same length.\n",
                "    # - 'truncation=True' limits sequences that are too long.\n",
                "    # - 'max_length=512' sets the maximum allowed length of each tokenized sentence.\n",
                "    encoded = tokenizer(sentences,\n",
                "                        padding=True,\n",
                "                        truncation=True,\n",
                "                        max_length=512)\n",
                "\n",
                "    # Convert the token IDs back into token strings.\n",
                "    # This returns a list where each element corresponds to the tokenized representation of a sentence.\n",
                "    return [tokenizer.convert_ids_to_tokens(ids) for ids in encoded[\"input_ids\"]]\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6QeKkAiCC9zl"
            },
            "source": [
                "**Tokenize the sentences with the BERT tokenizer**\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tokenize with BERT-base-uncased and XLM-RoBERTa-base\n",
                "bert_tokenized_train = tokenize_with_pretrained(\"bert-base-uncased\", train_sents)\n",
                "# xlmr_tokenized_train = tokenize_with_pretrained(\"xlm-roberta-base\", train_tok_sents)\n",
                "\n",
                "bert_tokenized_valid = tokenize_with_pretrained(\"bert-base-uncased\", valid_sents)\n",
                "# xlmr_tokenized_valid = tokenize_with_pretrained(\"xlm-roberta-base\", valid_tok_sents)\n",
                "\n",
                "bert_tokenized_test = tokenize_with_pretrained(\"bert-base-uncased\", test_sents)\n",
                "# xlmr_tokenized_test = tokenize_with_pretrained(\"xlm-roberta-base\", test_tok_sents)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bert_voc = compute_vocabulary(bert_tokenized_train)\n",
                "voc_stats(bert_tokenized_valid,bert_voc)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# xlmr_voc = compute_vocabulary(xlmr_tokenized_train)\n",
                "# voc_stats(xlmr_tokenized_valid,xlmr_voc)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uGLsWv-6dpJh"
            },
            "source": [
                "# Classification Task\n",
                "\n",
                "In this final section we are going to measure the performance of a model when using as features the different tokenization levels described above."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RmBiQzymX2dU"
            },
            "source": [
                "The first step in order to train a model is decide how to feed our data into the model. For these experiments, we are going to employ bag of words vectors as they do not require any further preprocess.\n",
                "\n",
                "Bag of Words (BOW) consists in representing each of the sentences of the dataset as fixed size vectors of the size of the vocabulary. For example, for our 5000 tokens vocabularies, each sentence will be represented as a 5000 dimension vectors, following these steps:\n",
                "\n",
                "- Initially all vector dimensions are initialized as 0.\n",
                "- For each word in the sentence, 1 is added to the position of such word in the vocabulary.\n",
                "\n",
                "The final vector is a sparce vector, most of its values are 0, that contains the number of times each word appears in the sentence and the sum of all dimensions of the vector is the number of tokens in the sentence.\n",
                "\n",
                "Note, that while this representation provides a fixed size representation of the sentences it lacks information of the order in which words appear."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def idx_voc(voc):\n",
                "  items = list(voc.items())\n",
                "  items.sort(key=lambda x: x[1], reverse=True)\n",
                "  return {item[0]:n for n,item in enumerate(items)}\n",
                "\n",
                "def bag_of_words(splits,voc):\n",
                "  voc = idx_voc(voc)\n",
                "  bow_splits = []\n",
                "  for split in splits:\n",
                "    bow = []\n",
                "    for sent in split:\n",
                "      vector = [0] * len(voc)\n",
                "      for s in sent:\n",
                "        try:\n",
                "          vector[voc[s]] += 1\n",
                "        except:\n",
                "          vector[voc['<unk>']] += 1\n",
                "      bow.append(vector)\n",
                "    bow_splits.append(bow)\n",
                "  return bow_splits\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "y5OrYj_spm4X"
            },
            "source": [
                "When you use a pretrained BERT tokenizer, it splits text into subword units rather than whole words. Applying a simple bag-of-words model on these subwords can lead to an extremely large and fragmented vocabulary, resulting in very high-dimensional and sparse representations. This dense representation, if not handled correctly, can be both computationally and memory inefficient. In contrast, TF-IDF creates a sparse matrix that only stores non-zero entries and also weights tokens by their importance across the corpus. This sparse format is more memory-friendly and better suited for handling the many subword tokens produced by BERT tokenization.\n",
                "\n",
                "**TF-IDF Vectorization:**\n",
                "The TfidfVectorizer is used to convert the text into a numerical representation.\n",
                "When fitting on the training texts with vectorizer.fit_transform(train_texts), the vectorizer learns the vocabulary and calculates the inverse document frequency (IDF) for each token.\n",
                "The test texts are then transformed using vectorizer.transform(test_texts) based on the vocabulary learned from the training data.\n",
                "The parameter max_features=5000 limits the number of features (tokens) to 5000 to control memory usage and potentially reduce noise.\n",
                "\n",
                "\n",
                "*   For character-level processing:\n",
                "Each sentence (which is a list of characters) is joined using \"\".join(sent) to create a continuous string without spaces. This ensures that each character is preserved in sequence.\n",
                "\n",
                "*   For word-level processing:\n",
                "Each sentence (a list of word tokens) is joined with spaces using \" \".join(sent). This creates a typical sentence string where words are separated by spaces, which is what the vectorizer expects.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "def compute_tfidf(splits, char_level=False):\n",
                "    \"\"\"\n",
                "    Computes TF-IDF features for training and testing datasets.\n",
                "\n",
                "    Parameters:\n",
                "        splits (list): A list of three elements where:\n",
                "                        - splits[0] is the training data,\n",
                "                        - splits[1] is the validation data (unused here),\n",
                "                        - splits[2] is the test data.\n",
                "                        Each element should be a list of sentences, and each sentence is a list of tokens.\n",
                "        char_level (bool): If True, use character-level tokenization; otherwise, use word-level tokenization.\n",
                "\n",
                "    Returns:\n",
                "        train_tfidf, test_tfidf: The TF-IDF transformed training and test data.\n",
                "    \"\"\"\n",
                "    if char_level:\n",
                "        # If char_level is True, we configure the vectorizer to break text into characters.\n",
                "        # analyzer='char' means that individual characters (or character n-grams) are the tokens.\n",
                "        # ngram_range=(1, 1) specifies that we only consider individual characters.\n",
                "        # max_features limits the vocabulary size to 5000 to avoid memory issues.\n",
                "        vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 1), max_features=5000)\n",
                "        # For character-level analysis, join each sentence's characters without spaces.\n",
                "        # This produces a continuous string where each character is kept in order.\n",
                "        train_texts = [\"\".join(sent) for sent in splits[0]]\n",
                "        test_texts = [\"\".join(sent) for sent in splits[2]]\n",
                "    else:\n",
                "        # Default behavior uses word-level tokenization.\n",
                "        # The vectorizer will automatically tokenize the text into words.\n",
                "        vectorizer = TfidfVectorizer(max_features=5000)\n",
                "        # For word-level analysis, join tokens with a space to form a proper sentence.\n",
                "        train_texts = [\" \".join(sent) for sent in splits[0]]\n",
                "        test_texts = [\" \".join(sent) for sent in splits[2]]\n",
                "\n",
                "    # Fit the vectorizer on the training data to learn the vocabulary and IDF values,\n",
                "    # then transform the training texts into TF-IDF feature matrices.\n",
                "    train_tfidf = vectorizer.fit_transform(train_texts)\n",
                "    # Use the learned vocabulary to transform the test texts into TF-IDF feature matrices.\n",
                "    test_tfidf = vectorizer.transform(test_texts)\n",
                "\n",
                "    return train_tfidf, test_tfidf\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WLCAoylszhtE"
            },
            "source": [
                "As a classifier we are going to employ Random forest (https://towardsdatascience.com/understanding-random-forest-58381e0602d2), a model that consists in an ensemble of Decision Trees that see different data by means of bagging and boosting.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "def get_accuracy(preds,labels):\n",
                "    return sum([p == l for p,l in zip(preds,labels)])/len(preds)*100\n",
                "\n",
                "def preprocess_data(data):\n",
                "    # Check if the data is sparse (i.e., has a toarray() method)\n",
                "    if hasattr(data, \"toarray\"):\n",
                "        return data.toarray()\n",
                "    else:\n",
                "        return np.array(data, dtype='float')\n",
                "\n",
                "def trainRandomForestClassifier(train_idx, test_idx):\n",
                "    # Preprocess both train and test data\n",
                "    train_idx = preprocess_data(train_idx)\n",
                "    test_idx = preprocess_data(test_idx)\n",
                "\n",
                "    clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=SEED)\n",
                "    clf.fit(train_idx, train_targets)\n",
                "\n",
                "    preds = clf.predict(test_idx)\n",
                "    print('Accuracy', get_accuracy(preds, test_targets), '%')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XoMiEaDjrj3g"
            },
            "source": [
                "**Train the model for the different preprocesses. How they affect the results?**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# NOT tokenized data\n",
                "train_idx,valid_idx,test_idx = bag_of_words([train_sents,valid_sents,test_sents],voc)\n",
                "trainRandomForestClassifier(train_idx,test_idx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tokenized at word level and preprocessed data\n",
                "# TODO: continue the training and evaluation for tokenized data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Tokenized and character data\n",
                "# TODO: continue the training and evaluation for Char data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#BPE tokenized data\n",
                "# TODO: continue the training and evaluation for Bpe data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IO-VgoI0lDP2"
            },
            "source": [
                "**Running the same experiment with TF-IDF and comparing the results**\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_idx, test_idx = compute_tfidf([train_sents,valid_sents,test_sents])\n",
                "trainRandomForestClassifier(train_idx,test_idx)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: continue the training and evaluation for tokenized data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: continue the training and evaluation for Char data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#BPE bpe data\n",
                "# TODO: continue the training and evaluation for Bpe data\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lglGti7pk-l8"
            },
            "source": [
                "**Now we are going to try the BERT tokenizer with TF-IDF**\n",
                "\n",
                "---\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_idx, test_idx = compute_tfidf([bert_tokenized_train, bert_tokenized_valid, bert_tokenized_test])\n",
                "trainRandomForestClassifier(train_idx,test_idx)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "FWmiBxUmagDS"
            },
            "source": [
                "# Conclusions\n",
                "* Preprocessing plays an important role in the performance of NLP systems.\n",
                "* Even for the same model performance may vary a lot depending on how expressive are the features selected for the task.\n",
                "* All methods  presented (with the exception of no preprocess) are employed in state of the art work for several tasks.\n",
                "* We can use pre-trained tokenizers from language models instead of creating them\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
