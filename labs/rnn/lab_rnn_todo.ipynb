{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "bNgvzqC1IBjH"
            },
            "source": [
                "# Recurrent Neural Networks\n",
                "\n",
                "## Lab credit\n",
                "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2019).\n",
                "\n",
                "Updated by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) (2021), [Javier Ferrando](https://www.linkedin.com/in/javierferrandomonsonis/) (2022), and [Ioannis Tsiamas](https://www.linkedin.com/in/i-tsiamas/) (2023).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PqBBz46awSAh"
            },
            "source": [
                "# The Fault in Our Time\n",
                "\n",
                "This lab session introduces our beloved friends, the [recurrent neural networks (RNNs)](https://en.wikipedia.org/wiki/Recurrent_neural_network). Concretely, the topology we will be seeing is the Elman type, nowadays widely known plainly as RNN. Recurrent neural networks are the super cool queens of sequences: they know about order in sequences. As a quick test for how important sequential context is, and to prove that it is also very important for you... **CAN YOU TELL THE SIXTH DIGIT OF YOUR MOBILE PHONE NUMBER? WHAT PROCESS ARE YOU FOLLOWING TO RECALL IT?** Exactly, you went straight from the beginning of the full sequence, hence this is how important sequences are to us too :)\n",
                "\n",
                "As in the example before, we always work with sequences when using RNNs. In each batch of data, we have as many elements as the length of the sequence (seq_len), and each of these elements can contain multiple features (num_feats):\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/input_batch.png?raw=true\" class=\"center\" title=\"input batch\" width=\"300\"/>\n",
                "</p><br>\n",
                "\n",
                "A fully connected layer is defined as:\n",
                "\n",
                "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{b})$\n",
                "\n",
                "With \"only one\" (but super important) change we formulate the RNN:\n",
                "\n",
                "$\\boldsymbol{h}_t = \\tanh(\\boldsymbol{W}\\boldsymbol{x}_t + \\boldsymbol{U}\\boldsymbol{h}_{t-1} + \\boldsymbol{b})$\n",
                "\n",
                "Exactly, we added the matrix $\\boldsymbol{U}$ which is a set of connections among all the neurons from the hidden layers to themselves (hence a feedback)!\n",
                "\n",
                "This looks like the following, which is typically unrolled in time to show both flows of data, feed-forward ($\\boldsymbol{W}$) + time ($\\boldsymbol{U}$):\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/one_layer_rnn.png?raw=true\" class=\"center\" title=\"one layer RNN\" width=\"300\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's first import the typical stuff to play with deep nets\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import numpy as np\n",
                "import torch.optim as optim\n",
                "import matplotlib\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "from timeit import default_timer as timer\n",
                "\n",
                "torch.manual_seed(1)\n",
                "device = 'cpu'\n",
                "if torch.cuda.is_available():\n",
                "  device = 'cuda'\n",
                "  torch.cuda.manual_seed_all(1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e8YHdo2o0Ago"
            },
            "source": [
                "### Exercise 1: Building a recurrent neural layer\n",
                "\n",
                "In the next cell, we will define our own unidirectional RNN layer. The class `MyUnidirectionalRNN` must make use of `nn.Linear` layers to make the feed-forward and time projections, and use the `nn.Parameter` class to build the biases. Please build the recurrent neural component with the addition of the recurrent connections.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MyUnidirectionalRNN(nn.Module):\n",
                "\n",
                "  def __init__(self, num_feats, rnn_size=128):\n",
                "    super().__init__()\n",
                "    self.rnn_size = rnn_size\n",
                "\n",
                "    # Definition of the RNN parameters with the use of Linear layers:\n",
                "\n",
                "    # Define the input activation matrix W\n",
                "    self.W = nn.Linear(num_feats, rnn_size, bias=False)\n",
                "\n",
                "    # TODO: Define the hidden activation matrix U\n",
                "    self.U = ...\n",
                "\n",
                "    # Define the bias\n",
                "    self.b = nn.Parameter(torch.zeros(1, rnn_size))\n",
                "\n",
                "  def forward(self, x, state=None):\n",
                "    # Assuming x is of shape [batch_size, seq_len, num_feats]\n",
                "    xs = torch.chunk(x, x.shape[1], dim=1)\n",
                "    hts = []\n",
                "    if state is None:\n",
                "      state = self.init_state(x.shape[0])\n",
                "    ht = state\n",
                "    for xt in xs:\n",
                "      # turn x[t] into shape [batch_size, num_feats] to be projected\n",
                "      xt = xt.squeeze(1)\n",
                "      ct = self.W(xt)\n",
                "      ct = ct + self.U(ht)\n",
                "      ht = ct + self.b\n",
                "      # give the temporal dimension back to h[t] to be cated\n",
                "      hts.append(ht.unsqueeze(1))\n",
                "    hts = torch.cat(hts, dim=1)\n",
                "    return hts\n",
                "\n",
                "  def init_state(self, batch_size):\n",
                "    return torch.zeros(batch_size, self.rnn_size)\n",
                "\n",
                "# To correctly assess the answer, we build an example RNN with 10 inputs and 32 neurons\n",
                "rnn = MyUnidirectionalRNN(10, 32)\n",
                "# Then we will forward some random sequences, each of length 15\n",
                "xt = torch.randn(5, 15, 10)\n",
                "# The returned tensor will be h[t]\n",
                "ht = rnn(xt)\n",
                "assert ht.shape[0] == 5 and ht.shape[1] == 15 and ht.shape[2] == 32, \\\n",
                "'Something went wrong within the RNN :('\n",
                "print('Success! Output shape: {} sequences, each of length {}, each '\\\n",
                "      'token with {} dims'.format(ht.shape[0], ht.shape[1], ht.shape[2]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "THnEZ6UmPAJU"
            },
            "source": [
                "### But Why Would You Do That?\n",
                "\n",
                "Congratz on finishing your first RNN definition! Now you should understand a bit more on the intrinsics of our sequential friends. But why would you define your own RNN? We didn't even operate with a GPU. We didn't even consider that possibility. So in the real world, we use PyTorch's `nn.RNN`, which allows for building a **stack of RNN layers directly**. Let's see some examples:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we will work with 10 input features\n",
                "NUM_FEATS = 10\n",
                "# and sequences of length 25\n",
                "SEQ_LEN = 25\n",
                "# and 5 samples per batch\n",
                "BATCH_SIZE = 5 \n",
                "# and 128 neurons\n",
                "HIDDEN_SIZE = 128\n",
                "\n",
                "# The first RNN contains a single layer\n",
                "rnn1 = nn.RNN(NUM_FEATS, HIDDEN_SIZE)\n",
                "print(rnn1)\n",
                "\n",
                "# Now let's build a random input tensor to forward through it\n",
                "xt = torch.randn(SEQ_LEN, BATCH_SIZE, NUM_FEATS)\n",
                "ht, state = rnn1(xt)\n",
                "print('Output h[t] tensor shape: ', ht.shape)\n",
                "print('Output state tensor shape: ', state.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eg4c-TaLQ5Ag"
            },
            "source": [
                "#### OK STOP IT HERE, We've got to talk\n",
                "\n",
                "Think about how many things are happening in the previous cell. First, we define some hyper-parameters to define the input tensor shape and the RNN size. Then, we build one RNN layer. Then, we build random data. Finally, we forward the random data, and what is returned? Why does the input tensor `x` have that shape? Why is the RNN returning 2 output values?\n",
                "\n",
                "**First answer:** The input data to an RNN can be shaped in 2 formats: `batch_first=True` and `batch_first=False`. As its name indicates, when it is `False`, the `batch_size` dimension is not the first but the second one. Then which is the first one? The `sequence_length`. If we do not specify anything, by default `batch_first=False`, so the tensor $\\boldsymbol{x}_t$ must have the dimensions: [`seq_len`, `batch_size`, `num_feats`]. We normally use `batch_first=True` to couple the RNN easily with other layers like the `nn.Linear` one.\n",
                "\n",
                "### Exercise 2\n",
                "\n",
                "Find the second answer on \"**Why is the RNN returning 2 output values?**\". Understand what is the `state` output and answer: \"**what does it contain?**\". Your source of knowledge is in the following URL, where the outputs description for the `RNN` module is given: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1wAY1wWTT3pR"
            },
            "source": [
                "Now we can continue defining some more examples of RNN layers as promised before"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2 Layer RNN\n",
                "rnn2 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2)\n",
                "ht, state = rnn2(xt)\n",
                "print('RNN 2 layers >> ht.shape: ', ht.shape)\n",
                "print('RNN 2 layers >> state.shape: ', state.shape)\n",
                "\n",
                "# Batch Size first RNN\n",
                "xt_bf = torch.randn(BATCH_SIZE, SEQ_LEN, NUM_FEATS)\n",
                "rnn3 = nn.RNN(NUM_FEATS, HIDDEN_SIZE, num_layers=2, batch_first=True)\n",
                "ht, state = rnn3(xt_bf)\n",
                "print('RNN 2 layers, batch_first >> ht.shape: ', ht.shape)\n",
                "print('RNN 2 layers, batch_first >> state.shape: ', state.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "aNGrWTyorkNg"
            },
            "source": [
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/rnn/images/two_layers_rnn.png?raw=true\" class=\"center\" title=\"two layers RNN\" width=\"300\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cfRkpNiOUwxA"
            },
            "source": [
                "### Exercise 3.1\n",
                "Build a **bidirectional RNN with 3 layers** by completing the TODO in the code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: build the bidirectional RNN layer\n",
                "bi_rnn = ...\n",
                "\n",
                "# forward xt_bf\n",
                "bi_ht, bi_state = bi_rnn(xt_bf)\n",
                "print('Bidirectional RNN layer >> bi_ht.shape: ', bi_ht.shape)\n",
                "print('Bidirectional RNN layer >> bi_state.shape: ', bi_state.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fDE81Z2XKN2J"
            },
            "source": [
                "### Exercise 3.2\n",
                "What is the output $\\boldsymbol{h}_t$ shape and why?\n",
                "\n",
                "### Exercise 3.3\n",
                "What is the output `state` shape and why?."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HgKP_9eKTQcp"
            },
            "source": [
                "### Hold The Gates! A Recurrent Re-Evolution\n",
                "\n",
                "You've surely heard about the `LSTM` or the `GRU`, two practically sibling recurrent models. Well those are the actual RNNs you will use in your everyday. Why? Because they:\n",
                "1. Improve the memory capacity of the RNN.\n",
                "2. Improve the gradient flow of vanilla RNNs thanks to the learnable gate mechanisms.\n",
                "\n",
                "An LSTM or GRU cell is a composition of different neurons working jointly, and the whole thing replaces a single RNN neuron. The RNN cell (with one $\\tanh$ neuron), the LSTM cell and the GRU cell are depicted in the following figure from [this article](https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwiMiPbfoPHlAhUQCxoKHW9qA04Qjhx6BAgBEAI&url=http%3A%2F%2Fdprogrammer.org%2Frnn-lstm-gru&psig=AOvVaw3mU76KRvFfY9WiOF4N12ex&ust=1574080203478260):\n",
                "\n",
                "![lstm](http://dprogrammer.org/wp-content/uploads/2019/04/RNN-vs-LSTM-vs-GRU-1200x361.png)\n",
                "\n",
                "Now check that out. In the case of the LSTM, we have **two signals flowing in time** apart from the feed-forward input per time-step: $\\boldsymbol{c}_t$ and $\\boldsymbol{h}_t$. The first one is called the cumulative cell state. It basically will add everything it is \"allowed to see\" from the input, and will forget portions of it. This is unbounded. On the other hand, the output cell state $\\boldsymbol{h}_t$ will be the final layer activation (what is allowed to come out of it). This is bounded [-1, 1]."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4WhMLsykdUEq"
            },
            "source": [
                "### Exercise 4: An LSTM Character-based Language Model\n",
                "\n",
                "In this final exercise we will train a language model that will work at the character level. This is, a neural network based on an RNN architecture that will complete language (textual) sequences.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "KFGtDsWkUZoK"
            },
            "source": [
                "Our dataset will be composed of scripts from the *Friends* TV show. Download the episode 1 trainset:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!wget -q https://raw.githubusercontent.com/telecombcn-dl/labs-all/master/labs/rnn/episode1_english.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's prepare some synthetic data\n",
                "\n",
                "def prepare_sequence(seq, char2idx, onehot=True):\n",
                "    # convert sequence of words to indices\n",
                "    idxs = [char2idx[c] for c in seq]\n",
                "    idxs = torch.tensor(idxs, dtype=torch.long)\n",
                "    if onehot:\n",
                "      # conver to onehot (if input to network)\n",
                "      ohs = F.one_hot(idxs, len(char2idx)).float()\n",
                "      return ohs\n",
                "    else:\n",
                "      return idxs\n",
                "\n",
                "with open('episode1_english.txt', 'r') as txt_f:\n",
                "  training_data = [l.rstrip() for l in txt_f if l.rstrip() != '']\n",
                "\n",
                "# merge the training data into one big text line\n",
                "training_data = '$'.join(training_data)\n",
                "\n",
                "# Assign a unique ID to each different character found in the training set\n",
                "char2idx = {}\n",
                "for c in training_data:\n",
                "    if c not in char2idx:\n",
                "        char2idx[c] = len(char2idx)\n",
                "idx2char = dict((v, k) for k, v in char2idx.items())\n",
                "VOCAB_SIZE = len(char2idx)\n",
                "RNN_SIZE = 1024\n",
                "MLP_SIZE = 2048\n",
                "SEQ_LEN = 50\n",
                "print('Number of found vocabulary tokens: ', VOCAB_SIZE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ehk_6JNIkZOs"
            },
            "source": [
                "##### Exercise 4.1\n",
                "* What is the amount of outputs needed by the character prediction model?\n",
                "\n",
                "##### Exercise 4.2\n",
                "* What is the proper activation to plug on top of the MLP (if any)? (Note that we use [`NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) later on).\n",
                "\n",
                "##### Exercise 4.3\n",
                "* Finish the definition of the `CharLSTM` model to include a `nn.LSTM` layer, with `batch_first=True`, `vocab_size` inputs and `rnn_size` cells, and an MLP that projects the `rnn_size` to `mlp_size` with one `ReLU` hidden layer and then to the appropriate amount of outputs. Put a `Dropout(0.4)` after the `ReLU`. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CharLSTM(nn.Module):\n",
                "\n",
                "    def __init__(self, vocab_size, rnn_size, mlp_size):\n",
                "        super().__init__()\n",
                "        self.rnn_size = rnn_size\n",
                "\n",
                "        # TODO: Define the LSTM\n",
                "        self.lstm = ...\n",
                "\n",
                "        self.dout = nn.Dropout(0.4)\n",
                "\n",
                "        # TODO: Create an MLP with a hidden layer of mlp_size neurons that maps\n",
                "        # from the RNN hidden state space to the output space of vocab_size\n",
                "        self.mlp = nn.Sequential(\n",
                "          # Linear layer\n",
                "          # Activation function\n",
                "          # Dropout (0.4)\n",
                "          # Linear layer\n",
                "          # Activation function\n",
                "        )\n",
                "\n",
                "    def forward(self, sentence, state=None):\n",
                "        bsz, slen, vocab = sentence.shape\n",
                "        ht, state = self.lstm(sentence, state)\n",
                "        ht = self.dout(ht)\n",
                "        h = ht.contiguous().view(-1, self.rnn_size)\n",
                "        logprob = self.mlp(h)\n",
                "        return logprob, state"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4K3L5BIyLrja"
            },
            "source": [
                "Test how the model performs when using randomly initialized weights and biases:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's build an example model and see what the scores are before training\n",
                "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
                "\n",
                "# This should output crap as it is not trained, so a fixed random tag for everything\n",
                "\n",
                "def gen_text(model, seed, char2idx, num_chars=150):\n",
                "  model.eval()\n",
                "  # Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
                "  with torch.no_grad():\n",
                "      inputs = prepare_sequence(seed, char2idx)\n",
                "      # fill the RNN memory with the seed sentence\n",
                "      seed_pred, state = model(inputs.unsqueeze(0))\n",
                "      # now begin looping with feedback char by char from the last prediction\n",
                "      preds = seed\n",
                "      curr_pred = torch.argmax(seed_pred[-1, :])\n",
                "      curr_pred = idx2char[curr_pred.item()]\n",
                "      preds += curr_pred\n",
                "      for _ in range(num_chars):\n",
                "\n",
                "        # TODO: Get the next char prediction from the model given the current prediction and current state\n",
                "        inputs = ...\n",
                "        curr_pred, state = ... \n",
                "        \n",
                "        curr_pred = torch.argmax(curr_pred[-1, :])\n",
                "        curr_pred = idx2char[curr_pred.item()]\n",
                "        if curr_pred == '$':\n",
                "          # special token to add newline char\n",
                "          preds += '\\n'\n",
                "        else:\n",
                "          preds += curr_pred\n",
                "      return preds\n",
                "\n",
                "      \n",
                "print(gen_text(model, 'Monica was ', char2idx))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "a_HuhKqRL0p-"
            },
            "source": [
                "Prepare the training data by defining the data batches:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 64\n",
                "T = len(training_data)\n",
                "CHUNK_SIZE = T // BATCH_SIZE\n",
                "# let's first chunk the huge train sequence into BATCH_SIZE sub-sequences\n",
                "trainset = [training_data[beg_i:end_i] \\\n",
                "            for beg_i, end_i in zip(range(0, T - CHUNK_SIZE, CHUNK_SIZE),\n",
                "                                    range(CHUNK_SIZE, T, CHUNK_SIZE))]\n",
                "print('Original training string len: ', T)\n",
                "print('Sub-sequences len: ', CHUNK_SIZE)\n",
                "\n",
                "# The way training works is the following:\n",
                "# at each batch sampling from the trainset, we pick a portion of sequences\n",
                "# continuous with a sliding window in time. Hence, each of the BATCH_SIZE sub-sequences\n",
                "# in batch b[i] will continue in batch b[i + 1] in the same position of the batch dimension.\n",
                "# This is called stateful sampling, where we train with consecutive windows of sequences\n",
                "# We broke the long string into BATCH_SIZE subsequence, so we introduced BATCH_SIZE - 1 \n",
                "# discontinuities... YES. But we can assume that each sub-sequence is continuous in a long\n",
                "# enough chunk so that those discontinuities are negligible."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "U_VVeGkjk5bs"
            },
            "source": [
                "##### Exercise 4.4\n",
                "\n",
                "What is the length of the sliding window that will run over each of the training sub-sequences? \n",
                "\n",
                "NOTE: it is defined as a hyper-parameter above. How is this related to the backpropagation through time (BPTT)?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's now build a model to train with its optimizer and loss\n",
                "model = CharLSTM(VOCAB_SIZE, RNN_SIZE, MLP_SIZE)\n",
                "model.to(device)\n",
                "loss_function = nn.NLLLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "NUM_EPOCHS = 2000\n",
                "tr_loss = []\n",
                "state = None\n",
                "timer_beg = timer()\n",
                "for epoch in range(NUM_EPOCHS):\n",
                "  model.train()\n",
                "  # let's slide over our dataset\n",
                "  for beg_t, end_t in zip(range(0, CHUNK_SIZE - SEQ_LEN - 1, SEQ_LEN + 1),\n",
                "                          range(SEQ_LEN + 1, CHUNK_SIZE, SEQ_LEN + 1)):\n",
                "    # Step 1. Remember that Pytorch accumulates gradients.\n",
                "    # We need to clear them out before each instance\n",
                "    optimizer.zero_grad()\n",
                "\n",
                "    dataX = []\n",
                "    dataY = []\n",
                "    # Step 2. Get our inputs ready for the network, that is, turn them into\n",
                "    # Tensors of one-hot sequences. \n",
                "    for sent in trainset:\n",
                "      # chunk the sentence\n",
                "      chunk = sent[beg_t:end_t]\n",
                "\n",
                "      # TODO: get X and Y with a shift of 1\n",
                "      X = ...    # remove last char\n",
                "      Y = ...    # remove first char\n",
                "\n",
                "      # convert each sequence to one-hots and labels respectively\n",
                "      X = prepare_sequence(X, char2idx)\n",
                "      Y = prepare_sequence(Y, char2idx, onehot=False)\n",
                "      dataX.append(X.unsqueeze(0)) # create batch-dim\n",
                "      dataY.append(Y.unsqueeze(0)) # create batch-dim\n",
                "    dataX = torch.cat(dataX, dim=0).to(device)\n",
                "    dataY = torch.cat(dataY, dim=0).to(device)\n",
                "\n",
                "    # Step 3. Run our forward pass.\n",
                "    # Forward through model and carry the previous state forward in time (statefulness)\n",
                "    y_, state = model(dataX, state)\n",
                "    # detach the previous state graph to not backprop gradients further than the BPTT span\n",
                "    state = (state[0].detach(), # detach c[t]\n",
                "             state[1].detach()) # detach h[t]\n",
                "\n",
                "    # Step 4. Compute the loss, gradients, and update the parameters by\n",
                "    #  calling optimizer.step()\n",
                "    loss = loss_function(y_, dataY.view(-1))\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    tr_loss.append(loss.item())\n",
                "  timer_end = timer()  \n",
                "  if (epoch + 1) % 50 == 0:\n",
                "    # Generate a seed sentence to play around\n",
                "    model.to('cpu')\n",
                "    print('-' * 30) \n",
                "    print(gen_text(model, 'They ', char2idx))\n",
                "    print('-' * 30)\n",
                "    model.to(device)\n",
                "    print('Finished epoch {} in {:.1f} s: loss: {:.6f}'.format(epoch + 1, \n",
                "                                                               timer_end - timer_beg,\n",
                "                                                               np.mean(tr_loss[-10:])))\n",
                "  timer_beg = timer()\n",
                "\n",
                "plt.plot(tr_loss)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('NLLLoss')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Iulcm9gPNhwK"
            },
            "source": [
                "Now that the generator of characters is trained, we can ask it to predict the rest of a sentence that begins with `Professor `:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(gen_text(model.to('cpu'), 'Professor ', char2idx, 1000))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lVRsFgHZa3f3"
            },
            "source": [
                "### References\n",
                "\n",
                "[1] https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3.7.10 ('trading')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.10"
        },
        "vscode": {
            "interpreter": {
                "hash": "148c72a8fa8931f1b4adec61e5c626da15d84fdba20a1a50eaf0317d3b0337d5"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
