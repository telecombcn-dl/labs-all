{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ci2vrgB9J3vf"
            },
            "source": [
                "# Terrassa Buildings Visual Search 2019\n",
                "\n",
                "Notebook created by [Eva Mohedano](https://www.youtube.com/watch?v=SsHohytl1NA&feature=emb_logo) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020).\n",
                "\n",
                "\n",
                "Related [slides](https://www.slideshare.net/xavigiro/d1l5-contentbased-image-retrieval-upc-2018-deep-learning-for-computer-vision) and [video](https://www.youtube.com/watch?v=UyEXEGevhZs) from [Deep Learning for Computer Vision UPC TelecomBCN 2018](https://telecombcn-dl.github.io/2018-dlcv/)\n",
                "\n",
                "![Eva Mohedano](https://telecombcn-dl.github.io/2018-dlcv/img/instructors/EvaMohedano.jpg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SmY2C87MaAp0"
            },
            "source": [
                "This is a code example to perform retrieval in the training partition provided in the [kaggle 2019 competition](https://www.kaggle.com/c/terrassa-buildings-2019/data).\n",
                "\n",
                "The notebook contains:\n",
                "\n",
                "\n",
                "1.   Making data accessible for colab\n",
                "2.   Setting pytorch Dataset\n",
                "1.   Feature extraction with pre-trained model\n",
                "2.   Ranking generation\n",
                "1.   Performance evaluation\n",
                "2.   Network fine-tuning\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "%pylab inline\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from PIL import Image\n",
                "from torchvision import transforms\n",
                "from torchvision.models import resnet50\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "from tqdm import tqdm\n",
                "\n",
                "import torch\n",
                "from torch import nn"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yj8_kdgLhLLY"
            },
            "source": [
                "##1) Making data accessible for colab\n",
                "\n",
                "First we get the data from the [kaggle 2019 competition](https://www.kaggle.com/c/terrassa-buildings-2019/data) and make it accessible for the colab notebook.\n",
                "\n",
                "For that, we have to upload the \"terrassa_buildings_2019\" folder to our googledrive.\n",
                "\n",
                "Then we mount our googledrive folder to the colab workspace (this will require user authentification):"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from google.colab import drive \n",
                "drive.mount('mydrive') "
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IjiDaDonck7J"
            },
            "source": [
                "Here we make sure we find the relevant data, in this example the data is located in the following path:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "! ls \"/content/mydrive/My Drive/terrassa_buildings_2019/\""
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5g1Kqa5Tiz_P"
            },
            "source": [
                "##2) Setting pytorch Dataset"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q9HQzaTUdygc"
            },
            "source": [
                "In this section we implement a pytorch [Dataset class](https://pytorch.org/docs/stable/data.html#) to load the images from the challenge.\n",
                "\n",
                "We'll implement the class *BuildingsDataset* that will return in the \"__getitem__\" method a loaded image, its class id, and the name of the filename. Also, we'll generate a training/validation partition of the data (80% training /20% partition).\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class BuildingsDataset(Dataset):\n",
                "  def __init__(self, mode='training', seed=1234, transform=None):\n",
                "    data = np.loadtxt(\"/content/mydrive/My Drive/terrassa_buildings_2019/queries.txt\", dtype='str')\n",
                "    path_images = \"/content/mydrive/My Drive/terrassa_buildings_2019/database-2061/database-2061/database-2061\"\n",
                "\n",
                "    image_id = data[1:,0]\n",
                "    class_name = data[1:,1]\n",
                "\n",
                "    # convert string of class names into int\n",
                "    unique_class_name = np.unique(class_name)\n",
                "    unique_class_name.sort()\n",
                "    dic_class={}\n",
                "    for i, name in enumerate(unique_class_name):\n",
                "      dic_class[name]=i\n",
                "\n",
                "    labels = np.array([dic_class[n] for n in class_name])\n",
                "\n",
                "    # generate partition 20/80%\n",
                "    idx = np.arange(labels.shape[0])\n",
                "    np.random.seed(seed)\n",
                "    np.random.shuffle(idx)\n",
                "\n",
                "\n",
                "    N = idx.shape[0]\n",
                "    N_train = int(0.8*N)\n",
                "\n",
                "    if mode=='training':\n",
                "      idx = idx[:N_train]\n",
                "    else:\n",
                "      idx = idx[N_train:]\n",
                "\n",
                "    # keep important info\n",
                "    self.path_images = path_images\n",
                "    self.image_id = image_id[idx]\n",
                "    self.labels = labels[idx]\n",
                "    self.dic_class = dic_class\n",
                "    self.transform = transform\n",
                "  \n",
                "  def __getitem__(self,index):\n",
                "    filename = os.path.join(self.path_images, self.image_id[index]+'.jpg')\n",
                "    label = self.labels[index]\n",
                "    name = self.image_id[index]\n",
                "\n",
                "    # load image\n",
                "    image = Image.open(filename)\n",
                "    image = image.convert('RGB')\n",
                "\n",
                "    # apply transforms\n",
                "    if self.transform is not None:\n",
                "      image = self.transform(image)\n",
                "\n",
                "    return image, label, name\n",
                "  \n",
                "  def __len__(self):\n",
                "    return self.labels.shape[0]\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "h2UvVQvngEP1"
            },
            "source": [
                "If we want to use a imagenet pre-trained model, we will have to pre-process the images do they are compatible with the network.\n",
                "Basically, we'll have to make sure that all images have a consisent shape (224,224) and are normalize according with the ImageNet statistics.\n",
                "\n",
                "The image transforms, using [torchvision](https://pytorch.org/docs/stable/torchvision/index.html) library are the following:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "image_transforms = transforms.Compose([\n",
                "            transforms.Resize(256),\n",
                "            transforms.CenterCrop(224),\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
                "                                     std=[0.229, 0.224, 0.225])\n",
                "])"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OZcpjqXEgYSp"
            },
            "source": [
                "##3) Feature extraction with pretrained model\n",
                "\n",
                "Now that we have the data ready to read, we use a pretrained model from torchvision. This library have a variety of already trained models, you can check them [here](https://pytorch.org/vision/stable/models.html).\n",
                "\n",
                "For the experiments, we select a resnet50 from the catalog:\n",
                "\n",
                "\n",
                "```\n",
                "from torchvision.models import resnet50\n",
                "```\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = resnet50(pretrained=True, progress=True)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5dX3PslekU5W"
            },
            "source": [
                "We'll extract features from the layer before the classification. For that, we have to remove the last fully connected layer. We can do that by creating a new sequential model that includes all the original layers of the pre-trained resnet50, except the last one:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "fx_model = nn.Sequential(*list(model.children())[:-1])\n",
                "fx_model.eval()\n",
                "fx_model.cuda()\n",
                "print(\"Pretrained resnet50 feature extractor\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hmWwfGvMlSCS"
            },
            "source": [
                "Some utility feature extraction functions:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def convert_to_numpy(tensor):\n",
                "  if tensor.is_cuda:\n",
                "    data = tensor.data.cpu().numpy()\n",
                "  else:\n",
                "    data = tensor.data.numpy()\n",
                "  return data\n",
                "\n",
                "def extract_features(dataloader, model):\n",
                "  '''\n",
                "\n",
                "  Functions that extract feature vectors for all the images contained in a \n",
                "  pytorch Dataloader.\n",
                "\n",
                "  args:\n",
                "    dataloader: torch.data.Dataloader object returning (images, labels, image_name)\n",
                "    model:feature extractor pytorch model\n",
                "  returns:\n",
                "    all_features: numpy array (n_sample, feat_dim)\n",
                "    all_labels: list (n_sample)\n",
                "    all_names: list (all_names)\n",
                "  '''\n",
                "  model.eval()\n",
                "\n",
                "  all_features=[]\n",
                "  all_labels=[]\n",
                "  all_names=[]\n",
                "\n",
                "  for i, batch in tqdm(enumerate(dataloader)):\n",
                "    x = batch[0]\n",
                "    x = x.cuda()\n",
                "    y = batch[1]\n",
                "    name = batch[2]\n",
                "\n",
                "    batch_feats = model.forward(x).squeeze()\n",
                "    batch_feats_np = convert_to_numpy(batch_feats)\n",
                "    batch_y_np = convert_to_numpy(y)\n",
                "\n",
                "    all_features.extend(batch_feats_np)\n",
                "    all_labels.extend(batch_y_np)\n",
                "    all_names.extend(name)\n",
                "  \n",
                "  all_features = np.array(all_features)\n",
                "  return all_features, all_labels, all_names"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3gR6ujPmmM5R"
            },
            "source": [
                "We'll create a [DataLoader](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) based on the Dataset partitions defined in our *BuildingsDataset* class that will allow us to create batches of data. In this example we use a batch size of 50 samples:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ds_train = BuildingsDataset('training', transform=image_transforms)\n",
                "ds_val = BuildingsDataset('validation', transform=image_transforms)\n",
                "\n",
                "dataloader ={\n",
                "  'training':DataLoader(ds_train,batch_size=50, shuffle=False, num_workers=5),\n",
                "  'validation':DataLoader(ds_val,batch_size=50, shuffle=False, num_workers=5)}"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vG5d2EjGnEQw"
            },
            "source": [
                "Now we compute the features for the training and validation sets:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "for mode  in ['training', 'validation']:\n",
                "  print(f\"Extracting features for {mode}\")\n",
                "  loader = dataloader[mode]\n",
                "  if mode=='training':\n",
                "    ds_features, ds_labels, ds_names = extract_features(loader, fx_model)\n",
                "  else:\n",
                "    query_features, query_labels, query_names = extract_features(loader, fx_model)\n",
                "\n",
                "\n",
                "print(\"\\nTraining dimensions: \", ds_features.shape)\n",
                "print(\"\\nValidation dimensions: \", query_features.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "B-9BO3q8nlBS"
            },
            "source": [
                "(We are using the validation set as query set so later we can evaluate the performance of the retrieval system)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NFgkN2kInwER"
            },
            "source": [
                "##4) Ranking generation\n",
                "\n",
                "Now that we have extracted features for our training and validation set (used as dataset and query partitions), we can generated a ranked list of the images in the training (dataset) for each image of the query (validation) set.\n",
                "\n",
                "We'll us pytorch tensors for the similarity ang rank generation:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ds_features = torch.Tensor(ds_features)\n",
                "ds_labels = torch.Tensor(ds_labels)\n",
                "query_features = torch.Tensor(query_features)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zM3Srr6dpOHh"
            },
            "source": [
                "The similarity score between two image representations  $\\textbf{x}$ and $\\textbf{y} $ \n",
                "is given by\n",
                "$$sim_{\\textbf{x},\\textbf{y}} = \\textbf{x}\\cdot \\textbf{y}^T $$\n",
                "\n",
                "However, if we want to obtain a score $\\in [0,1]$ we must l2-normalize each ima representation:\n",
                "\n",
                "$$ \\textbf{x}_{l_{2}norm} = \\frac{\\textbf{x}}{||\\textbf{x}||}$$\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def l2norm(tensor):\n",
                "  norm = torch.sqrt(torch.sum(tensor**2, axis=1))\n",
                "  return tensor/norm[:,None]\n",
                "\n",
                "def generate_rankings(ds_features, query_features):\n",
                "  '''\n",
                "\n",
                "  Function for ranking generation given dataset and query features.\n",
                "\n",
                "  args:\n",
                "    ds_features: pytorch tensor (n_dataset, feat_dim)\n",
                "    query_features: pytorch tensor (n_query, feat_dim)\n",
                "  returns:\n",
                "    ranks_id: sorted dataser indices (n_query,n_dataset)\n",
                "  '''\n",
                "\n",
                "  dataset = l2norm(ds_features)\n",
                "  queries = l2norm(query_features)\n",
                "\n",
                "  # similarities\n",
                "  similaritites = torch.mm(queries, dataset.t()).squeeze()\n",
                "\n",
                "  # rank from more to less similar\n",
                "  ranks_id = torch.argsort(similaritites, dim=-1, descending=True)\n",
                "  return ranks_id"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ranks_id_fx_model = generate_rankings(ds_features, query_features)\n",
                "ranks_id_fx_model.size()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RnnQXN2TqUuS"
            },
            "source": [
                "Qualitative results of 5 queries:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ds_query = BuildingsDataset('valitation', transform=None)\n",
                "ds_dataset = BuildingsDataset('training', transform=None)\n",
                "\n",
                "def show_rank(ds_query, ds_dataset,ranks_id, id_query=0):\n",
                "  figsize(20,20)\n",
                "  title(\"Top-10 mods_labelsst similar results\")\n",
                "  subplot(1,11,1), imshow(ds_query[id_query][0])\n",
                "  title('QUERY')\n",
                "  axis('off')\n",
                "\n",
                "\n",
                "  for i, id in enumerate(ranks_id[id_query][:10]): # We disply from the second result because the query is included in the DS\n",
                "    subplot(1,11,i+2), imshow(ds_dataset[id][0])\n",
                "    title(f'#{i+1}')\n",
                "    axis('off')\n",
                "    \n",
                "  show()\n",
                "\n",
                "for i in range(5):\n",
                "  idx_query=np.random.randint(0,len(ds_query))\n",
                "  show_rank(ds_query, ds_dataset,ranks_id_fx_model, id_query=idx_query)\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "EtJNctIHrsUb"
            },
            "source": [
                "##5) Performance evaluation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RemKaFgxrvSi"
            },
            "source": [
                "We will calculate the **mean Averace Precision (mAP)** of a set of queries given by\n",
                "$$ mAP = \\frac{1}{N}\\sum_{j}^{N}\\frac{1}{Q_{j}}\\sum_{i=1}^{Q_{j}}P(doc_{i})$$\n",
                "\n",
                "where $Q_{j}$ is the  number of relevant documents for query $j$, $N$ is number of queries, $P(doc_{i} )$ is the precision at $i$th relevant document. The example below ilustrates the Average Precision scores associated to the ranked retrieved results of two queries (Examples taken from [Simone Teufel lecture slides, Lecture 5: Information Retrieval](https://www.cl.cam.ac.uk/teaching/1415/InfoRtrv/lecture5.pdf))"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def precision_at_k(r, k):\n",
                "    \"\"\"Score is precision @ k\n",
                "    Relevance is binary (nonzero is relevant).\n",
                "    >>> r = [0, 0, 1]\n",
                "    >>> precision_at_k(r, 1)\n",
                "    0.0\n",
                "    >>> precision_at_k(r, 2)\n",
                "    0.0\n",
                "    >>> precision_at_k(r, 3)\n",
                "    0.33333333333333331\n",
                "    >>> precision_at_k(r, 4)\n",
                "    Traceback (most recent call last):\n",
                "        File \"<stdin>\", line 1, in ?\n",
                "    ValueError: Relevance score length < k\n",
                "    Args:\n",
                "        r: Relevance scores (list or numpy) in rank order\n",
                "            (first element is the first item)\n",
                "    Returns:\n",
                "        Precision @ k\n",
                "    Raises:\n",
                "        ValueError: len(r) must be >= k\n",
                "    \"\"\"\n",
                "    assert k >= 1\n",
                "    r = np.asarray(r)[:k] != 0\n",
                "    if r.size != k:\n",
                "        raise ValueError('Relevance score length < k')\n",
                "    return np.mean(r)\n",
                "\n",
                "def average_precision(r):\n",
                "    \"\"\"Score is average precision (area under PR curve)\n",
                "    Relevance is binary (nonzero is relevant).\n",
                "    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n",
                "    >>> delta_r = 1. / sum(r)\n",
                "    >>> sum([sum(r[:x + 1]) / (x + 1.) * delta_r for x, y in enumerate(r) if y])\n",
                "    0.7833333333333333\n",
                "    >>> average_precision(r)\n",
                "    0.78333333333333333\n",
                "    Args:\n",
                "        r: Relevance scores (list or numpy) in rank order\n",
                "            (first element is the first item)\n",
                "    Returns:\n",
                "        Average precision\n",
                "    \"\"\"\n",
                "    r = np.asarray(r) != 0\n",
                "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
                "    if not out:\n",
                "        return 0.\n",
                "    return np.mean(out)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def compute_map(ds_labels, query_labels, ranks_id):\n",
                "  ap_queries=[] \n",
                "  for i, label in enumerate(query_labels):\n",
                "    # show_rank(ds_query, ds_dataset,ranks_id, id_query=i)\n",
                "\n",
                "    labels_dataset_ranked = ds_labels[ranks_id[i]]\n",
                "    rank = torch.zeros_like(labels_dataset_ranked)\n",
                "    rank[labels_dataset_ranked==label]=1\n",
                "\n",
                "    rank = convert_to_numpy(rank)\n",
                "    ap=average_precision(rank)\n",
                "\n",
                "    ap_queries.append(ap)\n",
                "\n",
                "  return np.mean(ap_queries)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "mAp = compute_map(ds_labels, query_labels, ranks_id_fx_model)\n",
                "print(f\"Pre-trained features achieve {mAp} mAP\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "nPdwVZBOq-Uu"
            },
            "source": [
                "##6) Network fine-tuning"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = resnet50(pretrained=True, progress=True)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model.fc=nn.Linear(2048,13)\n",
                "model = model.cuda()\n",
                "print(\"modified resnet50 for Terrassa Buildings\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimizer = torch.optim.Adam(model.parameters())\n",
                "cross_entropy = nn.CrossEntropyLoss()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "ds_train = BuildingsDataset('training', transform=image_transforms)\n",
                "ds_val = BuildingsDataset('validation', transform=image_transforms)\n",
                "\n",
                "dataloader ={\n",
                "  'training':DataLoader(ds_train,batch_size=50, shuffle=True, num_workers=5), # note we randomize the order of the features!\n",
                "  'validation':DataLoader(ds_val,batch_size=50, shuffle=False, num_workers=5)}"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def compute_accuracy(pred, y):\n",
                "  id_pred = torch.argmax(nn.Softmax(dim=-1)(pred), dim=-1)\n",
                "  return float((id_pred==y).sum())/id_pred.size()[0]\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "total_epochs=0"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "for epoch in range(20):\n",
                "  for mode  in ['training', 'validation']:\n",
                "    loss_epoch=[]\n",
                "    acc_epoch=[]\n",
                "\n",
                "    if mode=='training':\n",
                "      model.train()\n",
                "      total_epochs+=1\n",
                "    else:\n",
                "      model.eval()\n",
                "\n",
                "    loader = dataloader[mode]\n",
                "\n",
                "    for i, (x,y,names) in enumerate(loader):\n",
                "      x=x.cuda()\n",
                "      y=y.cuda()\n",
                "      pred = model.forward(x)\n",
                "      loss = cross_entropy(pred, y.long())\n",
                "      acc = compute_accuracy(pred, y)\n",
                "\n",
                "      if mode=='training':\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "      # print(i, loss.item(), acc)\n",
                "      loss_epoch.append(loss.item())\n",
                "      acc_epoch.append(acc)\n",
                "    loss_epoch = np.mean(loss_epoch)\n",
                "    acc_epoch = np.mean(acc_epoch)\n",
                "    \n",
                "\n",
                "    print(f\"{total_epochs}, {mode}, {loss_epoch}, {acc_epoch}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# optionally save the model\n",
                "torch.save(model.state_dict(), 'trained_valacc_XXX.pt')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Pm_aDxPwsXib"
            },
            "source": [
                "###All retrieval pipeline with the fine-tuned model:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Feature extraction\n",
                "fx_model_finetuned = nn.Sequential(*list(model.children())[:-1])\n",
                "fx_model_finetuned.eval()\n",
                "fx_model_finetuned.cuda()\n",
                "print(\"pretrained fx\")\n",
                "\n",
                "for mode  in ['training', 'queries']:\n",
                "  print(f\"Extracting features for {mode}\")\n",
                "  ds = BuildingsDataset(mode, transform=image_transforms)\n",
                "  dataloader = DataLoader(ds,batch_size=50, shuffle=False, num_workers=5)\n",
                "  if mode=='training':\n",
                "    ds_features, ds_labels, ds_names = extract_features(dataloader, fx_model_finetuned)\n",
                "  else:\n",
                "    query_features, query_labels, query_names = extract_features(dataloader, fx_model_finetuned)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# conversion to torch\n",
                "ds_features = torch.Tensor(ds_features)\n",
                "ds_labels = torch.Tensor(ds_labels)\n",
                "query_features = torch.Tensor(query_features)\n",
                "\n",
                "# generate ranks\n",
                "ranks_id_finetuned = generate_rankings(ds_features, query_features)\n",
                "\n",
                "# quantitaive evaluation\n",
                "mAp = compute_map(ds_labels, query_labels, ranks_id_finetuned)\n",
                "print(f\"Fine-tuned features achieve {mAp} mAP\")"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
