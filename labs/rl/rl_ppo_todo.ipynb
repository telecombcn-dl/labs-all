{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CExziYkkFVl5"
            },
            "source": [
                "# **PROXIMAL POLICY OPTIMIZATION**\n",
                "\n",
                "**Lab exercise created by [Juan Jos\u00e9 Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/) for the [Postgraduate course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) at [UPC School](https://www.talent.upc.edu/ing/) (2021).**\n",
                "\n",
                "(This version is adapted for a short lab)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8X0JMdvPhCM8"
            },
            "source": [
                "# Installing dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install swig --quiet\n",
                "!pip install gym[box2d]==0.17.3 --quiet\n",
                "!pip install Box2D wandb --quiet\n",
                "\n",
                "# install utilities for rendering OpenAI Gym videos in Colab\n",
                "!apt-get -qq install -y xvfb x11-utils\n",
                "!sudo apt -qq install -y python3-opengl\n",
                "!pip install pyvirtualdisplay==0.2.* \\\n",
                "             PyOpenGL==3.1.* \\\n",
                "             PyOpenGL-accelerate==3.1.* \\\n",
                "             box2d-kengz \\\n",
                "             --quiet\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PzQyazSWhEtK"
            },
            "source": [
                "# Setting up the environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import base64\n",
                "import glob\n",
                "import io\n",
                "import os\n",
                "import math\n",
                "import timeit\n",
                "import warnings\n",
                "\n",
                "from IPython.display import HTML\n",
                "from IPython.display import display"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import wandb\n",
                "import random\n",
                "\n",
                "import numpy as np\n",
                "from random import randint\n",
                "from collections import namedtuple"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from torch.distributions import Categorical\n",
                "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# starting a fake screen in the background\n",
                "#  in order to render videos\n",
                "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
                "os.environ[\"DISPLAY\"] = \":1\"\n",
                "\n",
                "# utility to get video file from directory\n",
                "def get_video_filename(dir=\"video\"):\n",
                "  glob_mp4 = os.path.join(dir, \"*.mp4\") \n",
                "  mp4list = glob.glob(glob_mp4)\n",
                "  assert len(mp4list) > 0, \"couldnt find video files\"\n",
                "  return mp4list[-1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HqX5C4y9CqYI"
            },
            "source": [
                "# Random Agent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
                "\n",
                "env = gym.wrappers.RecordVideo(env, \"./video\")\n",
                "\n",
                "ob, _ = env.reset()\n",
                "done, total_rew = False, 0\n",
                "truncated = False\n",
                "\n",
                "while not (done or truncated):\n",
                "  env.render()\n",
                "  ac = env.action_space.sample()\n",
                "  ob, rew, done, truncated, info = env.step(ac)\n",
                "  total_rew += rew\n",
                "  \n",
                "print('Cumulative reward:', total_rew)\n",
                "  \n",
                "env.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PayBHx0o001d"
            },
            "source": [
                "# Visualize random policy in Wandb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "PROJECT = \"AIDL-DRL\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb.init(project=PROJECT)\n",
                "wandb.run.name = 'lunarlander_random_agent'\n",
                "mp4 = get_video_filename()\n",
                "wandb.log({\"Video eval\": wandb.Video(mp4, format=\"mp4\")})\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7IPb1a_T04rP"
            },
            "source": [
                "# Create the model\n",
                "PPO is an optimization algorithm that uses the actor-critic framework as in the previous lab. For this reason, we instantiate two branches, one for the action log probabilities and the other for estimating a state value."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Agent(nn.Module):\n",
                "    def __init__(self, obs_len, act_len):\n",
                "        super(Agent, self).__init__()\n",
                "        \n",
                "        self.obs_len = obs_len\n",
                "        self.act_len = act_len\n",
                "\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(obs_len, 64),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(64, 128),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "        self.actor = nn.Linear(128, act_len)\n",
                "        self.critic = nn.Linear(128, 1)\n",
                "\n",
                "\n",
                "    def forward(self, state):\n",
                "        out = self.mlp(state)\n",
                "        action_scores = self.actor(out)\n",
                "        state_value = self.critic(out)\n",
                "        return F.softmax(action_scores, dim=1), state_value\n",
                "\n",
                "    def compute_action(self, state):\n",
                "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
                "        probs, state_value = self(state)\n",
                "\n",
                "        m = torch.distributions.Categorical(probs)\n",
                "        action = m.sample()\n",
                "        \n",
                "        return action.item(), m.log_prob(action).item(), state_value.item()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LFjk_O0g2__r"
            },
            "source": [
                "# Replay memory\n",
                "Similar to DQN, now we can train again with old transitions that are going to be stored in a buffer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transition = np.dtype([('s', np.float64, (8,)), ('a', np.float64), ('a_logp', np.float64),\n",
                "                       ('r', np.float64), ('s_', np.float64, (8,))])\n",
                "\n",
                "\n",
                "class ReplayMemory():\n",
                "    def __init__(self, capacity):\n",
                "        self.buffer_capacity = capacity\n",
                "        self.buffer = np.empty(capacity, dtype=transition)\n",
                "        self.counter = 0\n",
                "\n",
                "    # Stores a transition and returns True or False depending on whether the buffer is full or not\n",
                "    def store(self, transition):\n",
                "        self.buffer[self.counter] = transition\n",
                "        self.counter += 1\n",
                "        if self.counter == self.buffer_capacity:\n",
                "            self.counter = 0\n",
                "            return True\n",
                "        else:\n",
                "            return False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_returns_and_advantages(rewards, values, gamma):\n",
                "    # Initializing returns and advantages tensors\n",
                "    returns = torch.zeros_like(rewards)\n",
                "    advantages = torch.zeros_like(rewards)\n",
                "    \n",
                "    # Initializing the variable for the next value\n",
                "    next_value = 0\n",
                "    next_advantage = 0\n",
                "\n",
                "    for t in reversed(range(len(rewards))):\n",
                "        # Compute returns: G_t = R_t + gamma * G_{t+1}\n",
                "        returns[t] = rewards[t] + gamma * next_value\n",
                "        next_value = returns[t]\n",
                "\n",
                "        advantages[t] = returns[t] - values[t]\n",
                "\n",
                "    # Normalize advantages and returns\n",
                "    returns = (returns - returns.mean()) / (returns.std() + 1e-10)\n",
                "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
                "\n",
                "    return returns, advantages"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7n-ck5fN30m6"
            },
            "source": [
                "**Exercise 1: Compute the ratio between the new and the old policy.**\n",
                "\n",
                "**Exercise 2: Modify the surrogate objective by clipping the probability ratio.**\n",
                "\n",
                "**Exercise 3: Compute the loss function weighting each term correspondingly.**\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(policy, optimizer, memory, hparams):\n",
                "\n",
                "    gamma = hparams['gamma']\n",
                "    ppo_epoch = hparams['ppo_epoch']\n",
                "    batch_size = hparams['batch_size']\n",
                "    clip_param = hparams['clip_param']\n",
                "    c1 = hparams['c1']\n",
                "    c2 = hparams['c2']\n",
                "\n",
                "\n",
                "    s = torch.tensor(memory.buffer['s'], dtype=torch.float)\n",
                "    a = torch.tensor(memory.buffer['a'], dtype=torch.float)\n",
                "    r = torch.tensor(memory.buffer['r'], dtype=torch.float).view(-1, 1)\n",
                "    s_ = torch.tensor(memory.buffer['s_'], dtype=torch.float)\n",
                "\n",
                "    old_a_logp = torch.tensor(memory.buffer['a_logp'], dtype=torch.float).view(-1, 1)\n",
                "\n",
                "\n",
                "    with torch.no_grad():\n",
                "        value_pred = policy(s)[1]\n",
                "        returns, advantages = compute_returns_and_advantages(r, value_pred, gamma)\n",
                "\n",
                "    for _ in range(ppo_epoch):\n",
                "        probs, _ = policy(s)\n",
                "        dist = Categorical(probs)\n",
                "        entropy = dist.entropy()\n",
                "        \n",
                "        a_logp = dist.log_prob(a).unsqueeze(dim=1)\n",
                "\n",
                "        # TODO: Compute ratio. Hint: pi/pi_old = e^(ln pi - ln pi_old)\n",
                "        ratio = ...\n",
                "\n",
                "        surr1 = ratio * advantages\n",
                "\n",
                "        # TODO: Modify the surrogate objective by clipping the probability ratio.\n",
                "        # Hint: Use the torch.clamp function and the clip_param variable\n",
                "        surr2 = TODO * advantages\n",
                "\n",
                "        policy_loss = torch.min(surr1, surr2).mean()\n",
                "        value_loss = F.smooth_l1_loss(policy(s)[1], returns)\n",
                "        entropy = entropy.mean()\n",
                "\n",
                "        # TODO: Compute the loss function weighting each term correspondingly.\n",
                "        # Take into account the needed signs for each term!\n",
                "        loss = ...\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "\n",
                "    return -policy_loss.item(), value_loss.item(), entropy.item(), ratio.mean().item()\n",
                "\n",
                "def test(env, policy, render=False):\n",
                "    state, _ = env.reset()\n",
                "    done, ep_reward = False, 0\n",
                "    truncated = False\n",
                "    while not (done or truncated):\n",
                "        env.render()\n",
                "        action, _, _ = policy.compute_action(state)\n",
                "        state, reward, done, truncated, _ = env.step(action)\n",
                "        ep_reward += reward\n",
                "\n",
                "    env.close()\n",
                "    mp4 = get_video_filename()\n",
                "    wandb.log({\"Video eval\": wandb.Video(mp4, format=\"mp4\")})\n",
                "    return ep_reward"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0TPjGR9HNNXl"
            },
            "source": [
                "## Hyperparameters\n",
                "\n",
                "These values were found running a sweep over some hyperparameters.\n",
                "\n",
                "In this [Colab](https://colab.research.google.com/drive/1A3yA_jAiPKi3H7YDSYJekl64Vz7JxhDi?usp=sharing) you will find the code used to execute it. And in this [report](https://wandb.ai/juanjo3ns/lunar-lander/reports/Hyperparameter-Sweeping-in-PPO-LunarLander--Vmlldzo3NjA0MDY?accessToken=ibdz6huvu28hl5wv53szeurltq481riu5wexihxqek645b1ymr27jilpo5xili4y) you can see the results of these runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hparams = {\n",
                "    'gamma' : 0.99,\n",
                "    'log_interval' : 100,\n",
                "    'num_episodes': 2000,\n",
                "    'lr' : 1e-3,\n",
                "    'clip_param': 0.1,\n",
                "    'ppo_epoch': 10,\n",
                "    'replay_size': 930,\n",
                "    'batch_size': 128,\n",
                "    'c1': 1.,\n",
                "    'c2': 0.021\n",
                "}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create environment\n",
                "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get number of actions from gym action space\n",
                "n_inputs = env.observation_space.shape[0]\n",
                "n_actions = env.action_space.n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fix random seed (for reproducibility)\n",
                "seed=0\n",
                "random.seed(seed)\n",
                "torch.manual_seed(seed)\n",
                "torch.cuda.manual_seed(seed)\n",
                "\n",
                "# Initialize wandb run\n",
                "wandb.finish() # execute to avoid overlapping runnings (advice: later remove duplicates in wandb)\n",
                "wandb.init(project=PROJECT, config=hparams)\n",
                "wandb.run.name = 'ppo_lunarlander_train_0'\n",
                "\n",
                "\n",
                "# Create policy and optimizer\n",
                "policy = Agent(n_inputs, n_actions)\n",
                "optimizer = torch.optim.Adam(policy.parameters(), lr=hparams['lr'])\n",
                "\n",
                "eps = np.finfo(np.float32).eps.item()\n",
                "memory = ReplayMemory(hparams['replay_size'])\n",
                "\n",
                "# Training loop\n",
                "print(\"Target reward: {}\".format(env.spec.reward_threshold))\n",
                "running_reward = -100\n",
                "ep_rew_history_reinforce = []\n",
                "for i_episode in range(hparams['num_episodes']):\n",
                "    # Collect experience\n",
                "    state, _ = env.reset()\n",
                "    ep_reward = 0\n",
                "    done, truncated = False, False\n",
                "\n",
                "    while not (done or truncated):  # Don't infinite loop while learning\n",
                "        action, a_logp, state_value = policy.compute_action(state)\n",
                "        next_state, reward, done, truncated, _ = env.step(action)\n",
                "        \n",
                "\n",
                "        if memory.store((state, action, a_logp, reward, next_state)):\n",
                "            policy_loss, value_loss, avg_entropy, ratio = train(policy, optimizer, memory, hparams)\n",
                "            wandb.log(\n",
                "                {\n",
                "                'policy_loss': policy_loss,\n",
                "                'value_loss': value_loss,\n",
                "                'running_reward': running_reward,\n",
                "                'mean_entropy': avg_entropy,\n",
                "                'ratio': ratio\n",
                "                })\n",
                "\n",
                "\n",
                "        state = next_state\n",
                "\n",
                "        ep_reward += reward\n",
                "        if done or truncated:\n",
                "            break\n",
                "\n",
                "    # Update running reward\n",
                "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
                "    \n",
                "    \n",
                "    ep_rew_history_reinforce.append((i_episode, ep_reward))\n",
                "    if i_episode % hparams['log_interval'] == 0:\n",
                "        print(f'Episode {i_episode}\\tLast reward: {ep_reward:.2f}\\tAverage reward: {running_reward:.2f}')\n",
                "        test_env = gym.wrappers.RecordVideo(env, \"./video\")\n",
                "        ep_reward = test(test_env, policy)\n",
                "\n",
                "    if running_reward > env.spec.reward_threshold:\n",
                "        print(\"Solved!\")\n",
                "        break\n",
                "\n",
                "print(f\"Finished training! Running reward is now {running_reward}\")\n",
                "test_env = gym.wrappers.RecordVideo(env, \"./video\")\n",
                "ep_reward = test(test_env, policy)\n",
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "0b51f2839ec74772bc44e2c11dd8588c": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "372b059dc007487887484f392a725abb": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "757676fed1f5413c8261e8145dc16e60": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "7d27c81c6e7b41f49d579074d3f7a3de": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "LabelModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "LabelModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "LabelView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_b44a6763240548a68cfe2fe9eb341e07",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_0b51f2839ec74772bc44e2c11dd8588c",
                        "value": " 0.03MB of 0.03MB uploaded (0.00MB deduped)\r"
                    }
                },
                "b44a6763240548a68cfe2fe9eb341e07": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "c4a42c5d79bb43bd97c06e54b9e7eeef": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_fae23cb6f07141b4b5e7acfcd56af391",
                        "max": 1,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_372b059dc007487887484f392a725abb",
                        "value": 1
                    }
                },
                "da1896caf79f471595f4b82da08fc8f2": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "VBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "VBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "VBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_7d27c81c6e7b41f49d579074d3f7a3de",
                            "IPY_MODEL_c4a42c5d79bb43bd97c06e54b9e7eeef"
                        ],
                        "layout": "IPY_MODEL_757676fed1f5413c8261e8145dc16e60"
                    }
                },
                "fae23cb6f07141b4b5e7acfcd56af391": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
