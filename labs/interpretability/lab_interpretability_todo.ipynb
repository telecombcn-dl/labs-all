{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "j3_tzzpZ3zce"
            },
            "source": [
                "# Interpretability of a Convolutional Neural Network\n",
                "\n",
                "Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) ([UPC School](https://www.talent.upc.edu/ing/), 2019). Updated by [Albert Mosella-Montoro](https://www.albertmosellamontoro.com/) in 2020 and for [Paula G. Duran](https://www.linkedin.com/in/paulagd-1995/) in 2022.\n",
                "\n",
                "Minor contributions by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) and [Pol Caselles](https://www.linkedin.com/in/pcaselles/) during 2022.\n",
                "\n",
                "Based on previous versions by [Amaia Salvador](https://www.linkedin.com/in/amaiasalvador/) ([Persontyle](https://github.com/telecombcn-dl/2017-persontyle), 2017) and [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) ([Barcelona Technology School](https://barcelonatechnologyschool.com/), 2019)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JwdaFL7Lx1Vc"
            },
            "source": [
                "In this notebook we will first train a simple model on [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and then we will perform some visualizations to understand it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import copy\n",
                "import time\n",
                "import itertools\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "import torch.optim as optim\n",
                "\n",
                "import matplotlib.pyplot as plt  \n",
                "%matplotlib inline\n",
                "\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "\n",
                "from scipy.ndimage.interpolation import zoom\n",
                "from scipy.ndimage.filters import gaussian_filter\n",
                "import copy\n",
                "\n",
                "from skimage import io\n",
                "import cv2\n",
                "import skimage.transform\n",
                "import os"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's define some hyper-parameters\n",
                "hparams = {\n",
                "    'sz':56,                    # size of the original noise image\n",
                "    'upscaling_steps':12,       # how many upscaling are going to be done\n",
                "    'upscaling_factor': 1.2,    # scale factor applied in each step\n",
                "    'lr':0.001,                 # learning rate\n",
                "    'opt_steps': 20,            # number of iterations on each up_scaling step\n",
                "    'blur': 5,                  # parameter applied to smooth the image after an upscaling step\n",
                "    'layer': 40,\n",
                "    'filter': 265,\n",
                "    'epochs': 5\n",
                "}\n",
                "\n",
                "# we select to work on GPU if it is available in the machine, otherwise\n",
                "# will run on CPU\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "pBsfbtZROCs7"
            },
            "source": [
                "## Load data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = transforms.Compose(\n",
                "    [transforms.ToTensor(),\n",
                "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
                "\n",
                "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
                "                                        download=True, transform=transform)\n",
                "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
                "                                          shuffle=True, num_workers=2)\n",
                "\n",
                "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
                "                                       download=True, transform=transform)\n",
                "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
                "                                         shuffle=False, num_workers=2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jUB2U4CQj6Db"
            },
            "source": [
                "## Load model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "M_W6lJSmhFTF"
            },
            "source": [
                "\n",
                "**Exercise 1**: Complete the forward method of the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Net(nn.Module):\n",
                "    def __init__(self) -> None:\n",
                "        super().__init__()\n",
                "        self.conv_layers = nn.Sequential(\n",
                "            \n",
                "            nn.Conv2d(3, 32, 3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.Conv2d(32, 32, 3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool2d(2, 2),\n",
                "            nn.Dropout(0.5),\n",
                "\n",
                "            nn.Conv2d(32, 64, 3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.Conv2d(64, 64, 3, padding=1),\n",
                "            nn.ReLU(),\n",
                "            nn.MaxPool2d(2, 2),\n",
                "            nn.Dropout(0.5),\n",
                "        )\n",
                "\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(8 * 8 * 64, 128),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(128, 10),\n",
                "            nn.LogSoftmax(-1),\n",
                "        )\n",
                "\n",
                "    def forward(self,\n",
                "                x: torch.Tensor) -> torch.Tensor:\n",
                "        # TODO\n",
                "        ...\n",
                "\n",
                "model = Net()\n",
                "model.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VPaLbmbtkDVJ"
            },
            "source": [
                "## Train the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = F.nll_loss\n",
                "optimizer = optim.Adam(model.parameters(), lr=hparams['lr'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = time.time()\n",
                "model.train()\n",
                "for epoch in range(hparams['epochs']):  # loop over the dataset multiple times\n",
                "\n",
                "    for i, (inputs, labels) in enumerate(trainloader):\n",
                "        # get the inputs to gpu; data is a list of [inputs, labels]\n",
                "        inputs, labels = inputs.to(device), labels.to(device)\n",
                "\n",
                "        # forward + backward + optimize\n",
                "        outputs = model(inputs)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # print statistics\n",
                "        if i % 300 == 299:    # print every 300 mini-batches\n",
                "            print(f\"Epoch {epoch+1}/{hparams['epochs']} [{i+1}/{len(trainloader)}] loss: {loss.item():.2f}\")\n",
                "\n",
                "print('Finished Training')\n",
                "print(f\"Time: {(time.time() - t):.1f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "slYyk2A6kceM"
            },
            "source": [
                "## Understanding the model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Rf07x9cIhPCi"
            },
            "source": [
                "We will disable the gradient computation for the rest of the session as we will not need to train anymore the model. To perform inference (evaluation) we do not need this option since we will not be doing backpropagation and so we do not need the model to perform variable's gradient."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Disable gradients computation\n",
                "_ = torch.set_grad_enabled(False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IOtCqY0ciEk2"
            },
            "source": [
                "Now we perform inference and we get the model's accuracy:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correct = 0\n",
                "total = 0\n",
                "model.eval()\n",
                "for inputs, labels in testloader:\n",
                "    inputs, labels = inputs.to(device), labels.to(device)\n",
                "    outputs = model(inputs)\n",
                "    _, predicted = torch.max(outputs.data, 1)\n",
                "    total += labels.size(0)\n",
                "    correct += (predicted == labels).sum().item()\n",
                "\n",
                "print(f\"Accuracy of the network on the 10000 test images: {correct / total*100:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AtrWW296U68j"
            },
            "source": [
                "**Exercise 2:** \n",
                "\n",
                "Now we get the weights from the first layer and we print their shape. Is this the shape you expected? What does each number stand for?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# get the weights from the first layer to cpu\n",
                "weights = model.conv_layers[0].weight.cpu().detach() # Detach, since we do not need the gradients\n",
                "weights.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "a4dWzRpPWplb"
            },
            "source": [
                "---\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4CgY72U0l4Jq"
            },
            "source": [
                "Now we will create a function to display the convolution kernels as if they were image. This can give us insight to know what patterns activate each filter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_filters(weights: torch.Tensor) -> None:\n",
                "    N = int(np.ceil(np.sqrt(weights.shape[0])))\n",
                "    f, axarr = plt.subplots(N, N)\n",
                "\n",
                "    p = 0\n",
                "    for i in range(N):\n",
                "        for j in range(N):\n",
                "            # empty plot white when out of kernels to display\n",
                "            if p >= weights.shape[0]:\n",
                "                krnl = torch.ones((weights.shape[2], weights.shape[3], 3))\n",
                "            else:\n",
                "                if weights.shape[1] == 1: \n",
                "                    krnl = weights[p, :, :, :].permute(1, 2, 0)\n",
                "                    axarr[i, j].imshow(krnl)\n",
                "                else:\n",
                "                    # We just display the first channel of each filter\n",
                "                    krnl = weights[p,0,:,:]\n",
                "                    axarr[i, j].imshow(krnl, cmap='gray')\n",
                "            axarr[i, j].axis('off')\n",
                "            p += 1\n",
                "    plt.show()\n",
                "\n",
                "display_filters(weights)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "nOiig_90OCtR"
            },
            "source": [
                "Do these visualizations help you understand what the model learned? If not, later you can try to visualize the activations when these filters are convolved with the image. For now, let's move on to other visualization methods, but you can come back to this later."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "ZU8oG0H0OCtS"
            },
            "source": [
                "**Exercise 3:** Visualize filters in the second convolutional layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO\n",
                "...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "eRuspKvcOCtV"
            },
            "source": [
                "## Activations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "EQ1BALOLOCtW"
            },
            "source": [
                "We can also use the model's activations fit into our data samples in order to understand what the model learnt. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "Wpg4m27fOCta"
            },
            "source": [
                "\n",
                "\n",
                "**Exercise 4:** \n",
                "\n",
                "We want to create a model that works as a feature extractor. For that, we need to pick just those layers that are useful. Please, complete the Feature Extractor which should output the results from the first Linear layer (before the corresponding activation function).  \n",
                "\n",
                "- *Hint n\u00ba1*: You can call the layers of the original model by doing `self.model.conv_layers(...)`.  \n",
                "- *Hint n\u00ba2*: You can get specific layers from a Sequential module by indexing (consider it a list): `self.model.mlp[...](...)`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Extractor(nn.Module):\n",
                "    def __init__(self,\n",
                "                 model: torch.nn.Module) -> None:\n",
                "        super().__init__()\n",
                "        self.model = model\n",
                "        \n",
                "    def forward(self,\n",
                "                x: torch.Tensor) -> torch.Tensor:\n",
                "        # TODO: Give the output of the first Linear layer.\n",
                "        ...\n",
                "\n",
                "extractor = Extractor(model).to(device).eval()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "5SpRQEAROCtc"
            },
            "source": [
                "Once we have our extractor, we can load the data and forward it through the network to get the activations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feats = []\n",
                "for data in testloader:\n",
                "    inputs, _ = data[0].to(device), data[1].to(device)\n",
                "    feats.append(extractor(inputs).cpu().detach().numpy())\n",
                "\n",
                "feats = np.concatenate(feats)\n",
                "feats.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "IrtfrserOCtf"
            },
            "source": [
                "Once we have extracted activations for all samples in our test set, we will use different visualization tools to understand what the model learned."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "WmLyvVJbnDdf"
            },
            "source": [
                "## Visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "fzuY701lOCtg"
            },
            "source": [
                "### Finding per unit top K samples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "fdl0LUp9OCtg"
            },
            "source": [
                "Let's now find the K images with highest activation for each neuron in the layer, using the original extracted activations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "testimages = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
                "\n",
                "K = 10\n",
                "idxs_top10 = np.argsort(feats, axis=0)[::-1][0:K, :]\n",
                "picked_samples = np.zeros((K, 128, 32, 32, 3), dtype=float)\n",
                "for i in range(idxs_top10.shape[0]):\n",
                "    for j in range(idxs_top10.shape[1]):\n",
                "        picked_samples[i, j, :, :, :] = np.asarray(testimages[idxs_top10[i, j]][0])/255\n",
                "picked_samples.shape\n",
                "# The shape of the tensor corresponds to: \n",
                "# (n_images,n_units,nb_rows,nb_cols,nb_channels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "S2CQfk0xOCtj"
            },
            "source": [
                "```picked_samples``` now contains the 10 images with highest activation for each neuron. Let's visualize these images for some neurons:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "XcD-dvkEOCtj"
            },
            "source": [
                "**Exercise 5:**\n",
                "\n",
                "The following array defines which units are selected to display their top K images. Try different units to display their top K images. Do all units respond to distinguishable concepts? Are there units that respond to similar things?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "units = [2, 4, 14, 23, 29, 17]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "nunits = len(units)\n",
                "ims = picked_samples[:, units, :, :].squeeze()\n",
                "\n",
                "def vis_topk(ims: np.ndarray,\n",
                "             units: list) -> None:\n",
                "    f, axarr = plt.subplots(ims.shape[0],ims.shape[1],figsize=(10,10))\n",
                "    \n",
                "    for i in range(ims.shape[0]):\n",
                "        for j in range(ims.shape[1]):\n",
                "\n",
                "            axarr[i,j].imshow(ims[i,j,:,:,:])\n",
                "            axarr[i,j].axis('off')\n",
                "            axarr[0,j].set_title('unit '+ str(units[j]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "vis_topk(ims, units)\n",
                "ims.shape\n",
                "# The shape of the tensor corresponds to: \n",
                "#(n_ims,n_units_picked,nb_rows,nb_cols,nb_channels)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "mjLwlxRDOCtn"
            },
            "source": [
                "Did you find any units with semantic meaning? You can try for different units and see what images they like the most."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "HXqR-aIjOCtn"
            },
            "source": [
                "### Occlusion experiment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "m7Rvv_KQOCtn"
            },
            "source": [
                "Now, can we find what image parts contribute to the activation the most? Let's create a NxN occluder and slide it through each image with a stride of 2, and feed each occluded image through the network. Then, we can obtain the difference between the activations between the original image and the occluded ones, and create a difference map that we can use as a mask on top of the image. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def occ_images(ims: np.ndarray,\n",
                "               occ: tuple=(11, 11),\n",
                "               stride: int=4) -> np.ndarray:\n",
                "    \n",
                "    # Reshape to put top images for all units stacked together\n",
                "    ims = np.rollaxis(ims, 1, 0)\n",
                "    ims = np.reshape(ims, (ims.shape[0]*ims.shape[1], ims.shape[2], ims.shape[3], ims.shape[4]))\n",
                "    ims_acc = ims\n",
                "    \n",
                "    # slide \n",
                "    npos = 1\n",
                "    st = int(np.floor(occ[0]/2))\n",
                "    \n",
                "    # slide occluder, set pixels to 0 and stack to matrix\n",
                "    for i in range(st, ims.shape[1], stride):\n",
                "        for j in range(st, ims.shape[2], stride):\n",
                "            ims_occ = copy.deepcopy(ims)\n",
                "            ims_occ[:, i-st:i+occ[0]-st, j-st:j+occ[1]-st, :] = 0\n",
                "            ims_acc = np.vstack((ims_acc, ims_occ))\n",
                "            npos += 1\n",
                "            \n",
                "    return ims_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ims_acc = occ_images(ims)\n",
                "print(ims_acc.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "feAZWiU6OCtp"
            },
            "source": [
                "Let's visualize some of the images with the occluded region in different positions:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "f, axarr = plt.subplots(10, 10, figsize=(10, 10))\n",
                "ims_acc_r = ims_acc.reshape(ims_acc.shape[0]//(ims.shape[0]*ims.shape[1]),\n",
                "                                ims.shape[1], ims.shape[0],\n",
                "                                ims_acc.shape[1], ims_acc.shape[2], ims_acc.shape[3])\n",
                "for i in range(10):\n",
                "    for j in range(10):\n",
                "        axarr[i,j].imshow(ims_acc_r[i,0,j,:,:,:])\n",
                "        axarr[i,j].axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "78O7Buq7OCtr"
            },
            "source": [
                "We should pick an occluder that is large enough to cover significant parts of objects. 11x11 is the default one, but you can experiment with other sizes and see their effect."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "Ey_G8l56OCtr"
            },
            "source": [
                "```ims_occ``` contains all images with the occluder set at different positions. Let's run these through our extractor:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ims_acc_tensor = torch.tensor(ims_acc, dtype=torch.float).permute(0, 3, 1, 2).contiguous().to(device)\n",
                "ims_acc_tensor = (ims_acc_tensor-0.5)/0.5  # Normalize\n",
                "output = extractor(ims_acc_tensor)\n",
                "feats_occ = output.cpu().detach().numpy()\n",
                "feats_occ.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "r4yB4yq8OCtt"
            },
            "source": [
                "Now that we have the features, we can compute the difference between the original activation and the activation for each of the occluded images:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feats_r = np.reshape(feats_occ,(feats_occ.shape[0] // (ims.shape[0] * ims.shape[1]),\n",
                "                                ims.shape[1], ims.shape[0], feats_occ.shape[1]))\n",
                "\n",
                "distances = feats_r[0] - feats_r[1:] # original activation minus all the occluded ones\n",
                "distances = np.rollaxis(distances, 0, 4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "Es1fCNVJOCtu"
            },
            "source": [
                "Reshaping the distance array into a 2D map will give a mask that we can display on top of the images:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "s = int(np.sqrt(distances.shape[3]))\n",
                "\n",
                "heatmaps = np.zeros((distances.shape[0],distances.shape[1],distances.shape[3]))\n",
                "for i in range(len(units)):    \n",
                "    heatmaps[i] = distances[i,:,units[i],:]\n",
                "heatmaps.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "uTfU0CfoOCtw"
            },
            "source": [
                "Let's write a function to display the masks on top of the images:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def vis_occ(ims: np.ndarray,\n",
                "            heatmaps: np.ndarray,\n",
                "            units: list,\n",
                "            th: float=0.5,\n",
                "            sig: int=2) -> None:\n",
                "        \n",
                "    ims = np.rollaxis(ims,1,0)\n",
                "    \n",
                "    s = int(np.sqrt(heatmaps.shape[2]))\n",
                "    heatmaps = np.reshape(heatmaps,(heatmaps.shape[0],heatmaps.shape[1],s,s))\n",
                "    \n",
                "    f, axarr = plt.subplots(ims.shape[1],ims.shape[0],figsize=(10,10))\n",
                "    \n",
                "    for i in range(ims.shape[0]):\n",
                "        for j in range(ims.shape[1]):\n",
                "            \n",
                "            im = copy.deepcopy(ims[i,j,:,:,:])\n",
                "            mask = copy.deepcopy(heatmaps[i,j,:,:])\n",
                "            if not mask.max() == mask.min():\n",
                "                # Normalize mask\n",
                "                mask = (mask - np.min(mask))/(np.max(mask)-np.min(mask))\n",
                "                # Resize to image size\n",
                "                mask = zoom(mask,float(im.shape[0])/heatmaps.shape[-1],order=1)\n",
                "                # Apply gaussian to smooth output\n",
                "                mask = gaussian_filter(mask,sigma=sig)\n",
                "                # threshold to obtain mask out of heatmap\n",
                "                mask[mask>=th] = 1\n",
                "                mask[mask<th] = 0.3\n",
                "            else:\n",
                "                # No heatmap\n",
                "                mask = zoom(mask,float(im.shape[0])/heatmaps.shape[-1],order=1)\n",
                "                mask = np.ones_like(mask)*0.3\n",
                "            \n",
                "            # Mask all image channels\n",
                "            for c in range(3):\n",
                "                im[:,:,c] = im[:,:,c]*mask\n",
                "                \n",
                "            axarr[j,i].imshow(im)\n",
                "            axarr[j,i].axis('off')\n",
                "            axarr[0,i].set_title('unit '+ str(units[i]))\n",
                "            \n",
                "vis_occ(ims, heatmaps, units, th=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "snbVm8uJOCty"
            },
            "source": [
                "**Exercise 6**:\n",
                "\n",
                "The obtained masks are of course not perfect, but we get to see what parts of the image are most relevant for each unit in the layer. Are these masks what you expected? Do the picked neurons maximally respond to what you have previously guessed? If you see some fully greyed out images, do they correspond to neurons without semantical meaning?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "LHiA6x3tOCty"
            },
            "source": [
                "### Additional: t-SNE"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "PqJSrkOsOCty"
            },
            "source": [
                "Here we will display our learned features in a 2D space using t-SNE. To do this, we will use the provided function in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). We will also reduce the dimensionality with PCA before running t-SNE to make it faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "t = time.time()\n",
                "\n",
                "# should do more iterations, but let's do the minimum due to time constraints\n",
                "n_iter = 800\n",
                "tsne = TSNE(init='pca', n_components=2, random_state=0, n_iter=n_iter)\n",
                "feats_2d = tsne.fit_transform(feats)\n",
                "\n",
                "print(f\"Time: {(time.time() - t):.1f}s\")\n",
                "feats_2d.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "3K13s3MPOCt2"
            },
            "source": [
                "Once we have our 2D features, we can display them with their class labels, to see if the learned features are discriminative enough."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cifar_labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
                "# 0: airplane\n",
                "# 1: automobile\n",
                "# 2: bird\n",
                "# 3: cat\n",
                "# 4: deer\n",
                "# 5: dog\n",
                "# 6: frog\n",
                "# 7: horse\n",
                "# 8: ship\n",
                "# 9: truck\n",
                "labels = [y for _, y in testimages]\n",
                "\n",
                "fig = plt.figure(figsize=(12, 10))\n",
                "plt.scatter(feats_2d[:,0], feats_2d[:,1], c=labels, cmap=plt.cm.get_cmap(\"jet\", 10), s=10)\n",
                "plt.clim(-0.5, 9.5)\n",
                "cbar = plt.colorbar(ticks=range(10))\n",
                "cbar.ax.set_yticklabels(cifar_labels);\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": true,
                "editable": true,
                "id": "YpFxGI70OCt3"
            },
            "source": [
                "**Exercise 7:** What categories seem to be easier for our model? Which ones are confusing?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uxXSHJ7lFAKm"
            },
            "source": [
                "\n",
                "## Optional: Synthesizing images to maximize activations\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9tLSVzVsuFaO"
            },
            "source": [
                "Here we will synthesize an image that maximizes the activation of a particular filter. We will be able to know what kind of pattern significantly activates a particular feature map. \n",
                "\n",
                "**Resources**\n",
                "\n",
                "Erhan, Dumitru & Bengio, Y. & Courville, Aaron & Vincent, Pascal. (2009). Visualizing Higher-Layer Features of a Deep Network. Technical Report, Univerist\u00e9 de Montr\u00e9al. \n",
                "\n",
                "J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson, Understanding neural networks through deep visualization, ICML DL workshop, 2015\n",
                "\n",
                "https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enable gradients computation\n",
                "_ = torch.set_grad_enabled(True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SxMlEmdPxYtV"
            },
            "source": [
                "A hook class that will help us to obtain the activation map of a certain layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SaveFeatures():\n",
                "    def __init__(self,\n",
                "                 module: torch.nn.modules.conv.Conv2d) -> None:\n",
                "        self.hook = module.register_forward_hook(self.hook_fn)\n",
                "    def hook_fn(self,\n",
                "                module: torch.nn.modules.conv.Conv2d,\n",
                "                input: torch.Tensor,\n",
                "                output: torch.Tensor) -> None:\n",
                "        self.features = output\n",
                "\n",
                "    def close(self) -> None:\n",
                "        self.hook.remove()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ADVFiQLLxntD"
            },
            "source": [
                "Helper function to visualize properly the results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_image(img: np.ndarray,\n",
                "                  title: str='',\n",
                "                  size: bool=False) -> None:\n",
                "  h = plt.imshow(img, interpolation='none')\n",
                "  if size:\n",
                "    dpi = h.figure.get_dpi()/size\n",
                "    h.figure.set_figwidth(img.shape[1] / dpi)\n",
                "    h.figure.set_figheight(img.shape[0] / dpi)\n",
                "    # h.figure.canvas.resize(img.shape[1] + 1, img.shape[0] + 1)\n",
                "    h.axes.set_position([0, 0, 1, 1])\n",
                "    h.axes.set_xlim(-1, img.shape[1])\n",
                "    h.axes.set_ylim(img.shape[0], -1)\n",
                "  plt.grid(False)\n",
                "  plt.title(title)  \n",
                "  plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gvEkeGGryuPk"
            },
            "source": [
                "Model definition. In this lab we are going to use VGG16"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "weights = torchvision.models.VGG16_BN_Weights.DEFAULT\n",
                "model = torchvision.models.vgg16_bn(weights=weights).features.cuda().eval()\n",
                "print(model)\n",
                "for p in model.parameters():\n",
                "    p.requires_grad = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qKIVM-UMzoHB"
            },
            "source": [
                "Now we are going to define the initial image, to do that we are going to define an image of size sz x sz with a uniform distribution. Moreover, we are going to use mean and std from imagenet to optimize our image."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img = torch.empty(3, hparams['sz'], hparams['sz'], device=device).uniform_(150, 180).round()/255\n",
                "\n",
                "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
                "                                 std=[0.229, 0.224, 0.225])\n",
                "\n",
                "reverse_normalize = transforms.Normalize(mean = [-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
                "                                         std = [1/0.229, 1/0.224, 1/0.225])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iRXew-qAzwHd"
            },
            "source": [
                "Time to optimize our image!!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sz = hparams['sz'] \n",
                "for _ in range(hparams['upscaling_steps']): \n",
                "  img = normalize(img)\n",
                "  img = img.unsqueeze(0)\n",
                "  img.requires_grad = True\n",
                "  optimizer = torch.optim.Adam([img], lr=0.1, weight_decay=1e-6)\n",
                "  activations = SaveFeatures(list(model.children())[hparams['layer']])\n",
                "\n",
                "  for n in range(hparams['opt_steps']):  \n",
                "      optimizer.zero_grad()\n",
                "      out=model(img)\n",
                "      activation = activations.features[0, hparams['filter']]\n",
                "      loss = -torch.mean(activation)\n",
                "      loss.backward()\n",
                "      optimizer.step()\n",
                "\n",
                "  img = img.squeeze(0)  \n",
                "  img = reverse_normalize(img)\n",
                "  img = img.cpu().detach().numpy()\n",
                "  img = img.transpose(1, 2, 0)\n",
                "  out = img.copy()\n",
                "  activations.close()\n",
                "  sz = int(hparams['upscaling_factor'] * sz)\n",
                "  img = cv2.resize(img, (sz, sz), interpolation = cv2.INTER_CUBIC) \n",
                "  if hparams['blur'] is not None: img = cv2.blur(img, (hparams['blur'], hparams['blur']))\n",
                "  img = torch.from_numpy(img.transpose(2,0,1)).cuda()\n",
                "display_image(np.clip(out, 0, 1), size=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1bxr-klJys7x"
            },
            "source": [
                "It seems that filter 265 from layer 40th has learned to identify some chain patterns. Is it possible to test this hypothesis? The answer is... YES!!! We can simply apply the network to a picture and plot the average activations of the feature maps in the 40th layer. Let's do it!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists('Broad_chain_closeup.jpg'):\n",
                "  !wget \"https://upload.wikimedia.org/wikipedia/commons/b/bb/Broad_chain_closeup.jpg\"\n",
                "\n",
                "#Instead of using the Convolutional layer, we are using the activation layer. \n",
                "#The only reason for choosing layer 42 instead of 40 is that otherwise, \n",
                "#the plot would show a large amount of negative noise that made it hard to see \n",
                "#the positive spikes we are interested in.\n",
                "layer = hparams['layer'] + 2 # 42\n",
                "total_filters_in_layer = 512\n",
                "\n",
                "\n",
                "img = skimage.io.imread(\"Broad_chain_closeup.jpg\")\n",
                "img = skimage.transform.resize(img, (224, 244))\n",
                "display_image(img, size=1)\n",
                "img = np.array(img).transpose(2,0,1).astype(np.double)\n",
                "img_t = torch.tensor(img)\n",
                "img_t = img_t.type(torch.FloatTensor)\n",
                "img_t = normalize(img_t)\n",
                "\n",
                "activations = SaveFeatures(list(model.children())[layer])\n",
                "out=model(img_t.unsqueeze(0).cuda())\n",
                "\n",
                "mean_act = [activations.features[0,i].mean().item() for i in range(total_filters_in_layer)]\n",
                "\n",
                "plt.figure(figsize=(7,5))\n",
                "act = plt.plot(mean_act, linewidth=2.)\n",
                "extraticks=[hparams['filter']]\n",
                "ax = act[0].axes\n",
                "ax.set_xlim(0,500)\n",
                "plt.axvline(x=hparams['filter'], color='grey', linestyle='--')\n",
                "ax.set_xlabel(\"feature map\")\n",
                "ax.set_ylabel(\"mean activation\")\n",
                "ax.set_xticks([0,200,400] + extraticks)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JuTRoRpX0lb9"
            },
            "source": [
                "We can see a strong spike at feature map 265th. That shows that an image of a chain has a strong activation on the selected feature map."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "1e51a56ff47a427c9ba31360b3c145f2": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "31f9d09d0a14440b8de9169d3ec8803a": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_62f51eb4c2954ff6814a82c7e7e83108",
                            "IPY_MODEL_e4781fdced82427aaf45d6b6dd8947a8",
                            "IPY_MODEL_ff75f4e2296c4d7faafd10464f1199af"
                        ],
                        "layout": "IPY_MODEL_f504d2fe6cfb431eb41da72eea233669"
                    }
                },
                "3e7d4c1c693e420ca182337285842621": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "4517e1ec2f1c4c569b0aac600392b463": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "48d7ab2c76a24c38be54d3e29cd418ac": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "557fdb63103c4826b84461ec49ee578a": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "60f6548ac85d457e85d60a84ec25e141": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "62f51eb4c2954ff6814a82c7e7e83108": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_e1c6cabee6e1468a8224bcd1c9637c81",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_9b801b3823284f3998b3761e4bcf1b0b",
                        "value": "100%"
                    }
                },
                "631447e5a7bb4f19ba9a5678964de139": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_747841e61a2b4cc4b03cc416da61831e",
                            "IPY_MODEL_9c34e716bae34593b1321071d9d929eb",
                            "IPY_MODEL_7a8eef79410644f2b990e6524e0cd8f5"
                        ],
                        "layout": "IPY_MODEL_4517e1ec2f1c4c569b0aac600392b463"
                    }
                },
                "747841e61a2b4cc4b03cc416da61831e": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_d62b0b9a9f804dd3865c8657d6815539",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_1e51a56ff47a427c9ba31360b3c145f2",
                        "value": "100%"
                    }
                },
                "7a8eef79410644f2b990e6524e0cd8f5": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_557fdb63103c4826b84461ec49ee578a",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_60f6548ac85d457e85d60a84ec25e141",
                        "value": " 528M/528M [00:02&lt;00:00, 222MB/s]"
                    }
                },
                "9b801b3823284f3998b3761e4bcf1b0b": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "9c34e716bae34593b1321071d9d929eb": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_a888d7763cfe4dd8a03018eba7260120",
                        "max": 553507836,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_e5a8bf89dfa748a5bce339c913594a21",
                        "value": 553507836
                    }
                },
                "a888d7763cfe4dd8a03018eba7260120": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "b975ef9e4850467d96c6c702817ef23e": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "c7deee6c618d4d9786a014f1d2a43609": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "d62b0b9a9f804dd3865c8657d6815539": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "e1c6cabee6e1468a8224bcd1c9637c81": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "e4781fdced82427aaf45d6b6dd8947a8": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_48d7ab2c76a24c38be54d3e29cd418ac",
                        "max": 170498071,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_3e7d4c1c693e420ca182337285842621",
                        "value": 170498071
                    }
                },
                "e5a8bf89dfa748a5bce339c913594a21": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "f504d2fe6cfb431eb41da72eea233669": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "ff75f4e2296c4d7faafd10464f1199af": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_c7deee6c618d4d9786a014f1d2a43609",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_b975ef9e4850467d96c6c702817ef23e",
                        "value": " 170498071/170498071 [00:01&lt;00:00, 103364752.66it/s]"
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
