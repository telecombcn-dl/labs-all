{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iZyb0SojcrgT"
            },
            "source": [
                "# **Recommender Systems**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R4lJC_9DoiS3"
            },
            "source": [
                "Notebook created in PyTorch by [Paula G. Duran](https://www.linkedin.com/in/paulagd-1995/) for the UPC School (2020) and updated in (2022)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oHpGD89JdMLn"
            },
            "source": [
                "\n",
                "The problem we are trying to solve here is to predict a ranking for each user thus recommending a set of items from the huge number available on the original dataset. The dataset we will use is the ML-100K dataset, a classic dataset in the machine learning community when talking about recommendation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iHcRKrNEbGNk"
            },
            "source": [
                "## **Installation and imports**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preparing imports\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import csv\n",
                "import os\n",
                "import scipy.sparse as sp\n",
                "\n",
                "from typing import Tuple, Dict, Any, List\n",
                "from tqdm import tqdm, trange\n",
                "from IPython import embed\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from torch.utils.tensorboard import SummaryWriter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "d31BpRdgtcPL"
            },
            "source": [
                "In this session, I wanted to use the original Tensorboard instead of using the TensorboardColab version. Doing this, for example, we are able to add images or graphs and not just scalars. Besides, we are able to load different experiments on the same graphics thus allowing us to compare them in the same plot."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext tensorboard "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logs_base_dir = \"runs\"\n",
                "os.makedirs(logs_base_dir, exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tb_fm = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_FM/')\n",
                "tb_rnd = SummaryWriter(log_dir=f'{logs_base_dir}/{logs_base_dir}_RANDOM/')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SdrwqsmqztOL"
            },
            "source": [
                "## **Setting up hyperparameters**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's define some hyper-parameters\n",
                "hparams = {\n",
                "    'batch_size':64,\n",
                "    'num_epochs':20,\n",
                "    'hidden_size': 32,\n",
                "    'learning_rate':1e-4,\n",
                "}\n",
                "\n",
                "# we select to work on GPU if it is available in the machine, otherwise\n",
                "# will run on CPU\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7Q_Ioyxr0kas"
            },
            "source": [
                "## **Movielens - 100k dataset**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qR1yjsk30tbF"
            },
            "source": [
                "MovieLens [datasets](https://grouplens.org/datasets/movielens/) were collected by the GroupLens Research Project at the University of Minnesota.\n",
                " \n",
                "&nbsp;\n",
                "\n",
                "\n",
                "This data set consists of:\n",
                "\n",
                "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
                "* Each user has rated at least 20 movies. \n",
                "* Simple demographic info for the users (age, gender, occupation, zip)\n",
                "\n",
                " &nbsp;\n",
                "\n",
                "The data was collected through the MovieLens web site (movielens.umn.edu) during the seven-month period from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up - users who had less than 20 ratings or did not have complete demographic information were removed from this data set. \n",
                "\n",
                "\n",
                "> Note that the rating matrix is quite sparse (93.6% to be precise) as it only holds 100,000 ratings out of a possible 1,586,126 (943*1682).\n",
                "\n",
                "&nbsp;\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TWJ7GgTNvEww"
            },
            "source": [
                "### **Downloading data and loading it with pandas ...**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists('ml-100k'):\n",
                "    !wget \"https://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
                "    !unzip \"ml-100k.zip\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zkH-eDE5x2JJ"
            },
            "source": [
                "The file `u.data` contains the entire data and it looks in the following way:\n",
                "\n",
                "\n",
                "    user_id   movie_id   rating   timestamp\n",
                "\n",
                "where `user_id` is an integer between 1 and 943, `movie_id` is an integer between 1 and 1682, `rating` is an integer between 1 and 5 and `timestamp`  is an epoch-based integer.\n",
                "\n",
                "<div>\n",
                "<center><img src=\"https://files.realpython.com/media/movielens-head.0542b4c067c7.jpg\" width=\"300\"/></center>\n",
                "</div>\n",
                "\n",
                "However, for this task we need to preprocess all data in order to convert all rating tags into binary labels to deal with this task from an `implicit feedback` point of view. So, all data from the dataset will have positive labels (`1`) denoting any interaction with a film as a case of being interesed in the film (even the user did not like it in the end). For negative labels we would have a (`0`) label but, as we can see below, we do not have any negative samples yet.\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_ml100k_dataset(data_path: str) -> pd.DataFrame:\n",
                "    '''\n",
                "    The dataset can be downloaded by doing\n",
                "        - wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
                "        - unzip ml-1m.zip -d ml-100k\n",
                "    '''\n",
                "    if not os.path.exists(os.path.join(data_path, 'interactions.csv')):\n",
                "        '''\n",
                "        Create a Pandas Data Frame for the \"Users\" csv file\n",
                "        The column names are: user, gender, age, occupation, and zipcode\n",
                "        '''\n",
                "        \n",
                "        df = pd.read_csv(os.path.join(data_path, \"u.data\"), delimiter='\\t',\n",
                "                         names=['user','item','rating','timestamp'])\n",
                "\n",
                "        df['user'] = pd.Categorical(df['user']).codes\n",
                "        df['item'] = pd.Categorical(df['item']).codes\n",
                "        df['rating'] = 1\n",
                "\n",
                "        df.to_csv(os.path.join(data_path, 'interactions.csv'), index=False)\n",
                "    else:\n",
                "        df = pd.read_csv(os.path.join(data_path, 'interactions.csv'))\n",
                "    return df\n",
                "\n",
                "df = get_ml100k_dataset(data_path='ml-100k')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vdXXhR7bfjRU"
            },
            "source": [
                "As we can observe below, the number of unique values for the label field is 1. This means we just have one type of data (positive data) and we will need to manually sample negative data.\n",
                "\n",
                " &nbsp;\n",
                "\n",
                "As we just need to collect negative samples for training (and not for test), first we are going to split our dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.nunique()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VNA4sMEKssVQ"
            },
            "source": [
                "### **Splitting dataset (TLOO strategy)**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Tw3mlCC8tuIb"
            },
            "source": [
                "In this section we follow the time leave-one-out methodology to split the dataset by holding out the more recent interaction of each user."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_train_test(data: np.ndarray,\n",
                "                     n_users: int) -> Tuple[np.ndarray, np.ndarray]:\n",
                "    # Split and remove timestamp\n",
                "    train_x, test_x = [], []\n",
                "    for u in trange(n_users, desc='spliting train/test and removing timestamp...'):\n",
                "        user_data = data[data[:, 0] == u]\n",
                "        sorted_data = user_data[user_data[:, -1].argsort()]\n",
                "        if len(sorted_data) == 1:\n",
                "            train_x.append(sorted_data[0][:-1])\n",
                "        else:\n",
                "            train_x.append(sorted_data[:-1][:, :-1])\n",
                "            test_x.append(sorted_data[-1][:-1])\n",
                "    return np.vstack(train_x), np.stack(test_x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data = df[['user', 'item', 'timestamp']].astype('int32').to_numpy()\n",
                "data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ttM4SbE3iXb-"
            },
            "source": [
                "> Note that we choose to preprocess the dataset by re-indexing the films in order to end up with a single identifier by entity (either user or item). Also, we will remove the timestamp from our data, as it is just useful for splitting data. \n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "add_dims=0\n",
                "for i in range(data.shape[1] - 1):  # do not affect to timestamp\n",
                "    # MAKE IT START BY 0\n",
                "    data[:, i] -= np.min(data[:, i])\n",
                "    # RE-INDEX\n",
                "    data[:, i] += add_dims\n",
                "    add_dims = np.max(data[:, i]) + 1\n",
                "dims = np.max(data, axis=0) + 1\n",
                "data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_x, test_x = split_train_test(data, dims[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f'Train shape: {train_x.shape}')\n",
                "print(f'Test shape: {test_x.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "assert train_x.shape[0] + test_x.shape[0] == 100000"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VD1LNFbbvZWx"
            },
            "source": [
                "As we can observed, we have just held one interaction per user as a test set (the most recent one when sorting by timestamp), which will be used as ground-truth when evaluating the model bu outputing a ranking per user."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lFMYAs68t5i3"
            },
            "source": [
                "### **NEGATIVE SAMPLING**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zxWuZMjwt-I4"
            },
            "source": [
                "As far as we know, a classifier needs both positive and negative data in order to learn. However, as our approach is to use implicit feedback we have converted any rating to the label `1`, and so No negative data is available in order to feed the model.\n",
                "\n",
                "For that reason, we will need to perform negative sampling by sampling interactions that did not actually occured between a given user and a given item."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_x = train_x[:, :2]\n",
                "dims = dims[:2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dims"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lXT-YbD0wJ6i"
            },
            "source": [
                "As we see, we are just taking the two first dimensions (user and item) to adress this problems. Hence, the `dims` variable contains the accumulated number of dimensions for each entity [#users, #(users+items)] because we have re-indexed them. By doing that, we get each user or item to have a unique identifier for the model. \n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qg_UzrAVxHS7"
            },
            "source": [
                "In order to be able to know which interactions did not occur we will build the rating matrix of this dataset. \n",
                "\n",
                "> Please note that the next function is extended to any extra dimension (such as context) that you might want to add."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_adj_mx(n_feat:int, data:np.ndarray) -> sp.dok_matrix :\n",
                "    train_mat = sp.dok_matrix((n_feat, n_feat), dtype=np.float32)\n",
                "    for x in tqdm(data, desc=f\"BUILDING ADJACENCY MATRIX...\"):\n",
                "        train_mat[x[0], x[1]] = 1.0\n",
                "        train_mat[x[1], x[0]] = 1.0\n",
                "        # IDEA: We treat features that are not user or item differently because we do not consider\n",
                "        #  interactions between contexts\n",
                "        if data.shape[1] > 2:\n",
                "            for idx in range(len(x[2:])):\n",
                "                train_mat[x[0], x[2 + idx]] = 1.0\n",
                "                train_mat[x[1], x[2 + idx]] = 1.0\n",
                "                train_mat[x[2 + idx], x[0]] = 1.0\n",
                "                train_mat[x[2 + idx], x[1]] = 1.0\n",
                "    return train_mat"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HnIPGduAhyzL"
            },
            "source": [
                "\n",
                "Note that due to reindex, our adjacency matrix from training is even bigger. However, now it is even sparser. Storing this as a dense matrix would be a massive waste of both storage and computing power!\n",
                "This is why use a scipy.lil_matrix sparse matrix for samples and a numpy array for labels."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8K4K0wpFw5N7"
            },
            "source": [
                "\n",
                "Into the next function, we can observe how to perform negative sampling from the rating matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ng_sample(data: np.ndarray, dims: list, num_ng:int=4) -> Tuple[np.ndarray, sp.dok_matrix]:\n",
                "    rating_mat = build_adj_mx(dims[-1], data)\n",
                "    interactions = []\n",
                "    min_item, max_item = dims[0], dims[1]\n",
                "    for num, x in tqdm(enumerate(data), desc='perform negative sampling...'):\n",
                "        interactions.append(np.append(x, 1))\n",
                "        for t in range(num_ng):\n",
                "            j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
                "            # IDEA: Loop to exclude true interactions (set to 1 in adj_train) user - item\n",
                "            while (x[0], j) in rating_mat or j == int(x[1]):\n",
                "                j = np.random.randint(min_item, max_item) #if not pop else random.sample(items_to_sample, 1)[0]\n",
                "            interactions.append(np.concatenate([[x[0], j], x[2:], [0]]))\n",
                "    return np.vstack(interactions), rating_mat\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_x, rating_mat = ng_sample(train_x, dims)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Checking how sparse the rating matrix is\n",
                "rating_mat"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JRZCCX2xyovD"
            },
            "source": [
                "So, if we show the first 10 samples of our data after performing negative sampling we can observe that four negative samples have been introduced for each positive one. \n",
                "\n",
                "**Now we are ready to train any classifier!!!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_x[:10]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CqmuDc2083Sq"
            },
            "source": [
                "### **Creating a dataset class**\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Y3f5Qm5yxLtD"
            },
            "source": [
                "We now create a Pytorch Dataset so that it is clear how we deal with the data. It could be a good practise to introduce the negative sampling inside this function if desired."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PointData(Dataset):\n",
                "    def __init__(self,\n",
                "                 data: np.ndarray,\n",
                "                 dims: list) -> None:\n",
                "        \"\"\"\n",
                "        Dataset formatter adapted point-wise algorithms\n",
                "        Parameters\n",
                "        \"\"\"\n",
                "        super(PointData, self).__init__()\n",
                "        self.interactions = data\n",
                "        self.dims = dims\n",
                "\n",
                "    def __len__(self) -> int:\n",
                "        return len(self.interactions)\n",
                "        \n",
                "    def __getitem__(self, \n",
                "                    index: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
                "        \"\"\"\n",
                "        Return the pairs user-item and the target.\n",
                "        \"\"\"\n",
                "        return self.interactions[index][:-1], self.interactions[index][-1]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = PointData(train_x, dims)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gzJNUD4K1FH8"
            },
            "source": [
                "Now we already have the entire training set all setted up to be fed into a Pytorch Dataloader and train a model.\n",
                "\n",
                "However, we still miss some part of our test set to be completed."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "X7LnShCM1Vjy"
            },
            "source": [
                "### **Preparing the test set for inference**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YvsijFMl1a69"
            },
            "source": [
                "If we plot the test_x set that we prepared we realize that we stored one sample for each user but, what does this sample actually mean?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8BWGqTfI1pI2"
            },
            "source": [
                "Remaining the goal of RS, we should have in mind that we want to provide a personalized ranking for each user. To do so, we need to compare the score that a given user gets for interacting with each of the items from the database. Once we get the score of each item for a given user, we will be able to sort the scores out and provide an accurate ranking."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3YPqIiy25Jmy"
            },
            "source": [
                "<div>\n",
                "<center><img src=\"https://drive.google.com/uc?export=view&id=1DVIdCPnxNfpFLZHDrLWH_FJVrUvLMSti\" width=\"700\" /></center>\n",
                "</div>\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "QmRtlDrW7u8H"
            },
            "source": [
                "To do so, we need to add to each user test set also all the other items from the database even, of course, we will subtract the ones that the user interacted with in training. \n",
                "\n",
                "By doing so, we will expect the score of our ground-truth (GT) sample to be in the top@1 (hopefully) of the ranking. For example, in the image above we can see an example of the test set of the user one, where the higher score is actually the score of the GT sample and so our metrics would be amazing!!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "zero_positions = np.asarray(np.where(rating_mat.A==0)).T\n",
                "zero_positions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MDw9FtXl_VrV"
            },
            "source": [
                "Above we have asked for those coordinates from the rating matrix where we have 0-values. Thus, all the zeros of a given row will need to be candidates items over which we will need to perform the ranking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "items2compute = []\n",
                "for user in trange(dims[0]):\n",
                "    aux = zero_positions[zero_positions[:, 0] == user][:, 1]\n",
                "    items2compute.append(aux[aux >= dims[0]])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "items2compute[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_test_set(itemsnoninteracted:list,\n",
                "                   gt_test_interactions: np.ndarray) -> list:\n",
                "    #max_users, max_items = dims # number users (943), number items (2625)\n",
                "    test_set = []\n",
                "    for pair, negatives in tqdm(zip(gt_test_interactions, itemsnoninteracted), desc=\"BUILDING TEST SET...\"):\n",
                "        # APPEND TEST SETS FOR SINGLE USER\n",
                "        negatives = np.delete(negatives, np.where(negatives == pair[1]))\n",
                "        single_user_test_set = np.vstack([pair, ] * (len(negatives)+1))\n",
                "        single_user_test_set[:, 1][1:] = negatives\n",
                "        test_set.append(single_user_test_set.copy())\n",
                "    return test_set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_x = build_test_set(items2compute, test_x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_x[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mBjZc_V7B2PD"
            },
            "source": [
                "In the previous cell, we can observe the complete test set for the `user 1`, where the first pair refers to the GT sample (the film we know that `user 1` will watch next and that we expect to be in the first position of the ranking) and the other positions to any other item that the user did not consumed previously.\n",
                "\n",
                "\n",
                "At this stage, we have both the training set and the test set setted up and ready to start training and evaluating the any model!! "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xNAKSGghsm7_"
            },
            "source": [
                "### **Building Factorization Machines model**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Vfztn6OHhh7Q"
            },
            "source": [
                "Now, before programming the training and the inference functions, we will explain a very common and used baseline as it is Factorization Machines. Please note that this model could be exchanged for any desired model by just changing the model class. Hence, all the pipeline regarding the data (defined previously) and also the one referring to the training and inference stages (which comes next) will still remind the same."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GSlwmfoK0mA-"
            },
            "source": [
                "\n",
                "<div>\n",
                "<center><img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/04/03/sagemaker-factorization-1.gif\" width=\"400\"/></center>\n",
                "</div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rdgA3E4Js4mx"
            },
            "source": [
                "Looking at the formula above and the [FM paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) statements, we know that, as demonstrated in the paper, we can write down an implementation of the last term in the equation in the following way:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# FM part of the equation\n",
                "class FM_operation(torch.nn.Module):\n",
                "\n",
                "    def __init__(self, \n",
                "                 reduce_sum: bool=True) -> None:\n",
                "        super().__init__()\n",
                "        self.reduce_sum = reduce_sum\n",
                "\n",
                "    def forward(self,\n",
                "                x: torch.Tensor) -> float:\n",
                "        \"\"\"\n",
                "        :param x: Float tensor of size ``(batch_size, num_fields, embed_dim)``\n",
                "        \"\"\"\n",
                "        square_of_sum = torch.sum(x, dim=1) ** 2\n",
                "        sum_of_square = torch.sum(x ** 2, dim=1)\n",
                "        ix = square_of_sum - sum_of_square\n",
                "        if self.reduce_sum:\n",
                "            ix = torch.sum(ix, dim=1, keepdim=True)\n",
                "        return 0.5 * ix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Bw-jiLKCsCDk"
            },
            "source": [
                "Once we have clear how to implement the last term of the FM equation, we will now program the entire FM model by using the FM_operation class defined above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FactorizationMachineModel(torch.nn.Module):\n",
                "    \"\"\"\n",
                "    A pytorch implementation of Factorization Machine.\n",
                "\n",
                "    Reference:\n",
                "        S Rendle, Factorization Machines, 2010.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, \n",
                "                 field_dims: list,\n",
                "                 embed_dim: float) -> None:\n",
                "        super().__init__()\n",
                "        self.linear = torch.nn.Linear(len(field_dims), 1)\n",
                "        self.embedding = torch.nn.Embedding(field_dims[-1], embed_dim)\n",
                "        self.fm = FM_operation(reduce_sum=True)\n",
                "\n",
                "        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n",
                "\n",
                "    def forward(self,\n",
                "                interaction_pairs: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"\n",
                "        :param interaction_pairs: Long tensor of size ``(batch_size, num_fields)``\n",
                "        \"\"\"\n",
                "        out = self.linear(interaction_pairs.float()) + self.fm(self.embedding(interaction_pairs))\n",
                "        return out.squeeze(1)\n",
                "        \n",
                "    def predict(self, \n",
                "                interactions: np.ndarray,\n",
                "                device: torch.device) -> torch.Tensor:\n",
                "        # return the score, inputs are numpy arrays, outputs are tensors\n",
                "        test_interactions = torch.from_numpy(interactions).to(dtype=torch.long, device=device) #, dtype=torch.long)\n",
                "        output_scores = self.forward(test_interactions)\n",
                "        return output_scores"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MpTBEpD3C-_P"
            },
            "source": [
                "### **Pipeline functions**\n",
                "\n",
                "In this section we define both training and inference functions which are programmed to be called once per epoch."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6eAjrVb-DnNx"
            },
            "source": [
                "#### **Training**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from statistics import mean\n",
                "\n",
                "def train_one_epoch(model: torch.nn.Module,\n",
                "                    optimizer: torch.optim,\n",
                "                    data_loader: torch.utils.data.DataLoader,\n",
                "                    criterion: torch.nn.functional,\n",
                "                    device: torch.device) -> float:\n",
                "    model.train()\n",
                "    total_loss = []\n",
                "\n",
                "    for i, (interactions, targets) in enumerate(data_loader):\n",
                "        interactions = interactions.to(device)\n",
                "        targets = targets.to(device)\n",
                "\n",
                "        predictions = model(interactions)\n",
                "    \n",
                "        loss = criterion(predictions, targets.float())\n",
                "        model.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss.append(loss.item())\n",
                "\n",
                "    return mean(total_loss)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Q1bdqLjbjCTQ"
            },
            "source": [
                "#### **Inference**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test(model: torch.nn.Module,\n",
                "         test_x: np.ndarray,\n",
                "         device: torch.device,\n",
                "         topk: int=10) -> Tuple[float, float]:\n",
                "    # Test the HR and NDCG for the model @topK\n",
                "    model.eval()\n",
                "\n",
                "    HR, NDCG = [], []\n",
                "    for user_test in test_x:\n",
                "        gt_item = user_test[0][1]\n",
                "        predictions = model.predict(user_test, device)\n",
                "        _, indices = torch.topk(predictions, topk)\n",
                "        recommend_list = user_test[indices.cpu().detach().numpy()][:, 1]\n",
                "\n",
                "        HR.append(getHitRatio(recommend_list, gt_item))\n",
                "        NDCG.append(getNDCG(recommend_list, gt_item))\n",
                "    return mean(HR), mean(NDCG)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3whzmXbmUjMc"
            },
            "source": [
                "### **Define metrics**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3H4ktCiKWkeK"
            },
            "source": [
                "Last but not least before start mounting the entire pipeline, we will define two metrics used in order to analyze the performance of the model: Hit Ratio and Normalized Cumulative Discounted Gain."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "def getHitRatio(recommend_list: list,\n",
                "                gt_item: int) -> bool:\n",
                "    if gt_item in recommend_list:\n",
                "        return 1\n",
                "    else:\n",
                "        return 0\n",
                "\n",
                "def getNDCG(recommend_list: list,\n",
                "            gt_item: int) -> float:\n",
                "    idx = np.where(recommend_list == gt_item)[0]\n",
                "    if len(idx) > 0:\n",
                "        return math.log(2)/math.log(idx+2)\n",
                "    else:\n",
                "        return 0"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pklQzM2JHI3-"
            },
            "source": [
                "### **PIPELINE**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uiujfokgjCTR"
            },
            "source": [
                "#### **Defining the model, the loss and the optimizer**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dims = train_dataset.dims\n",
                "model = FactorizationMachineModel(dims, hparams['hidden_size']).to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
                "optimizer = torch.optim.Adam(params=model.parameters(), lr=hparams['learning_rate'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5QTZBll9jCTR"
            },
            "source": [
                "#### **Random evaluation**\n",
                "\n",
                "In this subsection we run an inference test (obviously without previously train the model) to check that everything works correctly and also observe the initial result, which will be equivalent to using a random model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "class RandomModel(torch.nn.Module):\n",
                "    def __init__(self, \n",
                "                 dims: list) -> None:\n",
                "        super(RandomModel, self).__init__()\n",
                "        \"\"\"\n",
                "        Simple random based recommender system\n",
                "        \"\"\"\n",
                "        self.all_items = list(range(dims[0], dims[1]))\n",
                "\n",
                "    def forward(self) -> None:\n",
                "        pass\n",
                "\n",
                "    def predict(self,\n",
                "                interactions: np.ndarray,\n",
                "                device=None) -> torch.Tensor:\n",
                "        return torch.FloatTensor(random.sample(self.all_items, len(interactions)))\n",
                "\n",
                "rnd_model = RandomModel(dims)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YS3iBGRNjCTS"
            },
            "source": [
                "#### **Final Pipeline**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_loader = DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True, num_workers=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LHbKyX8vpqOa"
            },
            "source": [
                "#### **Start training the model**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DO EPOCHS NOW\n",
                "topk = 10\n",
                "for epoch_i in range(hparams['num_epochs']):\n",
                "    #data_loader.dataset.negative_sampling()\n",
                "    train_loss = train_one_epoch(model, optimizer, data_loader, criterion, device)\n",
                "    hr, ndcg = test(model, test_x, device, topk=topk)\n",
                "\n",
                "    print(f'epoch {epoch_i}:')\n",
                "    print(f'training loss = {train_loss:.4f} | Eval: HR@{topk} = {hr:.4f}, NDCG@{topk} = {ndcg:.4f} ')\n",
                "    print('\\n')\n",
                " \n",
                "    tb_fm.add_scalar('train/loss', train_loss, epoch_i)\n",
                "    tb_fm.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
                "    tb_fm.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n",
                "\n",
                "    hr, ndcg = test(rnd_model, test_x, device, topk=topk)\n",
                "    tb_rnd.add_scalar('eval/HR@{topk}', hr, epoch_i)\n",
                "    tb_rnd.add_scalar('eval/NDCG@{topk}', ndcg, epoch_i)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tMfW67UKWkfT"
            },
            "source": [
                "## **Visualizing results**\n",
                "\n",
                "Once we have trained both models (*fm with usual embbedding layers* vs *fm with embeddings from gcn*), we can observe both metrics and loss in the same graphic in order to compare:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%tensorboard --logdir runs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "k_GxjHwqukOC"
            },
            "source": [
                "# **QUESTIONNAIRE:**\n",
                "\n",
                "- You can answer the test on the questionnaire section from MyTech."
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
