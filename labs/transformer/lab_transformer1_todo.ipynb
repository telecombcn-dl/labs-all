{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4ZsnaW2_s_iB"
            },
            "source": [
                "# Build your own Transformer\n",
                "\n",
                "Created by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) for the [Postgraduate Course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) ([UPC School](https://www.talent.upc.edu/ing/), 2021)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "FlLmT6kia5GO"
            },
            "source": [
                "In this lab we will learn about the [Transformer](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), a popular architecture that revolutionized Deep Learning a few years ago.\n",
                "\n",
                "This architecture was firstly designed for Machine Translation. In this field, Recurrent Neural Networks (e.g. LSTM) had been the state-of-the-art since [the introduction of the Attention mechanism](https://arxiv.org/abs/1409.0473) in 2015. The Transformer surpassed them by introducing a key idea: getting rid of any recurrence and mainly using the **(Self-)Attention** mechanism, as the title of the paper states: [Attention is All you Need](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).\n",
                "\n",
                "Actually, Transformer-based architectures are used in many fields beyond Machine Translation. First, many models arised for other text-related tasks (e.g. [BERT](https://arxiv.org/abs/1810.04805) or [GPT-3](https://arxiv.org/abs/2005.14165)), but now they're also used in other fields, like [Speech processing](https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html) and [Computer Vision](https://arxiv.org/abs/2010.11929).\n",
                "\n",
                "Throughout this notebook, we will build our own Transformer, and you'll understand module by module how this architecture works. Once it's finished, we will train it with a dummy dataset and we will try to interpret the results."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "import math\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pylab as plt"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oP-hS-tdOqgk"
            },
            "source": [
                "##\u00a0Transformer architecture\n",
                "\n",
                "Here we go!\n",
                "\n",
                "In the left part of the figure below, you can see the Transformer architecture. Take a look at it carefully to get used to all this new terminology.\n",
                "\n",
                "You can also see a breakdown of the most important module in the Transfomer: the Multi-Head Attention, which is based on the Scaled-Dot Product Attention.\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=\"1000px\" alt=\"Zoom in to the Transformer\"/>\n",
                "</p>\n",
                "\n",
                "Don't panic! ;) We will start from the most simple structure and we will build upon it, little by little. Let's start with the Scaled Dot-Product Attention!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8xk2rD305QTJ"
            },
            "source": [
                "### Scaled Dot-Product Attention\n",
                "\n",
                "The first key idea to understand how the Transformer works is the Scaled Dot Product Attention (we will call it SDPA from now on).\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/SCALDE.png\" height=\"300px\" alt=\"Scaled Dot-Product Attention\"/>\n",
                "</p>\n",
                "\n",
                "This is the equation:\n",
                "\n",
                "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
                "\n",
                "You are probably trying to figure out what $Q$, $K$ and $V$ are, right? They are the queries, the keys and the values. A good example to understand these concepts is the one given by [@dontloo](https://stats.stackexchange.com/users/95569/dontloo) in StackExchange\n",
                "\n",
                ">[...]\n",
                ">\n",
                ">The key/value/query concepts come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description, etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n",
                ">\n",
                ">The attention operation turns out can be thought of as a retrieval process as well, so the key/value/query concepts also apply here.\n",
                ">\n",
                ">[...]\n",
                ">\n",
                ">[[Read the full answer]](https://stats.stackexchange.com/a/424127)\n",
                "\n",
                "This keys/values/query can represent tokens in a sentence. Furthermore, the attention function can be computed on a set of queries simultaneously. Hence, they are packed together into a matrix $Q$, like the keys ($K$) and the values ($V$). Take into account that you need the same amount of keys and values, but the number of queries can differ.\n",
                "\n",
                "Also, why is the scaling needed? Well, it turns out that for high-dimensional keys (large $d_k$) the dot-product grow large in magnitude, hurting the gradients.\n",
                "\n",
                "Finally, note that in the figure there is a module called \"Module (opt.)\" which doesn't appear in the equation. This allows us to control which values can the queries \"attend\" to. This will be useful, for example, to avoid attention to padding tokens, which we use to batch sentences of different length."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def scaled_dot_product(q, k, v, mask=None):\n",
                "    \"\"\" Computes the Scaled Dot-Product Attention\n",
                "\n",
                "    Args:\n",
                "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
                "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
                "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
                "        mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
                "\n",
                "    Returns:\n",
                "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
                "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
                "\n",
                "    \"\"\"\n",
                "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
                "\n",
                "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
                "    attn_logits = \n",
                "\n",
                "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
                "    attn_logits = \n",
                "\n",
                "    if mask is not None:\n",
                "        attn_logits = attn_logits.masked_fill(mask, -float(\"inf\"))\n",
                "\n",
                "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
                "    attention =\n",
                "\n",
                "    output = torch.matmul(attention, v)\n",
                "\n",
                "    return output, attention"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def plot_attention(attention, queries, keys, xtitle=\"Keys\", ytitle=\"Queries\"):\n",
                "    \"\"\" Plots the attention map\n",
                "    \n",
                "    Args:\n",
                "        att (torch.FloatTensor): Attention map (T_q x T_k)\n",
                "        queries (List[str]): Query Tensor\n",
                "        keys (List[str]): Key Tensor\n",
                "    \"\"\"\n",
                "\n",
                "    sns.set(rc={'figure.figsize':(12, 8)})\n",
                "    ax = sns.heatmap(\n",
                "        attention.detach().cpu(),\n",
                "        linewidth=0.5,\n",
                "        xticklabels=keys,\n",
                "        yticklabels=queries,\n",
                "        cmap=\"coolwarm\")\n",
                "\n",
                "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
                "    ax.set_xlabel(xtitle) \n",
                "    ax.set_ylabel(ytitle)\n",
                "\n",
                "    plt.show()"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r_qeDynpxEAh"
            },
            "source": [
                "Let's create some random queries, keys and values, with the following dimensions:\n",
                "- $T_Q=5$\n",
                "- $T_K=T_V=8$\n",
                "- $d_Q=d_K=d_V=4$\n",
                "\n",
                "We will use them to test our SDPA function."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "q = torch.randn(5, 4)\n",
                "k = torch.randn(8, 4)\n",
                "v = torch.randn(8, 4)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "output, attention = scaled_dot_product(q, k, v)\n",
                "\n",
                "print(f\"Output:\\n{output}\\n{output.shape}\\n\")\n",
                "print(f\"Attention weights:\\n{attention}\\n{attention.shape}\\n\")\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                "    [str([round(float(k__), 1) for k__ in k_]) for k_ in k],\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0M5G6XU3txyF"
            },
            "source": [
                "After computing the SDPA, we get:\n",
                "- The output, of dimensions $T_Q x d_V$\n",
                "- The attention weights, which relate the queries and the keys, of dimensions $T_Q x T_K$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Lk0fglFM6E91"
            },
            "source": [
                "But you've already heard about Self-Attention again, right?\n",
                "\n",
                "Basically, Self-Attention consists of using the same set of vectors as queries, keys and values. Let's try it:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "x = torch.randn(5, 4)\n",
                "output, attention = scaled_dot_product(q=x, k=x, v=x)\n",
                "\n",
                "print(f\"Output:\\n{output}\\n{output.shape}\\n\")\n",
                "print(f\"Attention weights:\\n{attention}\\n{attention.shape}\\n\")\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Rv6iR7Y0wWR3"
            },
            "source": [
                "But then... that's all, \"Attention is all you need\"?\n",
                "\n",
                "Well, no, **it's not enough with just attention**... We need learnable parameters somewhere!\n",
                "\n",
                "Actually, the inputs of the SDPA need to be projected with a Linear layer. This way we can get different representations from the same input vectors. We do something like this:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://miro.medium.com/max/1578/1*_92bnsMJy8Bl539G4v93yg.gif\" height=\"600px\" alt=\"Self-Attention\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hFc8IdXL_UoF"
            },
            "source": [
                "Here you can find an implementation of this \"learnable\" SDPA. Be aware that this class will not used by the model we will implement, it's just an intermediate step we use now for didactic purposes."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class LearnableScaledDotProductAttention(nn.Module):\n",
                "    def __init__(self, embed_dim):\n",
                "        super(LearnableScaledDotProductAttention, self).__init__()\n",
                "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
                "    \n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        q = self.proj_q(q)\n",
                "        k = self.proj_k(k)\n",
                "        v = self.proj_v(v)\n",
                "        output, _ = scaled_dot_product(q, k, v, mask)\n",
                "        return output"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NSpzJMgwAK7n"
            },
            "source": [
                "Let's test that it can learn now, by trying to reconstruct the input tensor with self-attention:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "sdpa = LearnableScaledDotProductAttention(embed_dim=4)\n",
                "optimizer = optim.Adam(sdpa.parameters())\n",
                "\n",
                "losses_sdpa = []\n",
                "n_epochs = 10000\n",
                "for i in range(n_epochs):\n",
                "    optimizer.zero_grad()\n",
                "    output = sdpa(q=x, k=x, v=x)    # Self-attention\n",
                "    loss = F.mse_loss(output, x)    # Reconstruct the input\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses_sdpa.append(loss.item())\n",
                "    if (i + 1) % 1000 == 0:\n",
                "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
                "\n",
                "\n",
                "print(f\"\\nOutput:\\n{output}\\n\")\n",
                "print(f\"Query:\\n{x}\\n\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rzyQtcqUA2uo"
            },
            "source": [
                "Ok, looks good, we can train it. It's starting to make sense now, right?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zHJiKgHqeyrO"
            },
            "source": [
                "### Multi-Head Attention\n",
                "\n",
                "To further exploit this attention mechanism, the original paper introduced the Multi-Head Attention mechanism (MHA).\n",
                "\n",
                "Instead of performing the attention mechanism just once, they found it benefitial to project the input multiple times into different \"attention heads\". This way, multiple attentions can be learned at the same time. \n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png\" height=\"400px\" alt=\"Scaled Dot-Product Attention\"/>\n",
                "</p>\n",
                "\n",
                "To combine the outputs of each head, they are concatenated and projected by a Linear transformation $W^O$, as defined by the following equation:\n",
                "\n",
                "$$\n",
                "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O\n",
                "$$\n",
                "$$\n",
                "where\\ head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)\n",
                "$$\n",
                "\n",
                "Note that $W^Q_i$, $W^K_i$, $W^V_i$ are the Linear projections we've seen in the previous section.\n",
                "\n",
                "To better understand this, see the following illustration created by [Jay Alammar](https://jalammar.github.io/) for his famous blog post [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/):\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" height=\"600px\" alt=\"Self-Attention\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class MultiheadAttention(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, num_heads):\n",
                "        super(MultiheadAttention, self).__init__()\n",
                "        assert embed_dim % num_heads == 0, \\\n",
                "            \"Embedding dimension must be multiple of the number of heads.\"\n",
                "\n",
                "        self.embed_dim = embed_dim\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = embed_dim // num_heads\n",
                "\n",
                "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
                "\n",
                "        self._reset_parameters()\n",
                "\n",
                "    def _reset_parameters(self):\n",
                "        # Original Transformer initialization\n",
                "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
                "        self.proj_q.bias.data.fill_(0)\n",
                "        self.proj_k.bias.data.fill_(0)\n",
                "        self.proj_v.bias.data.fill_(0)\n",
                "        self.proj_o.bias.data.fill_(0)\n",
                "\n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        batch_size = q.size(1)\n",
                "\n",
                "        q = self.proj_q(q)\n",
                "        k = self.proj_k(k)\n",
                "        v = self.proj_v(v)\n",
                "\n",
                "        # TODO: Split the tensors into multiple heads\n",
                "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
                "        q = q.reshape(...)\n",
                "        k = k.reshape(...)\n",
                "        v = v.reshape(...)\n",
                "\n",
                "        # The last two dimensions must be sequence length and the head dimension,\n",
                "        # to make it work with the scaled dot-product function. \n",
                "        # TODO: Rearrange the dimensions\n",
                "        #\u00a0T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
                "        q = q.permute(...)\n",
                "        k = k.permute(...)\n",
                "        v = v.permute(...)\n",
                "\n",
                "        # Apply the same mask to all the heads\n",
                "        if mask is not None:\n",
                "            mask = mask.unsqueeze(1)\n",
                " \n",
                "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
                "        output_heads, attn_w = ...\n",
                "\n",
                "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
                "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
                "\n",
                "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
                "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
                "        output = self.proj_o(output_cat)\n",
                "\n",
                "        return output, attn_w"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2xzZ19kbDAnH"
            },
            "source": [
                "Let's test the same dummy example than before, trying to reconstruct the input tensor with self-attention, this time with a MHA module:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "mha = MultiheadAttention(embed_dim=4, num_heads=2)\n",
                "optimizer = optim.Adam(mha.parameters())\n",
                "\n",
                "losses_mha = []\n",
                "n_epochs = 10000\n",
                "for i in range(n_epochs):\n",
                "    optimizer.zero_grad()\n",
                "    output = mha(                # Self-attention\n",
                "        q=x.unsqueeze(1),\n",
                "        k=x.unsqueeze(1),\n",
                "        v=x.unsqueeze(1)\n",
                "    )[0].squeeze(1)\n",
                "    loss = F.mse_loss(output, x) # Reconstruct input\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses_mha.append(loss.item())\n",
                "    if (i + 1) % 1000 == 0:\n",
                "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
                "\n",
                "print(f\"\\nOutput:\\n{output}\\n\")\n",
                "print(f\"Query:\\n{x}\\n\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c_j2pJpPGz4y"
            },
            "source": [
                "Ok, seems fine, it learns!\n",
                "\n",
                "At this point, you already know all you need about attention in the Transformer :D Now, we need to know where to apply it.\n",
                "\n",
                "But before looking again at the Transformer architecture, we need to make a small stop at the Positional Encoding."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OB06UfyPekC3"
            },
            "source": [
                "###\u00a0Positional Encoding\n",
                "\n",
                "As you may know, RNNs are designed in a way that they know the ordering of the input tokens. This is very important when processing sequences such as text sentences.\n",
                "\n",
                "However, when using Self-attention, there is no positional information used between the queries, keys and values. For this reason, the Transformer authors needed to give positional information to the model explicitly.\n",
                "\n",
                "They simply decided to create an embedding table, with the following equation, which is summed to the embeddings of the inputs.\n",
                "\n",
                "$$\n",
                "PE_{(pos,2i) = sin(pos\\ /\\ 10000^{2i\\ /\\ d\\_model})}\n",
                "$$\n",
                "$$\n",
                "PE_{(pos,2i+1) = cos(pos\\ /\\ 10000^{2i\\ /\\ d\\_model})}\n",
                "$$\n",
                "where $pos$ is the position and $i$ is the dimension.\n",
                "\n",
                "The resulting positional embedding table is like this:\n",
                "<p align=\"center\">\n",
                "<img src=\"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" height=\"350px\" alt=\"Positional Encoding\"\"/>\n",
                "</p>\n",
                "\n",
                "For more information about the positional encoding, check [this post](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, max_len=5000):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality\n",
                "            max_len (int): Maximum length of a sequence to expect\n",
                "        \"\"\"\n",
                "        super(PositionalEncoding, self).__init__()\n",
                "\n",
                "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
                "        # for max_len inputs\n",
                "        pe = torch.zeros(max_len, embed_dim)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        pe = pe.unsqueeze(1)\n",
                "\n",
                "        self.register_buffer('pe', pe, persistent=False)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.pe[:x.size(0)]\n",
                "        return x"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dfYCl_awY7FN"
            },
            "source": [
                "### Encoder\n",
                "\n",
                "Now the time has come to put everything together and build the Transformer Encoder!\n",
                "\n",
                "This structure mainly consists of a stack of layers defined as below:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" height=\"600px\" alt=\"Transformer Encoder\"/>\n",
                "</p>\n",
                "\n",
                "The [Layer Normalization](https://arxiv.org/abs/1607.06450) differs from Batch Normalization in that it works element-wise, as seen in the following figure:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png\" height=\"250px\" alt=\"LayerNormalization\"/>\n",
                "</p>\n",
                "\n",
                "The Feed Forward layers consist of a projection to a higher dimension (`ffn_dim`), a ReLU activation and another projection to the original dimension (`embed_dim`). They are defined by the following equation:\n",
                "\n",
                "$$\n",
                "FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
                "$$"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class TransformerEncoderLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
                "            ffn_dim (int): Inner dimensionality in the FFN\n",
                "            num_heads (int): Number of heads of the multi-head attention block\n",
                "            dropout (float): Dropout probability\n",
                "        \"\"\"\n",
                "        super(TransformerEncoderLayer, self).__init__()\n",
                "\n",
                "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(embed_dim, ffn_dim),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(ffn_dim, embed_dim)\n",
                "        )\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, mask=None, return_att=False):\n",
                "        src_len, batch_size, _ = x.shape\n",
                "        if mask is None:\n",
                "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
                "\n",
                "        selfattn_mask = mask.unsqueeze(-2)\n",
                "\n",
                "        # TODO: Self-Attention block\n",
                "        selfattn_out, selfattn_w = ...\n",
                "        selfattn_out = self.dropout(selfattn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (1)\n",
                "        x = ...\n",
                "\n",
                "        #\u00a0TODO: FFN block\n",
                "        ffn_out = ...\n",
                "        ffn_out = self.dropout(ffn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (2)\n",
                "        x = ...\n",
                "\n",
                "        if return_att:\n",
                "            return x, selfattn_w\n",
                "        else:\n",
                "            return x"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class TransformerEncoder(nn.Module):\n",
                "\n",
                "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, vocab_size, dropout=0.0):\n",
                "        super(TransformerEncoder, self).__init__()\n",
                "\n",
                "        #\u00a0Create an embedding table (T x B -> T x B x embed_dim)\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "\n",
                "        # Create the positional encoding with the class defined before\n",
                "        self.pos_enc = PositionalEncoding(embed_dim)\n",
                "\n",
                "        self.layers = nn.ModuleList([\n",
                "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "\n",
                "    def forward(self, x, mask=None, return_att=False):\n",
                "        x = self.embedding(x)\n",
                "        x = self.pos_enc(x)\n",
                "\n",
                "        selfattn_ws = []\n",
                "        for l in self.layers:\n",
                "            if return_att:\n",
                "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
                "                selfattn_ws.append(selfattn_w)\n",
                "            else:\n",
                "                x = l(x, mask=mask, return_att=False)\n",
                "\n",
                "        if return_att:\n",
                "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
                "            return x, selfattn_ws\n",
                "        else:\n",
                "            return x"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_l87e4EjOxMz"
            },
            "source": [
                "We have our Transformer Encoder implemented now! Let's try to do a forward pass:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transformer_encoder_cfg = {\n",
                "    \"num_layers\": 6,\n",
                "    \"embed_dim\": 512,\n",
                "    \"ffn_dim\": 2048,\n",
                "    \"num_heads\": 8,\n",
                "    \"vocab_size\": 8000,\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "\n",
                "transformer_encoder = TransformerEncoder(**transformer_encoder_cfg)\n",
                "\n",
                "src_batch_example = torch.randint(transformer_encoder_cfg['vocab_size'], (20, 4))\n",
                "\n",
                "encoder_out, attn_ws = transformer_encoder(src_batch_example, return_att=True)\n",
                "\n",
                "print(f\"Encoder output: {encoder_out.shape}\")\n",
                "print(f\"Self-Attention weights: {attn_ws.shape}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IPQUzo9PPJBB"
            },
            "source": [
                "We have built a random batch ($T\\ x\\ B$) containing $B=4$ sentences of length $T=20$.\n",
                "\n",
                "The output we get is ($T\\ x\\ B\\ x\\ embed\\_dim$) and the self-attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T\\ x\\ T$)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MyZaNmToLILp"
            },
            "source": [
                "### Decoder\n",
                "\n",
                "The Decoder has a similar structure than the Encoder but with two main differences.\n",
                "\n",
                "First, in addition to self-attention, it needs to attend to the encoder outputs. With this purpose, it includes an Encoder-Decoder attention block between the Self-Attention and the FFN. This new module, also based on MHA, uses the encoder outputs (also known as `memory`) as the keys and values.\n",
                "\n",
                "Secondly, the self-attention of the decoder cannot attend to \"future\" samples, because at inference time it works autorregresively. For this reason, we use a triangular mask in the self-attention.\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" height=\"600px\" alt=\"Transformer Encoder & Decoder\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class TransformerDecoderLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
                "            ffn_dim (int): Inner dimensionality in the FFN\n",
                "            num_heads (int): Number of heads of the multi-head attention block\n",
                "            dropout (float): Dropout probability\n",
                "        \"\"\"\n",
                "        super(TransformerDecoderLayer, self).__init__()\n",
                "\n",
                "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(embed_dim, ffn_dim),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(ffn_dim, embed_dim)\n",
                "        )\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        self.norm3 = nn.LayerNorm(embed_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
                "        tgt_len, batch_size, _ = x.shape\n",
                "        src_len, _, _ = memory.shape\n",
                "        if mask is None:\n",
                "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
                "            mask = mask.bool().to(x.device)\n",
                "        if memory_mask is None:\n",
                "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
                "            memory_mask = memory_mask.bool().to(memory.device)\n",
                "\n",
                "\n",
                "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
                "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
                "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
                "        \n",
                "        attn_mask = memory_mask.unsqueeze(-2)\n",
                "\n",
                "        # TODO: Self-Attention block\n",
                "        selfattn_out, selfattn_w = ...\n",
                "        selfattn_out = self.dropout(selfattn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (1)\n",
                "        x = ...\n",
                "\n",
                "        # TODO: Encoder-Decoder Attention block\n",
                "        attn_out, attn_w = ...\n",
                "        attn_out = self.dropout(attn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (2)\n",
                "        x = ...\n",
                "\n",
                "        #\u00a0TODO: FFN block\n",
                "        ffn_out = ...\n",
                "        ffn_out = self.dropout(ffn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (3)\n",
                "        x = ...\n",
                "\n",
                "        if return_att:\n",
                "            return x, selfattn_w, attn_w\n",
                "        else:\n",
                "            return x"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class TransformerDecoder(nn.Module):\n",
                "\n",
                "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, vocab_size, dropout=0.0):\n",
                "        super(TransformerDecoder, self).__init__()\n",
                "\n",
                "        #\u00a0Create an embedding table (T x B -> T x B x embed_dim)\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "\n",
                "        # Create the positional encoding with the class defined before\n",
                "        self.pos_enc = PositionalEncoding(embed_dim)\n",
                "\n",
                "        self.layers = nn.ModuleList([\n",
                "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "\n",
                "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
                "        self.proj = nn.Linear(embed_dim, vocab_size)\n",
                "\n",
                "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
                "        x = self.embedding(x)\n",
                "        x = self.pos_enc(x)\n",
                "\n",
                "        selfattn_ws = []\n",
                "        attn_ws = []\n",
                "        for l in self.layers:\n",
                "            if return_att:\n",
                "                x, selfattn_w, attn_w = l(\n",
                "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
                "                )\n",
                "                selfattn_ws.append(selfattn_w)\n",
                "                attn_ws.append(attn_w)\n",
                "            else:\n",
                "                x = l(\n",
                "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
                "                )\n",
                "\n",
                "        x = self.proj(x)\n",
                "        x = F.log_softmax(x, dim=-1)\n",
                "\n",
                "        if return_att:\n",
                "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
                "            attn_ws = torch.stack(attn_ws, dim=1)\n",
                "            return x, selfattn_ws, attn_ws\n",
                "        else:\n",
                "            return x"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "py5elDvWR92M"
            },
            "source": [
                "And now we also have our Transformer Decoder implemented! Let's try to do a forward pass:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transformer_decoder_cfg = {\n",
                "    \"num_layers\": 6,\n",
                "    \"embed_dim\": 512,\n",
                "    \"ffn_dim\": 2048,\n",
                "    \"num_heads\": 8,\n",
                "    \"vocab_size\": 8000,\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "\n",
                "transformer_decoder = TransformerDecoder(**transformer_decoder_cfg)\n",
                "\n",
                "tgt_batch_example = torch.randint(transformer_decoder_cfg['vocab_size'], (15, 4))\n",
                "\n",
                "decoder_out, selfattn_ws, attn_ws  = transformer_decoder(\n",
                "    tgt_batch_example,\n",
                "    memory=encoder_out,\n",
                "    return_att=True\n",
                ")\n",
                "\n",
                "print(f\"Decoder output: {decoder_out.shape}\")\n",
                "print(f\"Self-Attention weights: {selfattn_ws.shape}\")\n",
                "print(f\"Enc-Dec Attention weights: {attn_ws.shape}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ba3nJcx7Sa1Y"
            },
            "source": [
                "We have built a random target batch ($T_{tgt}\\ x\\ B$) containing $B=4$ sentences of length $T_{tgt}=15$, and we already had the output of the encoder fo size ($T_{src}\\ x\\ B\\ x\\ embed\\_dim$).\n",
                "\n",
                "The output we get from the decoder is ($T_{tgt}\\ x\\ B\\ x\\ vocab\\_size$), the self-attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T_{tgt}\\ x\\ T_{tgt}$), and the enc-dec attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T_{tgt}\\ x\\ T_{src}$)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R3shhXxMRGlF"
            },
            "source": [
                "### Transformer\n",
                "\n",
                "We already have all the components of the Transformer, it's time to put them all together!\n",
                "\n",
                "Note that we will implement two methods to generate results, one to be used during training (whole sequence in parallel) and another to be used during inference (autorregresive generation). The first one is depicted in the original Transformer figure, while the second can be seen in the animation below:\n",
                "\n",
                "\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/new_ModalNet-21.jpg\" height=\"600px\" alt=\"Transformer\"/>\n",
                "</p>\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\" height=\"600px\" alt=\"Autorregressive decoding\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(self, encoder_config, decoder_config):\n",
                "        super(Transformer, self).__init__()\n",
                "        self.encoder = TransformerEncoder(**encoder_config)\n",
                "        self.decoder = TransformerDecoder(**decoder_config)\n",
                "        \n",
                "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
                "        \"\"\" Forward method\n",
                "        \n",
                "        Method used at training time, when the target is known. The target tensor\n",
                "        passed to the decoder is shifted to the right (starting with BOS\n",
                "        symbol). Then, the output of the decoder starts directly with the first\n",
                "        token of the sentence.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Compute the encoder output\n",
                "        encoder_out = ...\n",
                "\n",
                "        # TODO: Compute the decoder output\n",
                "        decoder_out = self.decoder(\n",
                "            x=...\n",
                "            memory=...\n",
                "            mask=...\n",
                "            memory_mask=...\n",
                "        )\n",
                "        \n",
                "        return decoder_out\n",
                "\n",
                "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
                "        \"\"\" Generate method\n",
                "        \n",
                "        Method used at inference time, when the target is unknown. It\n",
                "        iteratively passes to the decoder the sequence generated so far\n",
                "        and appends the new token to the input again. It uses a Greedy\n",
                "        decoding (argmax).\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Compute the encoder output\n",
                "        encoder_out = ...\n",
                "\n",
                "        output = torch.LongTensor([bos_idx])\\\n",
                "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
                "        for i in range(max_len):\n",
                "            # TODO: Get the new token\n",
                "            new_token = self.decoder(\n",
                "                x=...,\n",
                "                memory=...\n",
                "                memory_mask=...\n",
                "            )[-1].argmax(-1)\n",
                "\n",
                "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
                "\n",
                "        return output"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transformer = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transformer(src_batch_example, tgt_batch_example).shape"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Hn3pMor_VkBS"
            },
            "source": [
                "You got it! You have built your own Transformer!\n",
                "\n",
                "**NOTE: Most of the modules we've implemented are available in `torch.nn`, you don't need to copy all this code the next time you want to use a Transformer ;D**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AvwHMaDvUiK4"
            },
            "source": [
                "## Training your new Transformer\n",
                "\n",
                "We will train our Transformer on a simple task, consisting of translating from numbers to their English written form."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "!pip install -q git+https://github.com/gegallego/seq2seq-numbers-dataset.git"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from seq2seq_numbers_dataset import generate_dataset_pytorch, Seq2SeqNumbersCollater\n",
                "\n",
                "numbers_dataset = generate_dataset_pytorch()\n",
                "\n",
                "# Downsample the dataset to reduce training time (remove for better performance)\n",
                "numbers_dataset['train'].src_sents = numbers_dataset['train'].src_sents[:25000]\n",
                "numbers_dataset['train'].tgt_sents = numbers_dataset['train'].tgt_sents[:25000]\n",
                "\n",
                "collater = Seq2SeqNumbersCollater(\n",
                "    numbers_dataset['train'].src_dict,\n",
                "    numbers_dataset['train'].tgt_dict,\n",
                ")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VNX1tbV9MlJ-"
            },
            "source": [
                "###\u00a0Training"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "lr = 5e-4\n",
                "batch_size = 32\n",
                "log_interval = 50\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "\n",
                "numbers_loader_train = DataLoader(\n",
                "    numbers_dataset['train'],\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "transformer_encoder_cfg = {\n",
                "    \"num_layers\": 3,\n",
                "    \"embed_dim\": 256,\n",
                "    \"ffn_dim\": 1024,\n",
                "    \"num_heads\": 4,\n",
                "    \"vocab_size\": len(src_dict),\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "transformer_decoder_cfg = {\n",
                "    \"num_layers\": 3,\n",
                "    \"embed_dim\": 256,\n",
                "    \"ffn_dim\": 1024,\n",
                "    \"num_heads\": 4,\n",
                "    \"vocab_size\": len(tgt_dict),\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
                "model.to(device)\n",
                "model.train()\n",
                "\n",
                "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "criterion = F.nll_loss\n",
                "\n",
                "print(\"Training model...\")\n",
                "\n",
                "loss_avg = 0\n",
                "for i, (src, tgt) in enumerate(numbers_loader_train):\n",
                "    src = {k: v.to(device) for k, v in src.items()}\n",
                "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "    optimizer.zero_grad()\n",
                "\n",
                "    output = model(\n",
                "        src['ids'],\n",
                "        tgt['ids'][:-1],\n",
                "        src['padding_mask'],\n",
                "        tgt['padding_mask'][:, :-1],\n",
                "    )\n",
                "\n",
                "    loss = criterion(\n",
                "        output.reshape(-1, output.size(-1)),\n",
                "        tgt['ids'][1:].flatten()\n",
                "    )\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "    loss_avg += loss.item()\n",
                "    if (i+1) % log_interval == 0:\n",
                "        loss_avg /= log_interval\n",
                "        print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HCMH_slyF7U9"
            },
            "source": [
                "### Testing"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "batch_size_test = 128\n",
                "log_interval_test = 50\n",
                "\n",
                "numbers_loader_test = DataLoader(\n",
                "    numbers_dataset['test'],\n",
                "    batch_size=batch_size_test,\n",
                "    shuffle=False,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "model.eval()\n",
                "\n",
                "print(\"\\nTesting model...\")\n",
                "\n",
                "n_correct = 0\n",
                "n_total = 0\n",
                "for i, (src, tgt) in enumerate(numbers_loader_test):\n",
                "    src = {k: v.to(device) for k, v in src.items()}\n",
                "    tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "    output = model.generate(\n",
                "        src['ids'],\n",
                "        src_mask=src['padding_mask'],\n",
                "        bos_idx=numbers_dataset['test'].tgt_dict.bos_idx(),\n",
                "    )\n",
                "    output = output[:tgt['ids'].size(0)]\n",
                "\n",
                "    n_correct += torch.eq(tgt['ids'], output).sum()\n",
                "    n_total += tgt['ids'].numel()\n",
                "    if (i+1) % log_interval_test == 0:\n",
                "        print(f\"{i+1}/{len(numbers_loader_test)}\")\n",
                "\n",
                "print(f\"Test Accuracy: {100 * n_correct / n_total}%\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e3FtdjQ_MgEb"
            },
            "source": [
                "###\u00a0Inference\n",
                "\n",
                "Check how the model works by selecting any number."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@title  { run: \"auto\" }\n",
                "#@markdown Select a number to pass to the model:\n",
                "input_num = 29284.3 #@param {type:\"slider\", min:-100000, max:100000, step:0.1}\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "input_num_str = \"{:,.2f}\".format(input_num)\n",
                "input_num_enc = torch.LongTensor(\n",
                "    src_dict.encode(input_num_str)\n",
                ").unsqueeze(-1).to(device)\n",
                "\n",
                "output_word_enc = model.generate(\n",
                "    input_num_enc,\n",
                "    bos_idx=tgt_dict.bos_idx()\n",
                ")\n",
                "\n",
                "output_word = tgt_dict.decode(\n",
                "    output_word_enc.flatten().tolist()\n",
                ")\n",
                "\n",
                "print(f\"Input: {input_num_str}\")\n",
                "print(f\"Output: {output_word}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "m86Z54jkMOXr"
            },
            "source": [
                "###\u00a0Attention visualization\n",
                "\n",
                "Analyze the attention weights with the following tool. Are they how you expected?"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@title  { run: \"auto\" }\n",
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@markdown Select a sample from the dataset:\n",
                "dataset_index =   0#@param {type:\"integer\"}\n",
                "#@markdown Select the attention to visualize:\n",
                "attention = \"encoder-decoder attention\" #@param [\"encoder-decoder attention\", \"encoder self-attention\", \"decoder self-attention\"]\n",
                "#@markdown Select the layer:\n",
                "layer = \"2\" #@param [\"3\", \"2\", \"1\"]\n",
                "#@markdown Select a head (or average them):\n",
                "head = \"avg\" #@param [\"avg\", \"1\", \"2\", \"3\", \"4\"]\n",
                "\n",
                "src, tgt = collater(\n",
                "    [numbers_dataset[\"train\"][dataset_index]]\n",
                ")\n",
                "src = {k: v.to(device) for k, v in src.items()}\n",
                "tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "enc_output, enc_selfattn = model.encoder(\n",
                "    src['ids'],\n",
                "    src['padding_mask'],\n",
                "    return_att=True,\n",
                ")\n",
                "\n",
                "dec_output, dec_selfattn, encdec_attn = model.decoder(\n",
                "    tgt['ids'][:-1],\n",
                "    enc_output,\n",
                "    tgt['padding_mask'][:, :-1],\n",
                "    src['padding_mask'],\n",
                "    return_att=True,\n",
                ")\n",
                "\n",
                "if attention==\"encoder-decoder attention\":\n",
                "    attention_w = encdec_attn\n",
                "    queries = [tgt_dict[i] for i in dec_output.argmax(-1)[:,0].tolist()]\n",
                "    keys = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "    ytitle = \"Output tokens\"\n",
                "\n",
                "\n",
                "elif attention==\"encoder self-attention\":\n",
                "    attention_w = enc_selfattn\n",
                "    queries = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "    keys = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "\n",
                "elif attention==\"decoder self-attention\":\n",
                "    attention_w = dec_selfattn\n",
                "    queries = [tgt_dict[i] for i in dec_output.argmax(-1)[:,0].tolist()]\n",
                "    keys = [tgt_dict[i] for i in tgt['ids'][:-1, 0].tolist()]\n",
                "    ytitle = \"Output tokens\"\n",
                "\n",
                "if head == \"avg\":\n",
                "    attention_w = attention_w[0][int(layer)-1].mean(0)\n",
                "else:\n",
                "    attention_w = attention_w[0][int(layer)-1][int(head)-1]\n",
                "\n",
                "plot_attention(\n",
                "    attention_w,\n",
                "    queries,\n",
                "    keys,\n",
                "    ytitle=ytitle,\n",
                ")\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "y1U3Qj-66ysR"
            },
            "source": [
                "## References\n",
                "\n",
                "The images are from:\n",
                "- https://jalammar.github.io/illustrated-transformer/\n",
                "- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
                "- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
                "- https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
                "- https://paperswithcode.com/method/layer-normalization\n",
                "- https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png\n",
                "- https://paperswithcode.com/method/scaled\n",
                "\n",
                "The code is partially inspired by:\n",
                "- https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
                "- https://nlp.seas.harvard.edu/2018/04/03/attention.html"
            ]
        }
    ]
}
