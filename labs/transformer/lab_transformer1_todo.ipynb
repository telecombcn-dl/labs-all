{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4ZsnaW2_s_iB"
            },
            "source": [
                "# Build your own Transformer\n",
                "\n",
                "Created by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) for the [Postgraduate Course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) ([UPC School](https://www.talent.upc.edu/ing/), 2021).\n",
                "\n",
                "Updated by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) (2024)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "FlLmT6kia5GO"
            },
            "source": [
                "In this lab we will learn about the [Transformer](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html), a popular architecture that revolutionized Deep Learning a few years ago.\n",
                "\n",
                "This architecture was firstly designed for Machine Translation. In this field, Recurrent Neural Networks (e.g. LSTM) had been the state-of-the-art since [the introduction of the Attention mechanism](https://arxiv.org/abs/1409.0473) in 2015. The Transformer surpassed them by introducing a key idea: getting rid of any recurrence and mainly using the **(Self-)Attention** mechanism, as the title of the paper states: [Attention is All you Need](https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).\n",
                "\n",
                "Actually, Transformer-based architectures are used in many fields beyond Machine Translation. First, many models arised for other text-related tasks (e.g. [BERT](https://arxiv.org/abs/1810.04805) or [GPT-3](https://arxiv.org/abs/2005.14165)), but now they're also used in other fields, like [Speech processing](https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html) and [Computer Vision](https://arxiv.org/abs/2010.11929).\n",
                "\n",
                "Throughout this notebook, we will build our own Transformer, and you'll understand module by module how this architecture works. Once it's finished, we will train it with a dummy dataset and we will try to interpret the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "import math\n",
                "import time\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pylab as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oP-hS-tdOqgk"
            },
            "source": [
                "##\u00a0Transformer architecture\n",
                "\n",
                "Here we go!\n",
                "\n",
                "In the left part of the figure below, you can see the Transformer architecture. Take a look at it carefully to get used to all this new terminology.\n",
                "\n",
                "You can also see a breakdown of the most important module in the Transfomer: the Multi-Head Attention, which is based on the Scaled-Dot Product Attention.\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=\"1000px\" alt=\"Zoom in to the Transformer\"/>\n",
                "</p>\n",
                "\n",
                "Don't panic! ;) We will start from the most simple structure and we will build upon it, little by little. Let's start with the Scaled Dot-Product Attention!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8xk2rD305QTJ"
            },
            "source": [
                "### Scaled Dot-Product Attention\n",
                "\n",
                "The first key idea to understand how the Transformer works is the Scaled Dot Product Attention (we will call it SDPA from now on).\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/SCALDE.png\" height=\"300px\" alt=\"Scaled Dot-Product Attention\"/>\n",
                "</p>\n",
                "\n",
                "This is the equation:\n",
                "\n",
                "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
                "\n",
                "You are probably trying to figure out what $Q$, $K$ and $V$ are, right? They are the queries, the keys and the values. A good example to understand these concepts is the one given by [@dontloo](https://stats.stackexchange.com/users/95569/dontloo) in StackExchange\n",
                "\n",
                ">[...]\n",
                ">\n",
                ">The key/value/query concepts come from retrieval systems. For example, when you type a query to search for some video on Youtube, the search engine will map your query against a set of keys (video title, description, etc.) associated with candidate videos in the database, then present you the best matched videos (values).\n",
                ">\n",
                ">The attention operation turns out can be thought of as a retrieval process as well, so the key/value/query concepts also apply here.\n",
                ">\n",
                ">[...]\n",
                ">\n",
                ">[[Read the full answer]](https://stats.stackexchange.com/a/424127)\n",
                "\n",
                "This keys/values/query can represent tokens in a sentence. Furthermore, the attention function can be computed on a set of queries simultaneously. Hence, they are packed together into a matrix $Q$, like the keys ($K$) and the values ($V$). Take into account that you need the same amount of keys and values, but the number of queries can differ.\n",
                "\n",
                "Also, why is the scaling needed? Well, it turns out that for high-dimensional keys (large $d_k$) the dot-product grow large in magnitude, hurting the gradients.\n",
                "\n",
                "Finally, note that in the figure there is a module called \"Module (opt.)\" which doesn't appear in the equation. This allows us to control which values can the queries \"attend\" to. This will be useful, for example, to avoid attention to padding tokens, which we use to batch sentences of different length."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scaled_dot_product(q, k, v, attn_mask=None):\n",
                "    \"\"\" Computes the Scaled Dot-Product Attention\n",
                "\n",
                "    Args:\n",
                "        q (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
                "        k (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
                "        v (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
                "        attn_mask (torch.BoolTensor): Attention mask (... x T_q x T_k)\n",
                "\n",
                "    Returns:\n",
                "        torch.FloatTensor: Result of the SDPA  (... x T_q x d_v)\n",
                "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
                "\n",
                "    \"\"\"\n",
                "    assert q.size(-1) == k.size(-1), \"Query and Key dimensions must coincide\"\n",
                "\n",
                "    # TODO: Matrix multiplication of the queries and the keys (use torch.matmul)\n",
                "    attn_logits = ...\n",
                "\n",
                "    # TODO: Scale attn_logits (see the SDPA formula, d_k is the last dim of k)\n",
                "    attn_logits = ...\n",
                "\n",
                "    if attn_mask is not None:\n",
                "        attn_logits = attn_logits.masked_fill(~attn_mask, -float(\"inf\"))\n",
                "\n",
                "    # TODO: Compute the attention weights (see the SDPA formula, use dim=-1)\n",
                "    attention = ...\n",
                "\n",
                "    output = torch.matmul(attention, v)\n",
                "\n",
                "    return output, attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_attention(attention, queries, keys, xtitle=\"Keys\", ytitle=\"Queries\"):\n",
                "    \"\"\" Plots the attention map\n",
                "\n",
                "    Args:\n",
                "        att (torch.FloatTensor): Attention map (T_q x T_k)\n",
                "        queries (List[str]): Query Tensor\n",
                "        keys (List[str]): Key Tensor\n",
                "    \"\"\"\n",
                "\n",
                "    sns.set(rc={'figure.figsize':(12, 8)})\n",
                "    ax = sns.heatmap(\n",
                "        attention.detach().cpu(),\n",
                "        linewidth=0.5,\n",
                "        xticklabels=keys,\n",
                "        yticklabels=queries,\n",
                "        cmap=\"coolwarm\")\n",
                "\n",
                "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
                "    ax.set_xlabel(xtitle)\n",
                "    ax.set_ylabel(ytitle)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "r_qeDynpxEAh"
            },
            "source": [
                "Let's create some random queries, keys and values, with the following dimensions:\n",
                "- $T_Q=5$\n",
                "- $T_K=T_V=8$\n",
                "- $d_Q=d_K=d_V=4$\n",
                "\n",
                "We will use them to test our SDPA function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "q = torch.randn(5, 4)\n",
                "k = torch.randn(8, 4)\n",
                "v = torch.randn(8, 4)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output, attention = scaled_dot_product(q, k, v)\n",
                "\n",
                "print(f\"Output:\\n{output}\\n{output.shape}\\n\")\n",
                "print(f\"Attention weights:\\n{attention}\\n{attention.shape}\\n\")\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                "    [str([round(float(k__), 1) for k__ in k_]) for k_ in k],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0M5G6XU3txyF"
            },
            "source": [
                "After computing the SDPA, we get:\n",
                "- The output, of dimensions $T_Q x d_V$\n",
                "- The attention weights, which relate the queries and the keys, of dimensions $T_Q x T_K$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Lk0fglFM6E91"
            },
            "source": [
                "But you've already heard about Self-Attention again, right?\n",
                "\n",
                "Basically, Self-Attention consists of using the same set of vectors as queries, keys and values. Let's try it:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.randn(5, 4)\n",
                "output, attention = scaled_dot_product(q=x, k=x, v=x)\n",
                "\n",
                "print(f\"Output:\\n{output}\\n{output.shape}\\n\")\n",
                "print(f\"Attention weights:\\n{attention}\\n{attention.shape}\\n\")\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                "    [str([round(float(q__), 1) for q__ in q_]) for q_ in q],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Rv6iR7Y0wWR3"
            },
            "source": [
                "But then... that's all, \"Attention is all you need\"?\n",
                "\n",
                "Well, no, **it's not enough with just attention**... We need learnable parameters somewhere!\n",
                "\n",
                "Actually, the inputs of the SDPA need to be projected with a Linear layer. This way we can get different representations from the same input vectors. We do something like this:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://miro.medium.com/max/1578/1*_92bnsMJy8Bl539G4v93yg.gif\" height=\"600px\" alt=\"Self-Attention\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hFc8IdXL_UoF"
            },
            "source": [
                "Here you can find an implementation of this \"learnable\" SDPA. Be aware that this class will not used by the model we will implement, it's just an intermediate step we use now for didactic purposes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LearnableScaledDotProductAttention(nn.Module):\n",
                "    def __init__(self, embed_dim):\n",
                "        super(LearnableScaledDotProductAttention, self).__init__()\n",
                "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
                "\n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        q = self.proj_q(q)\n",
                "        k = self.proj_k(k)\n",
                "        v = self.proj_v(v)\n",
                "        output, _ = scaled_dot_product(q, k, v, mask)\n",
                "        return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NSpzJMgwAK7n"
            },
            "source": [
                "Let's test that it can learn now, by trying to reconstruct the input tensor with self-attention:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sdpa = LearnableScaledDotProductAttention(embed_dim=4)\n",
                "optimizer = optim.Adam(sdpa.parameters())\n",
                "\n",
                "losses_sdpa = []\n",
                "n_epochs = 10000\n",
                "for i in range(n_epochs):\n",
                "    optimizer.zero_grad()\n",
                "    output = sdpa(q=x, k=x, v=x)    # Self-attention\n",
                "    loss = F.mse_loss(output, x)    # Reconstruct the input\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses_sdpa.append(loss.item())\n",
                "    if (i + 1) % 1000 == 0:\n",
                "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
                "\n",
                "\n",
                "print(f\"\\nOutput:\\n{output}\\n\")\n",
                "print(f\"Query:\\n{x}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rzyQtcqUA2uo"
            },
            "source": [
                "Ok, looks good, we can train it. It's starting to make sense now, right?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zHJiKgHqeyrO"
            },
            "source": [
                "### Multi-Head Attention\n",
                "\n",
                "To further exploit this attention mechanism, the original paper introduced the Multi-Head Attention mechanism (MHA).\n",
                "\n",
                "Instead of performing the attention mechanism just once, they found it benefitial to project the input multiple times into different \"attention heads\". This way, multiple attentions can be learned at the same time.\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png\" height=\"400px\" alt=\"Scaled Dot-Product Attention\"/>\n",
                "</p>\n",
                "\n",
                "To combine the outputs of each head, they are concatenated and projected by a Linear transformation $W^O$, as defined by the following equation:\n",
                "\n",
                "$$\n",
                "MultiHead(Q, K, V ) = Concat(head_1, ..., head_h)W^O\n",
                "$$\n",
                "$$\n",
                "where\\ head_i = Attention(QW^Q_i, KW^K_i, V W^V_i)\n",
                "$$\n",
                "\n",
                "Note that $W^Q_i$, $W^K_i$, $W^V_i$ are the Linear projections we've seen in the previous section.\n",
                "\n",
                "To better understand this, see the following illustration created by [Jay Alammar](https://jalammar.github.io/) for his famous blog post [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/):\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png\" height=\"600px\" alt=\"Self-Attention\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiheadAttention(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, num_heads):\n",
                "        super(MultiheadAttention, self).__init__()\n",
                "        assert embed_dim % num_heads == 0, \\\n",
                "            \"Embedding dimension must be multiple of the number of heads.\"\n",
                "\n",
                "        self.embed_dim = embed_dim\n",
                "        self.num_heads = num_heads\n",
                "        self.head_dim = embed_dim // num_heads\n",
                "\n",
                "        self.proj_q = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_k = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_v = nn.Linear(embed_dim, embed_dim)\n",
                "        self.proj_o = nn.Linear(embed_dim, embed_dim)\n",
                "\n",
                "        self.sdpa = scaled_dot_product\n",
                "\n",
                "        self._reset_parameters()\n",
                "\n",
                "    def _reset_parameters(self):\n",
                "        # Original Transformer initialization\n",
                "        nn.init.xavier_uniform_(self.proj_q.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_k.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_v.weight)\n",
                "        nn.init.xavier_uniform_(self.proj_o.weight)\n",
                "        self.proj_q.bias.data.fill_(0)\n",
                "        self.proj_k.bias.data.fill_(0)\n",
                "        self.proj_v.bias.data.fill_(0)\n",
                "        self.proj_o.bias.data.fill_(0)\n",
                "\n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        batch_size = q.size(1)\n",
                "\n",
                "        q = self.proj_q(q)\n",
                "        k = self.proj_k(k)\n",
                "        v = self.proj_v(v)\n",
                "\n",
                "        # TODO: Split the tensors into multiple heads\n",
                "        #  T x B x embed_dim -> T x B x num_heads x head_dim\n",
                "        q = q.reshape(...)\n",
                "        k = k.reshape(...)\n",
                "        v = v.reshape(...)\n",
                "\n",
                "        # The last two dimensions must be sequence length and the head dimension,\n",
                "        # to make it work with the scaled dot-product function.\n",
                "        # TODO: Rearrange the dimensions\n",
                "        #\u00a0T x B x num_heads x head_dim -> B x num_heads x T x head_dim\n",
                "        q = q.permute(...)\n",
                "        k = k.permute(...)\n",
                "        v = v.permute(...)\n",
                "\n",
                "        # Apply the same mask to all the heads\n",
                "        if mask is not None:\n",
                "            mask = mask.unsqueeze(1)\n",
                "            attn_mask = ~mask\n",
                "        else:\n",
                "            attn_mask = None\n",
                "\n",
                "        # TODO: Call the scaled dot-product function (remember to pass the mask!)\n",
                "        attn_result = self.sdpa(...)\n",
                "\n",
                "        # Check if `attn_result` contains one or two outputs.\n",
                "        # If it contains two outputs (a tuple), unpack them into `output_heads` and `attn_w`.\n",
                "        # Otherwise, it means no attention weights are provided.\n",
                "        if isinstance(attn_result, tuple) and len(attn_result) == 2:\n",
                "            output_heads, attn_w = attn_result\n",
                "        else:\n",
                "            output_heads, attn_w = attn_result, None\n",
                "\n",
                "        # B x num_heads x T x head_dim -> T x B x num_heads x head_dim\n",
                "        output_heads = output_heads.permute(2, 0, 1, 3)\n",
                "\n",
                "        # T x B x num_heads x head_dim -> T x B x embed_dim\n",
                "        output_cat = output_heads.reshape(-1, batch_size, self.embed_dim)\n",
                "        output = self.proj_o(output_cat)\n",
                "\n",
                "        return output, attn_w"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2xzZ19kbDAnH"
            },
            "source": [
                "Let's test the same dummy example than before, trying to reconstruct the input tensor with self-attention, this time with a MHA module:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mha = MultiheadAttention(embed_dim=4, num_heads=2)\n",
                "optimizer = optim.Adam(mha.parameters())\n",
                "\n",
                "losses_mha = []\n",
                "n_epochs = 10000\n",
                "for i in range(n_epochs):\n",
                "    optimizer.zero_grad()\n",
                "    output = mha(                # Self-attention\n",
                "        q=x.unsqueeze(1),\n",
                "        k=x.unsqueeze(1),\n",
                "        v=x.unsqueeze(1)\n",
                "    )[0].squeeze(1)\n",
                "    loss = F.mse_loss(output, x) # Reconstruct input\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses_mha.append(loss.item())\n",
                "    if (i + 1) % 1000 == 0:\n",
                "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
                "\n",
                "print(f\"\\nOutput:\\n{output}\\n\")\n",
                "print(f\"Query:\\n{x}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c_j2pJpPGz4y"
            },
            "source": [
                "Ok, seems fine, it learns!\n",
                "\n",
                "At this point, you already know all you need about attention in the Transformer :D Now, we need to know where to apply it.\n",
                "\n",
                "But before looking again at the Transformer architecture, we need to make a small stop at the Positional Encoding."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OB06UfyPekC3"
            },
            "source": [
                "###\u00a0Positional Encoding\n",
                "\n",
                "As you may know, RNNs are designed in a way that they know the ordering of the input tokens. This is very important when processing sequences such as text sentences.\n",
                "\n",
                "However, when using Self-attention, there is no positional information used between the queries, keys and values. For this reason, the Transformer authors needed to give positional information to the model explicitly.\n",
                "\n",
                "They simply decided to create an embedding table, with the following equation, which is summed to the embeddings of the inputs.\n",
                "\n",
                "$$\n",
                "PE_{(pos,2i) = sin(pos\\ /\\ 10000^{2i\\ /\\ d\\_model})}\n",
                "$$\n",
                "$$\n",
                "PE_{(pos,2i+1) = cos(pos\\ /\\ 10000^{2i\\ /\\ d\\_model})}\n",
                "$$\n",
                "where $pos$ is the position and $i$ is the dimension.\n",
                "\n",
                "The resulting positional embedding table is like this:\n",
                "<p align=\"center\">\n",
                "<img src=\"https://d33wubrfki0l68.cloudfront.net/ef81ee3018af6ab6f23769031f8961afcdd67c68/3358f/img/transformer_architecture_positional_encoding/positional_encoding.png\" height=\"350px\" alt=\"Positional Encoding\"\"/>\n",
                "</p>\n",
                "\n",
                "For more information about the positional encoding, check [this post](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, max_len=5000):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality\n",
                "            max_len (int): Maximum length of a sequence to expect\n",
                "        \"\"\"\n",
                "        super(PositionalEncoding, self).__init__()\n",
                "\n",
                "        # Create matrix of (T x embed_dim) representing the positional encoding\n",
                "        # for max_len inputs\n",
                "        pe = torch.zeros(max_len, embed_dim)\n",
                "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
                "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
                "        pe[:, 0::2] = torch.sin(position * div_term)\n",
                "        pe[:, 1::2] = torch.cos(position * div_term)\n",
                "        pe = pe.unsqueeze(1)\n",
                "\n",
                "        self.register_buffer('pe', pe, persistent=False)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x + self.pe[:x.size(0)]\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dfYCl_awY7FN"
            },
            "source": [
                "### Encoder\n",
                "\n",
                "Now the time has come to put everything together and build the Transformer Encoder!\n",
                "\n",
                "This structure mainly consists of a stack of layers defined as below:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png\" height=\"600px\" alt=\"Transformer Encoder\"/>\n",
                "</p>\n",
                "\n",
                "The [Layer Normalization](https://arxiv.org/abs/1607.06450) differs from Batch Normalization in that it works element-wise, as seen in the following figure:\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png\" height=\"250px\" alt=\"LayerNormalization\"/>\n",
                "</p>\n",
                "\n",
                "The Feed Forward layers consist of a projection to a higher dimension (`ffn_dim`), a ReLU activation and another projection to the original dimension (`embed_dim`). They are defined by the following equation:\n",
                "\n",
                "$$\n",
                "FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
                "$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerEncoderLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
                "            ffn_dim (int): Inner dimensionality in the FFN\n",
                "            num_heads (int): Number of heads of the multi-head attention block\n",
                "            dropout (float): Dropout probability\n",
                "        \"\"\"\n",
                "        super(TransformerEncoderLayer, self).__init__()\n",
                "\n",
                "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(embed_dim, ffn_dim),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(ffn_dim, embed_dim)\n",
                "        )\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, mask=None, return_att=False):\n",
                "        src_len, batch_size, _ = x.shape\n",
                "        if mask is None:\n",
                "            mask = torch.zeros(x.shape[1], x.shape[0]).bool().to(x.device)\n",
                "\n",
                "        selfattn_mask = mask.unsqueeze(-2)\n",
                "\n",
                "        # TODO: Self-Attention block\n",
                "        selfattn_out, selfattn_w = ...\n",
                "        selfattn_out = self.dropout(selfattn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (1)\n",
                "        x = ...\n",
                "\n",
                "        #\u00a0TODO: FFN block\n",
                "        ffn_out = ...\n",
                "        ffn_out = self.dropout(ffn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (2)\n",
                "        x = ...\n",
                "\n",
                "        if return_att:\n",
                "            return x, selfattn_w\n",
                "        else:\n",
                "            return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerEncoder(nn.Module):\n",
                "\n",
                "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, vocab_size, dropout=0.0):\n",
                "        super(TransformerEncoder, self).__init__()\n",
                "\n",
                "        #\u00a0Create an embedding table (T x B -> T x B x embed_dim)\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "\n",
                "        # Create the positional encoding with the class defined before\n",
                "        self.pos_enc = PositionalEncoding(embed_dim)\n",
                "\n",
                "        self.layers = nn.ModuleList([\n",
                "            TransformerEncoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "\n",
                "    def forward(self, x, mask=None, return_att=False):\n",
                "        x = self.embedding(x)\n",
                "        x = self.pos_enc(x)\n",
                "\n",
                "        selfattn_ws = []\n",
                "        for l in self.layers:\n",
                "            if return_att:\n",
                "                x, selfattn_w = l(x, mask=mask, return_att=True)\n",
                "                selfattn_ws.append(selfattn_w)\n",
                "            else:\n",
                "                x = l(x, mask=mask, return_att=False)\n",
                "\n",
                "        if return_att:\n",
                "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
                "            return x, selfattn_ws\n",
                "        else:\n",
                "            return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_l87e4EjOxMz"
            },
            "source": [
                "We have our Transformer Encoder implemented now! Let's try to do a forward pass:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transformer_encoder_cfg = {\n",
                "    \"num_layers\": 6,\n",
                "    \"embed_dim\": 512,\n",
                "    \"ffn_dim\": 2048,\n",
                "    \"num_heads\": 8,\n",
                "    \"vocab_size\": 8000,\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "\n",
                "transformer_encoder = TransformerEncoder(**transformer_encoder_cfg)\n",
                "\n",
                "src_batch_example = torch.randint(transformer_encoder_cfg['vocab_size'], (20, 4))\n",
                "\n",
                "encoder_out, attn_ws = transformer_encoder(src_batch_example, return_att=True)\n",
                "\n",
                "print(f\"Encoder output: {encoder_out.shape}\")\n",
                "print(f\"Self-Attention weights: {attn_ws.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IPQUzo9PPJBB"
            },
            "source": [
                "We have built a random batch ($T\\ x\\ B$) containing $B=4$ sentences of length $T=20$.\n",
                "\n",
                "The output we get is ($T\\ x\\ B\\ x\\ embed\\_dim$) and the self-attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T\\ x\\ T$)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MyZaNmToLILp"
            },
            "source": [
                "### Decoder\n",
                "\n",
                "The Decoder has a similar structure than the Encoder but with two main differences.\n",
                "\n",
                "First, in addition to self-attention, it needs to attend to the encoder outputs. With this purpose, it includes an Encoder-Decoder attention block between the Self-Attention and the FFN. This new module, also based on MHA, uses the encoder outputs (also known as `memory`) as the keys and values.\n",
                "\n",
                "Secondly, the self-attention of the decoder cannot attend to \"future\" samples, because at inference time it works autorregresively. For this reason, we use a triangular mask in the self-attention.\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\" height=\"600px\" alt=\"Transformer Encoder & Decoder\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerDecoderLayer(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, ffn_dim, num_heads, dropout=0.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality (input, output & self-attention)\n",
                "            ffn_dim (int): Inner dimensionality in the FFN\n",
                "            num_heads (int): Number of heads of the multi-head attention block\n",
                "            dropout (float): Dropout probability\n",
                "        \"\"\"\n",
                "        super(TransformerDecoderLayer, self).__init__()\n",
                "\n",
                "        self.self_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.encdec_attn = MultiheadAttention(embed_dim, num_heads)\n",
                "        self.ffn = nn.Sequential(\n",
                "            nn.Linear(embed_dim, ffn_dim),\n",
                "            nn.Dropout(dropout),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(ffn_dim, embed_dim)\n",
                "        )\n",
                "        self.norm1 = nn.LayerNorm(embed_dim)\n",
                "        self.norm2 = nn.LayerNorm(embed_dim)\n",
                "        self.norm3 = nn.LayerNorm(embed_dim)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
                "        tgt_len, batch_size, _ = x.shape\n",
                "        src_len, _, _ = memory.shape\n",
                "        if mask is None:\n",
                "            mask = torch.zeros(x.shape[1], x.shape[0])\n",
                "            mask = mask.bool().to(x.device)\n",
                "        if memory_mask is None:\n",
                "            memory_mask = torch.zeros(memory.shape[1], memory.shape[0])\n",
                "            memory_mask = memory_mask.bool().to(memory.device)\n",
                "\n",
                "\n",
                "        subsequent_mask = torch.triu(torch.ones(batch_size, tgt_len, tgt_len), 1)\n",
                "        subsequent_mask = subsequent_mask.bool().to(mask.device)\n",
                "        selfattn_mask = subsequent_mask + mask.unsqueeze(-2)\n",
                "\n",
                "        attn_mask = memory_mask.unsqueeze(-2)\n",
                "\n",
                "        # TODO: Self-Attention block\n",
                "        selfattn_out, selfattn_w = ...\n",
                "        selfattn_out = self.dropout(selfattn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (1)\n",
                "        x = ...\n",
                "\n",
                "        # TODO: Encoder-Decoder Attention block\n",
                "        attn_out, attn_w = ...\n",
                "        attn_out = self.dropout(attn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (2)\n",
                "        x = ...\n",
                "\n",
                "        #\u00a0TODO: FFN block\n",
                "        ffn_out = ...\n",
                "        ffn_out = self.dropout(ffn_out)\n",
                "\n",
                "        # TODO: Add + normalize block (3)\n",
                "        x = ...\n",
                "\n",
                "        if return_att:\n",
                "            return x, selfattn_w, attn_w\n",
                "        else:\n",
                "            return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TransformerDecoder(nn.Module):\n",
                "\n",
                "    def __init__(self, num_layers, embed_dim, ffn_dim, num_heads, vocab_size, dropout=0.0):\n",
                "        super(TransformerDecoder, self).__init__()\n",
                "\n",
                "        #\u00a0Create an embedding table (T x B -> T x B x embed_dim)\n",
                "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
                "\n",
                "        # Create the positional encoding with the class defined before\n",
                "        self.pos_enc = PositionalEncoding(embed_dim)\n",
                "\n",
                "        self.layers = nn.ModuleList([\n",
                "            TransformerDecoderLayer(embed_dim, ffn_dim, num_heads, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ])\n",
                "\n",
                "        # Add a projection layer (T x B x embed_dim -> T x B x vocab_size)\n",
                "        self.proj = nn.Linear(embed_dim, vocab_size)\n",
                "\n",
                "    def forward(self, x, memory, mask=None, memory_mask=None, return_att=False):\n",
                "        x = self.embedding(x)\n",
                "        x = self.pos_enc(x)\n",
                "\n",
                "        selfattn_ws = []\n",
                "        attn_ws = []\n",
                "        for l in self.layers:\n",
                "            if return_att:\n",
                "                x, selfattn_w, attn_w = l(\n",
                "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=True\n",
                "                )\n",
                "                selfattn_ws.append(selfattn_w)\n",
                "                attn_ws.append(attn_w)\n",
                "            else:\n",
                "                x = l(\n",
                "                    x, memory, mask=mask, memory_mask=memory_mask, return_att=False\n",
                "                )\n",
                "\n",
                "        x = self.proj(x)\n",
                "        x = F.log_softmax(x, dim=-1)\n",
                "\n",
                "        if return_att:\n",
                "            selfattn_ws = torch.stack(selfattn_ws, dim=1)\n",
                "            attn_ws = torch.stack(attn_ws, dim=1)\n",
                "            return x, selfattn_ws, attn_ws\n",
                "        else:\n",
                "            return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "py5elDvWR92M"
            },
            "source": [
                "And now we also have our Transformer Decoder implemented! Let's try to do a forward pass:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transformer_decoder_cfg = {\n",
                "    \"num_layers\": 6,\n",
                "    \"embed_dim\": 512,\n",
                "    \"ffn_dim\": 2048,\n",
                "    \"num_heads\": 8,\n",
                "    \"vocab_size\": 8000,\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "\n",
                "transformer_decoder = TransformerDecoder(**transformer_decoder_cfg)\n",
                "\n",
                "tgt_batch_example = torch.randint(transformer_decoder_cfg['vocab_size'], (15, 4))\n",
                "\n",
                "decoder_out, selfattn_ws, attn_ws  = transformer_decoder(\n",
                "    tgt_batch_example,\n",
                "    memory=encoder_out,\n",
                "    return_att=True\n",
                ")\n",
                "\n",
                "print(f\"Decoder output: {decoder_out.shape}\")\n",
                "print(f\"Self-Attention weights: {selfattn_ws.shape}\")\n",
                "print(f\"Enc-Dec Attention weights: {attn_ws.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ba3nJcx7Sa1Y"
            },
            "source": [
                "We have built a random target batch ($T_{tgt}\\ x\\ B$) containing $B=4$ sentences of length $T_{tgt}=15$, and we already had the output of the encoder fo size ($T_{src}\\ x\\ B\\ x\\ embed\\_dim$).\n",
                "\n",
                "The output we get from the decoder is ($T_{tgt}\\ x\\ B\\ x\\ vocab\\_size$), the self-attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T_{tgt}\\ x\\ T_{tgt}$), and the enc-dec attention weights are ($B\\ x\\ num\\_layers\\ x\\ num\\_heads\\ x \\ T_{tgt}\\ x\\ T_{src}$)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R3shhXxMRGlF"
            },
            "source": [
                "### Transformer\n",
                "\n",
                "We already have all the components of the Transformer, it's time to put them all together!\n",
                "\n",
                "Note that we will implement two methods to generate results, one to be used during training (whole sequence in parallel) and another to be used during inference (autorregresive generation). The first one is depicted in the original Transformer figure, while the second can be seen in the animation below:\n",
                "\n",
                "\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://paperswithcode.com/media/methods/new_ModalNet-21.jpg\" height=\"600px\" alt=\"Transformer\"/>\n",
                "</p>\n",
                "\n",
                "<p align=\"center\">\n",
                "<img src=\"https://jalammar.github.io/images/t/transformer_decoding_2.gif\" height=\"600px\" alt=\"Autorregressive decoding\"/>\n",
                "</p>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(self, encoder_config, decoder_config):\n",
                "        super(Transformer, self).__init__()\n",
                "        self.encoder = TransformerEncoder(**encoder_config)\n",
                "        self.decoder = TransformerDecoder(**decoder_config)\n",
                "\n",
                "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
                "        \"\"\" Forward method\n",
                "\n",
                "        Method used at training time, when the target is known. The target tensor\n",
                "        passed to the decoder is shifted to the right (starting with BOS\n",
                "        symbol). Then, the output of the decoder starts directly with the first\n",
                "        token of the sentence.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Compute the encoder output\n",
                "        encoder_out = ...\n",
                "\n",
                "        # TODO: Compute the decoder output\n",
                "        decoder_out = self.decoder(\n",
                "            x=...\n",
                "            memory=...\n",
                "            mask=...\n",
                "            memory_mask=...\n",
                "        )\n",
                "\n",
                "        return decoder_out\n",
                "\n",
                "    def generate(self, src, src_mask=None, bos_idx=0, max_len=50):\n",
                "        \"\"\" Generate method\n",
                "\n",
                "        Method used at inference time, when the target is unknown. It\n",
                "        iteratively passes to the decoder the sequence generated so far\n",
                "        and appends the new token to the input again. It uses a Greedy\n",
                "        decoding (argmax).\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Compute the encoder output\n",
                "        encoder_out = ...\n",
                "\n",
                "        output = torch.LongTensor([bos_idx])\\\n",
                "                    .expand(1, encoder_out.size(1)).to(src.device)\n",
                "        for i in range(max_len):\n",
                "            # TODO: Get the new token\n",
                "            new_token = self.decoder(\n",
                "                x=...,\n",
                "                memory=...\n",
                "                memory_mask=...\n",
                "            )[-1].argmax(-1)\n",
                "\n",
                "            output = torch.cat([output, new_token.unsqueeze(0)], dim=0)\n",
                "\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transformer = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transformer(src_batch_example, tgt_batch_example).shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Hn3pMor_VkBS"
            },
            "source": [
                "You got it! You have built your own Transformer!\n",
                "\n",
                "**NOTE: Most of the modules we've implemented are available in `torch.nn`, you don't need to copy all this code the next time you want to use a Transformer ;D**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AvwHMaDvUiK4"
            },
            "source": [
                "## Training your new Transformer\n",
                "\n",
                "We will train our Transformer on a simple task, consisting of translating from numbers to their English written form."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q git+https://github.com/gegallego/seq2seq-numbers-dataset.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from seq2seq_numbers_dataset import generate_dataset_pytorch, Seq2SeqNumbersCollater\n",
                "\n",
                "numbers_dataset = generate_dataset_pytorch()\n",
                "\n",
                "# Downsample the dataset to reduce training time (remove for better performance)\n",
                "numbers_dataset['train'].src_sents = numbers_dataset['train'].src_sents[:25000]\n",
                "numbers_dataset['train'].tgt_sents = numbers_dataset['train'].tgt_sents[:25000]\n",
                "\n",
                "# Downsample the dataset to reduce evaluation time\n",
                "numbers_dataset['test'].src_sents = numbers_dataset['test'].src_sents[:7500]\n",
                "numbers_dataset['test'].tgt_sents = numbers_dataset['test'].tgt_sents[:7500]\n",
                "\n",
                "collater = Seq2SeqNumbersCollater(\n",
                "    numbers_dataset['train'].src_dict,\n",
                "    numbers_dataset['train'].tgt_dict,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(model, dataloader, lr, batch_size, device, log_interval=50):\n",
                "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
                "    criterion = F.nll_loss\n",
                "\n",
                "    model.to(device)\n",
                "    model.train()\n",
                "\n",
                "    print(\"Training model...\")\n",
                "    torch.cuda.synchronize()\n",
                "    ini = time.time()\n",
                "\n",
                "    loss_avg = 0\n",
                "    for i, (src, tgt) in enumerate(dataloader):\n",
                "        src = {k: v.to(device) for k, v in src.items()}\n",
                "        tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        output = model(\n",
                "            src['ids'],\n",
                "            tgt['ids'][:-1],\n",
                "            src['padding_mask'],\n",
                "            tgt['padding_mask'][:, :-1],\n",
                "        )\n",
                "\n",
                "        loss = criterion(\n",
                "            output.reshape(-1, output.size(-1)),\n",
                "            tgt['ids'][1:].flatten()\n",
                "        )\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        loss_avg += loss.item()\n",
                "        if (i+1) % log_interval == 0:\n",
                "            loss_avg /= log_interval\n",
                "            print(f\"{i+1}/{len(numbers_loader_train)}\\tLoss: {loss_avg}\")\n",
                "\n",
                "    torch.cuda.synchronize()\n",
                "    end = time.time()\n",
                "    secs = round(end - ini, 2)\n",
                "    print(f\"Done training (took {secs} seconds)\")\n",
                "\n",
                "    return model\n",
                "\n",
                "\n",
                "def test(model, dataloader, batch_size, device, log_interval=50):\n",
                "    model.eval()\n",
                "\n",
                "    print(\"\\nTesting model...\")\n",
                "    torch.cuda.synchronize()\n",
                "    ini = time.time()\n",
                "\n",
                "    n_correct = 0\n",
                "    n_total = 0\n",
                "    for i, (src, tgt) in enumerate(dataloader):\n",
                "        src = {k: v.to(device) for k, v in src.items()}\n",
                "        tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "        output = model.generate(\n",
                "            src['ids'],\n",
                "            src_mask=src['padding_mask'],\n",
                "            bos_idx=numbers_dataset['test'].tgt_dict.bos_idx(),\n",
                "        )\n",
                "        output = output[:tgt['ids'].size(0)]\n",
                "\n",
                "        n_correct += torch.eq(tgt['ids'], output).sum().item()\n",
                "        n_total += tgt['ids'].numel()\n",
                "        if (i+1) % log_interval == 0:\n",
                "            print(f\"{i+1}/{len(numbers_loader_test)}\")\n",
                "\n",
                "    acc = round(100 * n_correct / n_total, 2)\n",
                "\n",
                "    torch.cuda.synchronize()\n",
                "    end = time.time()\n",
                "    secs = round(end - ini, 2)\n",
                "    print(f\"Test Accuracy: {acc}% (took {secs} seconds)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size = 32\n",
                "lr = 5e-4\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "\n",
                "numbers_loader_train = DataLoader(\n",
                "    numbers_dataset['train'],\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "transformer_encoder_cfg = {\n",
                "    \"num_layers\": 3,\n",
                "    \"embed_dim\": 256,\n",
                "    \"ffn_dim\": 1024,\n",
                "    \"num_heads\": 4,\n",
                "    \"vocab_size\": len(src_dict),\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "transformer_decoder_cfg = {\n",
                "    \"num_layers\": 3,\n",
                "    \"embed_dim\": 256,\n",
                "    \"ffn_dim\": 1024,\n",
                "    \"num_heads\": 4,\n",
                "    \"vocab_size\": len(tgt_dict),\n",
                "    \"dropout\": 0.1,\n",
                "}\n",
                "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VNX1tbV9MlJ-"
            },
            "source": [
                "### Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = train(\n",
                "    model,\n",
                "    numbers_loader_train,\n",
                "    lr,\n",
                "    batch_size,\n",
                "    device=device\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HCMH_slyF7U9"
            },
            "source": [
                "### Testing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "batch_size_test = 128\n",
                "\n",
                "numbers_loader_test = DataLoader(\n",
                "    numbers_dataset['test'],\n",
                "    batch_size=batch_size_test,\n",
                "    shuffle=False,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "test(model, numbers_loader_test, batch_size_test, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "e3FtdjQ_MgEb"
            },
            "source": [
                "###\u00a0Inference\n",
                "\n",
                "Check how the model works by selecting any number."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@title  { run: \"auto\" }\n",
                "#@markdown Select a number to pass to the model:\n",
                "input_num = 29284.3 #@param {type:\"slider\", min:-100000, max:100000, step:0.1}\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "input_num_str = \"{:,.2f}\".format(input_num)\n",
                "input_num_enc = torch.LongTensor(\n",
                "    src_dict.encode(input_num_str)\n",
                ").unsqueeze(-1).to(device)\n",
                "\n",
                "output_word_enc = model.generate(\n",
                "    input_num_enc,\n",
                "    bos_idx=tgt_dict.bos_idx()\n",
                ")\n",
                "\n",
                "output_word = tgt_dict.decode(\n",
                "    output_word_enc.flatten().tolist()\n",
                ")\n",
                "\n",
                "print(f\"Input: {input_num_str}\")\n",
                "print(f\"Output: {output_word}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "m86Z54jkMOXr"
            },
            "source": [
                "###\u00a0Attention visualization\n",
                "\n",
                "Analyze the attention weights with the following tool. Are they how you expected?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@title  { run: \"auto\" }\n",
                "#@title  { run: \"auto\", vertical-output: true }\n",
                "#@markdown Select a sample from the dataset:\n",
                "dataset_index =   0#@param {type:\"integer\"}\n",
                "#@markdown Select the attention to visualize:\n",
                "attention = \"encoder-decoder attention\" #@param [\"encoder-decoder attention\", \"encoder self-attention\", \"decoder self-attention\"]\n",
                "#@markdown Select the layer:\n",
                "layer = \"2\" #@param [\"3\", \"2\", \"1\"]\n",
                "#@markdown Select a head (or average them):\n",
                "head = \"avg\" #@param [\"avg\", \"1\", \"2\", \"3\", \"4\"]\n",
                "\n",
                "src, tgt = collater(\n",
                "    [numbers_dataset[\"train\"][dataset_index]]\n",
                ")\n",
                "src = {k: v.to(device) for k, v in src.items()}\n",
                "tgt = {k: v.to(device) for k, v in tgt.items()}\n",
                "\n",
                "src_dict = numbers_dataset['train'].src_dict\n",
                "tgt_dict = numbers_dataset['train'].tgt_dict\n",
                "\n",
                "enc_output, enc_selfattn = model.encoder(\n",
                "    src['ids'],\n",
                "    src['padding_mask'],\n",
                "    return_att=True,\n",
                ")\n",
                "\n",
                "dec_output, dec_selfattn, encdec_attn = model.decoder(\n",
                "    tgt['ids'][:-1],\n",
                "    enc_output,\n",
                "    tgt['padding_mask'][:, :-1],\n",
                "    src['padding_mask'],\n",
                "    return_att=True,\n",
                ")\n",
                "\n",
                "if attention==\"encoder-decoder attention\":\n",
                "    attention_w = encdec_attn\n",
                "    queries = [tgt_dict[i] for i in dec_output.argmax(-1)[:,0].tolist()]\n",
                "    keys = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "    ytitle = \"Output tokens\"\n",
                "\n",
                "\n",
                "elif attention==\"encoder self-attention\":\n",
                "    attention_w = enc_selfattn\n",
                "    queries = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "    keys = [src_dict[i] for i in src['ids'][:, 0].tolist()]\n",
                "\n",
                "elif attention==\"decoder self-attention\":\n",
                "    attention_w = dec_selfattn\n",
                "    queries = [tgt_dict[i] for i in dec_output.argmax(-1)[:,0].tolist()]\n",
                "    keys = [tgt_dict[i] for i in tgt['ids'][:-1, 0].tolist()]\n",
                "    ytitle = \"Output tokens\"\n",
                "\n",
                "if head == \"avg\":\n",
                "    attention_w = attention_w[0][int(layer)-1].mean(0)\n",
                "else:\n",
                "    attention_w = attention_w[0][int(layer)-1][int(head)-1]\n",
                "\n",
                "plot_attention(\n",
                "    attention_w,\n",
                "    queries,\n",
                "    keys,\n",
                "    ytitle=ytitle,\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LCHK_7BE6N9L"
            },
            "source": [
                "## Extra: Variations of your Transformer\n",
                "\n",
                "Congratulations on implementing and successfully training your own Transformer! In this section, we will explore ways to extend and modify its functionality by introducing variations commonly used in advanced Transformer designs. These exercises will give you hands-on experience with some of the latest innovations in Transformer models. Specifically, we will work on:\n",
                "- **Learnable Positional Encodings**: Replace the fixed sinusoidal positional encodings with learnable embeddings, allowing the model to adapt positional information to the specific dataset.\n",
                "- **Optimized Scaled Dot-Product Attention**: Introduce more efficient implementations of attention mechanisms to reduce computational overhead and memory usage.\n",
                "- **Multi-Query Attention**: Experiment with a variant of Multi-Head Attention that reduces the number of keys and values, improving efficiency without significantly impacting performance.\n",
                "\n",
                "To implement these modifications, we will use a technique called [monkey patching](https://en.wikipedia.org/wiki/Monkey_patch). This allows us to replace or modify specific parts of your existing implementation at runtime. While this is not the most robust or maintainable approach for production code, it enables rapid prototyping by avoiding extensive rewrites of earlier sections.\n",
                "\n",
                "Please note that monkey patching should be used with caution in real-world projects. In practice, a cleaner approach would involve subclassing or re-implementing components to maintain modularity and code clarity."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uh8PkubcWV2R"
            },
            "source": [
                "### Learnable Positional Encoding\n",
                "\n",
                "In this section, we will implement Learnable Positional Encodings, where the positional information is represented as learnable parameters. This approach allows the model to optimize positional embeddings during training, potentially improving performance on tasks with unique positional patterns.\n",
                "\n",
                "Create an embedding table (`nn.Embedding`) to represent positional encoding up to `max_len` inputs. During the forward pass, you'll need to encode positions with it, and sum the resulting embeddings to the input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LearnablePositionalEncoding(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, max_len=512):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            embed_dim (int): Embedding dimensionality\n",
                "            max_len (int): Maximum length of a sequence to expect\n",
                "        \"\"\"\n",
                "        super(LearnablePositionalEncoding, self).__init__()\n",
                "\n",
                "        # TODO: Create the embedding table\n",
                "        self.pe = nn.Embedding(\n",
                "            num_embeddings=...\n",
                "            embedding_dim=...\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        seq_len, batch_size, _ = x.shape\n",
                "\n",
                "        # TODO: Prepare a tensor containing the position of each element in the sequence.\n",
                "        # Consider that `x` is (T x B x D), and that you need to generate (T x B).\n",
                "        # Hint: You can use the `torch.arange` function.\n",
                "        positions = ...\n",
                "\n",
                "        positions = positions.to(x.device)\n",
                "\n",
                "        # T x B -> T x B  x embed_dim\n",
                "        pos_emb = self.pe(positions)\n",
                "\n",
                "        return x + pos_emb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "G2nGmSXmtYuD"
            },
            "source": [
                "Let's instantiate our Transformer class, and apply a monkey patch to substitute the original sinusoidal positional encoding by the new one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
                "\n",
                "model.encoder.pos_enc = LearnablePositionalEncoding(transformer_encoder_cfg[\"embed_dim\"])\n",
                "model.decoder.pos_enc = LearnablePositionalEncoding(transformer_decoder_cfg[\"embed_dim\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reinitialize the dataloaders\n",
                "numbers_loader_train = DataLoader(\n",
                "    numbers_dataset['train'],\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "numbers_loader_test = DataLoader(\n",
                "    numbers_dataset['test'],\n",
                "    batch_size=batch_size_test,\n",
                "    shuffle=False,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "# Train and test the model again\n",
                "model = train(\n",
                "    model,\n",
                "    numbers_loader_train,\n",
                "    lr,\n",
                "    batch_size,\n",
                "    device=device\n",
                ")\n",
                "test(model, numbers_loader_test, batch_size_test, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OHS_K6oIysjF"
            },
            "source": [
                "If your model achieved similar accuracy to the previous implementation, congratulations, it worked! Since we are working on a relatively simple task, there was not much room for improvement. However, you have now gained a deeper understanding of how learnable positional encodings work and how they can serve as an alternative to fixed sinusoidal encodings."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "lrg1zpF7WF0o"
            },
            "source": [
                "### Optimized Scaled Dot-Product Attention\n",
                "\n",
                "In our Transformer, we implemented the scaled dot-product attention (SDPA) with a custom function. While functional, this implementation may not take full advantage of the hardware acceleration and optimizations available in PyTorch's native implementation, [`F.scaled_dot_product_attention`](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html).\n",
                "\n",
                "In this section, we will replace all attention computations with PyTorch's native function. This optimized implementation not only improves computational efficiency but also allows seamless switching to advanced backends like FlashAttention, significantly enhancing speed and memory usage for large models and long sequences.\n",
                "\n",
                "As in the previous exercise, we will integrate the optimized function using monkey patching to avoid modifying other parts of our Transformer. To do so, override the `sdpa` attribute in every `MultiHeadAttention`, which defines the function to be used, by `F.scaled_dot_product_attention`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
                "\n",
                "# TODO: Override `sdpa` in every MultiHeadAttention to use PyTorch's native function\n",
                "for l in model.encoder.layers:\n",
                "    l.self_attn.sdpa = ...\n",
                "for l in ...\n",
                "    ...\n",
                "    ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numbers_loader_train = DataLoader(\n",
                "    numbers_dataset['train'],\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "numbers_loader_test = DataLoader(\n",
                "    numbers_dataset['test'],\n",
                "    batch_size=batch_size_test,\n",
                "    shuffle=False,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "with nn.attention.sdpa_kernel(nn.attention.SDPBackend.EFFICIENT_ATTENTION):\n",
                "    model = train(\n",
                "        model,\n",
                "        numbers_loader_train,\n",
                "        lr,\n",
                "        batch_size,\n",
                "        device=device\n",
                "    )\n",
                "    test(model, numbers_loader_test, batch_size_test, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-Nwtun_7baP8"
            },
            "source": [
                "You may have noticed that training and evaluation were slightly faster with the optimized implementation (maybe just a bit?). However, the true benefits of efficient attention mechanisms become more apparent when working with long sequences, which is not the case for our very simple task.\n",
                "\n",
                "Additionally, we used the `EFFICIENT_ATTENTION` backend for this exercise. You can find a full list of available backends and their descriptions [here](https://pytorch.org/docs/stable/generated/torch.nn.attention.SDPBackend.html). Unfortunately, we could not use the more advanced `FLASH_ATTENTION` backend due to hardware limitations on the free tier of Google Colab, as it requires more modern GPUs. If you have access to compatible hardware, consider experimenting with `FLASH_ATTENTION` to explore its significant performance advantages."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MK2vl3sFWOq5"
            },
            "source": [
                "### Multi-Query Attention\n",
                "\n",
                "Multi-Head Attention is one of the key innovations of the Transformer architecture, allowing the model to focus on multiple aspects of the input simultaneously. However, as the number of attention heads increases, the computational and memory costs grow significantly.\n",
                "\n",
                "Multi-Query Attention provides a more efficient alternative by sharing a single set of key-value pairs across all attention heads while keeping separate query projections for each head. This approach reduces the computational overhead, particularly in tasks with long sequences, without sacrificing much of the flexibility of traditional multi-head attention.\n",
                "\n",
                "In this section, you will implement Multi-Query Attention by modifying the previously defined Multi-Head Attention class."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5A2N9Jf-rZCY"
            },
            "source": [
                "Copy the code from the `MultiheadAttention` class and modify it accordingly to do Multi-Query Attention."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiQueryAttention(nn.Module):\n",
                "\n",
                "    def __init__(self, embed_dim, num_heads):\n",
                "        super(MultiQueryAttention, self).__init__()\n",
                "        # TODO\n",
                "        ...\n",
                "\n",
                "    def _reset_parameters(self):\n",
                "        # TODO\n",
                "        ...\n",
                "\n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        # TODO\n",
                "        ...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "s5uWM6lorZCY"
            },
            "source": [
                "Similarly than before, let's test it with the same dummy example, trying to reconstruct the input tensor with self-attention:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.randn(5, 4)\n",
                "\n",
                "mha = MultiQueryAttention(embed_dim=4, num_heads=2)\n",
                "optimizer = optim.Adam(mha.parameters())\n",
                "\n",
                "losses_mha = []\n",
                "n_epochs = 10000\n",
                "for i in range(n_epochs):\n",
                "    optimizer.zero_grad()\n",
                "    output = mha(                # Self-attention\n",
                "        q=x.unsqueeze(1),\n",
                "        k=x.unsqueeze(1),\n",
                "        v=x.unsqueeze(1)\n",
                "    )[0].squeeze(1)\n",
                "    loss = F.mse_loss(output, x) # Reconstruct input\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    losses_mha.append(loss.item())\n",
                "    if (i + 1) % 1000 == 0:\n",
                "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
                "\n",
                "print(f\"\\nOutput:\\n{output}\\n\")\n",
                "print(f\"Query:\\n{x}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LR9So--qrZCZ"
            },
            "source": [
                "Let's instantiate our Transformer class, and apply a monkey patch to substitute the original `MultiheadAttention` with the new `MultiQueryAttention` class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = Transformer(transformer_encoder_cfg, transformer_decoder_cfg)\n",
                "\n",
                "for l in model.encoder.layers:\n",
                "    l.self_attn = MultiQueryAttention(\n",
                "        embed_dim=l.self_attn.embed_dim,\n",
                "        num_heads=l.self_attn.num_heads,\n",
                "    )\n",
                "\n",
                "for l in model.decoder.layers:\n",
                "    l.self_attn = MultiQueryAttention(\n",
                "        embed_dim=l.self_attn.embed_dim,\n",
                "        num_heads=l.self_attn.num_heads,\n",
                "    )\n",
                "    l.encdec_attn = MultiQueryAttention(\n",
                "        embed_dim=l.encdec_attn.embed_dim,\n",
                "        num_heads=l.encdec_attn.num_heads,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "numbers_loader_train = DataLoader(\n",
                "    numbers_dataset['train'],\n",
                "    batch_size=batch_size,\n",
                "    shuffle=True,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "numbers_loader_test = DataLoader(\n",
                "    numbers_dataset['test'],\n",
                "    batch_size=batch_size_test,\n",
                "    shuffle=False,\n",
                "    collate_fn=collater,\n",
                ")\n",
                "\n",
                "model = train(\n",
                "    model,\n",
                "    numbers_loader_train,\n",
                "    lr,\n",
                "    batch_size,\n",
                "    device=device\n",
                ")\n",
                "test(model, numbers_loader_test, batch_size_test, device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "BgGh2BpRrZCa"
            },
            "source": [
                "You may not notice substantial differences in performance and efficiency when running this exercise, as the true advantages of Multi-Query Attention become evident with much longer sequences. However, if you've successfully implemented this variation, congratulations! You've taken an important step towards understanding how to adapt and optimize the core mechanisms of the Transformer architecture."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tg6SUefMrZCa"
            },
            "source": [
                "By completing this notebook, you've gained hands-on experience building a Transformer from scratch, exploring key architectural components, and implementing advanced variations such as learnable positional encodings, different attention backends, and Multi-Query Attention. These skills will serve as a solid foundation for experimenting with more sophisticated Transformer models and tailoring them to specific tasks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "y1U3Qj-66ysR"
            },
            "source": [
                "## References\n",
                "\n",
                "The images are from:\n",
                "- https://jalammar.github.io/illustrated-transformer/\n",
                "- https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
                "- https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
                "- https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
                "- https://paperswithcode.com/method/layer-normalization\n",
                "- https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png\n",
                "- https://paperswithcode.com/method/scaled\n",
                "\n",
                "The code is partially inspired by:\n",
                "- https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html\n",
                "- https://nlp.seas.harvard.edu/2018/04/03/attention.html"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
