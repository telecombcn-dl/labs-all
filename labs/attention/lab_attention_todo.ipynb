{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "c-WJMjRIwPwu"
            },
            "source": [
                "# Attention\n",
                "\n",
                "Created by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/), [Laia Tarr\u00e9s](https://www.linkedin.com/in/laia-tarres/) and [Javier Ferrando](https://www.linkedin.com/in/javierferrandomonsonis/) for the [Postgraduate Course in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) ([UPC School](https://www.talent.upc.edu/ing/), 2022).\n",
                "\n",
                "Updated by [Ioannis Tsiamas](https://www.linkedin.com/in/i-tsiamas/) (2023).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dUc3bLtmysUN"
            },
            "source": [
                "Resources:\n",
                "\n",
                "*   [This book, that has a chapter on attention](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)\n",
                "*   [Lab 1: Attention](https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/attention.ipynb#scrollTo=DzqrgWQMv3nQ)\n",
                "*   [Lab 2: Attention transformer intuition](https://colab.research.google.com/github/alvinntnu/NTNU_ENC2045_LECTURES/blob/main/nlp/dl-attention-transformer-intuition.ipynb#scrollTo=yG-ivNKJYTkE)\n",
                "*   [This article is very well explained](https://lilianweng.github.io/posts/2018-06-24-attention/)\n",
                "*   [This colab for RNN + attention](https://colab.research.google.com/drive/1eObkehym2HauZo-NBYi39aAsWE1ujExk?usp=sharing#scrollTo=roXSIRB_QWDr)\n",
                "*   [This video tutorial on attention](https://youtu.be/w0OZ5iHsamk)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vsMxXDnryEFQ"
            },
            "source": [
                "In this lab we are going to learn about Attention.\n",
                "\n",
                "Attention is a concept in machine learning and AI that goes back many years.\n",
                "Attention was inspired by how human brains deal with the massive amount of visual and audio input.\n",
                "As humans, we are not capable of processing all visual and audio inputs. Instead, we select - or attend to - the important parts of the input to be able make sense of our surroundings.\n",
                "\n",
                "For example, to detect there's a shiba inu dog in the following picture, we focus on the different elements in the face:\n",
                "\n",
                "![shiba attention](https://lilianweng.github.io/posts/2018-06-24-attention/shiba-example-attention.png)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from __future__ import unicode_literals, print_function, division\n",
                "\n",
                "import re\n",
                "import math\n",
                "import random\n",
                "import string\n",
                "import unicodedata\n",
                "from io import open\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch import optim\n",
                "import torch.nn.functional as F\n",
                "\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as ticker\n",
                "\n",
                "import seaborn as sns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_MXK4fuBec6o"
            },
            "source": [
                "To ensure reproducibility of the experiments, we can set the seed to a fixed number."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 123\n",
                "np.random.seed(seed)\n",
                "_ = torch.manual_seed(seed)\n",
                "_ = torch.cuda.manual_seed(seed)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_attention(attention, xtitle=\"Keys\", ytitle=\"Queries\"):\n",
                "    \"\"\" Plots the attention map.\"\"\"\n",
                "\n",
                "    sns.set(rc={'figure.figsize':(12, 8)})\n",
                "    ax = sns.heatmap(\n",
                "        attention.detach().cpu(),\n",
                "        linewidth=0.5,\n",
                "        cmap=\"Blues\",\n",
                "        square=True)\n",
                "\n",
                "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
                "    ax.set_xlabel(xtitle) \n",
                "    ax.set_ylabel(ytitle)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bOBLEt0CIyDe"
            },
            "source": [
                "# Attention Mechanisms\n",
                "\n",
                "https://www.kdnuggets.com/2021/01/attention-mechanism-deep-learning-explained.html\n",
                "\n",
                "In deep learning context, attention is a term used for a family of related mechanisms which, in general, learn to predict some probability distribution over a sequence of elements.\n",
                "\n",
                "Intuitively, this allows a model to \"pay more attention\" to elements from the sequence which get a higher probability weight.\n",
                "\n",
                "In this lab, we are going to dive deep into the **soft-attention** mechanism, the standard approach nowadays. We are going to also try different versions of it, the *additive*, and the multiplicative attentions.\n",
                "\n",
                "An attention mechanism has a least the following components:\n",
                "\n",
                "*   n key-value pairs:  {${(k_i, v_i)}$} $_{i=1}^n$ where $k_i \\in \u211d^{d_k}$ and $v_i \\in \u211d^{d_v}$, representing the same elements in the sequence\n",
                "*   A query: $q \\in \u211d^{d_q}$\n",
                "*   Some similarity function (sometimes called *energy*) between keys and queries: $e :  \u211d^{d_k}$ x $\u211d^{d_q}$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7CO5kgiUgF5b"
            },
            "source": [
                "\n",
                "\n",
                "## How the attention output is calculated?\n",
                "\n",
                "### Soft-attention mechanism\n",
                "\n",
                "A soft-attention mechanism performs a linear combination of the values. The weights associated with each value in the linear combination are computed by comparing the query with every key. The more similar the query is to the key, the larger the weight of the associated value in the attention output.\n",
                "\n",
                "More formally, the output of the attention mechanism is computed as:\n",
                "$ o = \\sum_{i=1}^n a_i v_i \\in \u211d^{d_v}$\n",
                "\n",
                "Each attention weights $a_i$ is computed by measuring the similarity between the query and the $i$-th key. First we obtain\n",
                "$b_i = e(k_i,q)$, and then, by applying $e$ to all $i$, we get a vector $b = [b_1,...,b_n]^T$, which is transformed into the probability simplex thanks to the softmax function, $a = \\text{softmax} (b)$.\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/soft_attention.png?raw=true\" class=\"center\" title=\"Soft-attention mechanism\" width=\"600\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vfc0SIGbu4uy"
            },
            "source": [
                "# How is the query-key comparison calculated?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Mps6zyrMrpE-"
            },
            "source": [
                "# Exercise 1: MultiplicativeAttention\n",
                "\n",
                "The multiplicative attention performs the following operation:\n",
                "\n",
                "$e(k,q) = q W_q k^T$\n",
                "\n",
                "We use a learnable matrix between the query and the key. This is equivalent to projecting the query through $W_q$ and performing the dot-product afterwards.\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/multiplicative_attention.png?raw=true\" class=\"center\" title=\"Mutliplicative attention\" width=\"300\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8phJz0FpkiDs"
            },
            "source": [
                "Let's create one random query, and eight random keys and values, with the following dimensions:\n",
                "- $T_Q=1$\n",
                "- $T_K=T_V=8$\n",
                "- $d_Q=d_K=d_V=128$\n",
                "\n",
                "Let's consider keys and values as representing the same element."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim = 128\n",
                "q = torch.randn(1, dim)\n",
                "k = torch.randn(8, dim)\n",
                "v = k"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Pk0DV4HRmdI0"
            },
            "source": [
                "### Training loop with a dummy task\n",
                "\n",
                "Let's now specify our training function. Since some attention types have learnable weight matrices, we can then train them to accomplish a specific task. Here we define an optimizer (Adam), a loss function (Mean Square Error), and perform a training loop to minimize the distance between the model `output` and the desired `target`.\n",
                "\n",
                "As a use case to check if the model works well, we are going to define the `target` as a linear combination of the values, with `target_attn_weights` as weights. That is, we force the model to compute:\n",
                "\n",
                "$ \\text{target} = \\sum_{i=1}^n \\text{target_attn_weights } v_i \\in \u211d^{d_v}$\n",
                "\n",
                "If we recall the output equation of the attention:\n",
                "\n",
                "$ o = \\sum_{i=1}^n a_i v_i \\in \u211d^{d_v}$\n",
                "\n",
                "we can see that if the model learns to output `target`, we expect it to find attention weights $a= \\text{target_attn_weights}$.\n",
                "\n",
                "Note that the `mse_loss` measures the distance between each component of `target` and `output`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dummy_train(attn_module, q, k, v, target_attn_weights):\n",
                "    optimizer = optim.Adam(attn_module.parameters(), lr=0.0001)\n",
                "    \n",
                "    target = torch.matmul(target_attn_weights, v)\n",
                "\n",
                "    attn_module.train()\n",
                "\n",
                "    n_epochs = 15000\n",
                "    for i in range(n_epochs):\n",
                "        optimizer.zero_grad()\n",
                "        output, attn_weights = attn_module(q, k, v)\n",
                "        loss = F.mse_loss(output, target)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        if (i + 1) % 1000 == 0:\n",
                "            print(f\"Epoch {i+1}/{n_epochs}\")\n",
                "            print(f\"\\tLoss:\\t\\t{loss.item()}\")\n",
                "            print(f\"\\tAttention:\\t{attn_weights.squeeze().detach().numpy().round(3)}\")\n",
                "\n",
                "\n",
                "    attn_module.eval()\n",
                "    output, attn_weights = attn_module(q, k, v)\n",
                "\n",
                "    print(f\"\\nOutput:\\n{output}\\n\")\n",
                "    print(f\"\\nTarget:\\n{target}\\n\")\n",
                "\n",
                "    return output, attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiplicativeAttention(nn.Module):\n",
                "    \"\"\"\n",
                "     Implements plain dot-product and multiplicative attention.\n",
                "     Args:\n",
                "         q_dim (int): dimension of the queries\n",
                "         k_dim (int): dimension of the keys\n",
                "         v_dim (int): dimension of the values\n",
                "         scaling (bool): whether to scale after the dot-product\n",
                "         sub_type (string): specify type of attention: dot-product / multiplicative\n",
                "     Inputs: query, key, value\n",
                "        query (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
                "        key (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
                "        value (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
                "\n",
                "    Returns:\n",
                "        torch.FloatTensor: Result of the Attention Mechanism  (... x T_q x d_v)\n",
                "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
                "\n",
                "    \"\"\"\n",
                "    def __init__(self, q_dim: int, k_dim: int, scaling: bool = False, sub_type: str = 'multiplicative') -> None:\n",
                "        super(MultiplicativeAttention, self).__init__()\n",
                "        self.sub_type = sub_type\n",
                "        if self.sub_type == 'dot_product':\n",
                "            assert q_dim == k_dim\n",
                "            self.scaling = scaling\n",
                "        else:\n",
                "          self.proj_w = nn.Linear(q_dim, k_dim, bias=False)\n",
                "          self.scaling = False                                  # We don't scale in the multiplicative attention\n",
                "\n",
                "    def forward(self, query, key, value):\n",
                "        if self.sub_type == 'dot_product':\n",
                "            qw = query\n",
                "            key_ = key\n",
                "\n",
                "        else:\n",
                "            qw = self.proj_w(query)                             # (... x T_q x q_dim) -> (... x T_q x k_dim)\n",
                "            key_ = key\n",
                "\n",
                "        # TODO: Compute the attention logits from qw and the key\n",
                "        #\u00a0(... x T_q x k_dim) * (... x k_dim x T_k) -> (... x T_q x T_k)\n",
                "        attn_logits = ...\n",
                "\n",
                "        if self.scaling:\n",
                "            attn_logits = attn_logits / math.sqrt(key_.size(-1))\n",
                "\n",
                "        # TODO: Compute the attention weights\n",
                "        attn_weights = ...\n",
                "\n",
                "        output = torch.matmul(attn_weights, value)\n",
                "        return output, attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_attn_weights = torch.Tensor([[0.1, 0.2, 0.1, 0.3, 0.0, 0.2, 0.0, 0.1]])\n",
                "\n",
                "attn_module = MultiplicativeAttention(q_dim=dim, k_dim=dim)\n",
                "output, attn_weights = dummy_train(attn_module, q, k, v, target_attn_weights)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cmXAQCix9wTP"
            },
            "source": [
                "We can see that the model is able to learn to produce an `output` vector very similar to `target` vector, and the attention weights converge to `target_attn_weights`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot attention weights of trained attention\n",
                "\n",
                "output, attention = attn_module(q,k,v)\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CKeimYAWV3nA"
            },
            "source": [
                "## Dot-product attention\n",
                "\n",
                "One basic type of attention mechanism, called dot-product attention, uses a simple dot product as a similarity function.\n",
                "\n",
                "This is the most commonly used type of attention nowadays, as it is used by models based on the Transformer architecture.\n",
                "\n",
                "$e(k,q) = k^T q$\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/dot_product_attn.png?raw=true\" class=\"center\" title=\"Dot-product attention\" width=\"300\"/>\n",
                "</p><br>\n",
                "\n",
                "The dot-product grows due to the dimensionality. Scaling can help reducing the sharpness of the probability distribution after applying the softmax, and therefore prevent tiny gradients. Assuming $d_k = d_q = d$ (all the dimensions are identical), then:\n",
                "\n",
                "$e(k,q) = \\frac{k^T q}{\\sqrt{d}}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attn_module = MultiplicativeAttention(q_dim=dim, k_dim=dim, sub_type='dot_product')\n",
                "output, attention_no_scaling = attn_module(q,k,v)\n",
                "\n",
                "plot_attention(\n",
                "    attention_no_scaling,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XL2I31YAVzEH"
            },
            "source": [
                "We use ```scaling``` argument to choose wether we also want to scale after the dot-product."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "attn_module = MultiplicativeAttention(q_dim=dim, k_dim=dim, sub_type='dot_product', scaling = True)\n",
                "output, attention_scaling = attn_module(q,k,v)\n",
                "\n",
                "plot_attention(\n",
                "    attention_scaling,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "s4mbsTqQUHsJ"
            },
            "source": [
                "You can see that the distribution of attention weights changes, right?\n",
                "Let's use a bar plot to compare them better:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_histogram(input_tensor_dict):\n",
                "    \"\"\"Helper function to make bar plots.\"\"\"\n",
                "    for key in input_tensor_dict.keys():\n",
                "      input_tensor = input_tensor_dict[key].squeeze().cpu().detach().numpy()\n",
                "      plt.bar(range(0,input_tensor.size),input_tensor, alpha=0.6, label = key)\n",
                "    plt.xticks(ticks = range(0,input_tensor.size))\n",
                "    plt.legend()\n",
                "    \n",
                "attn_results_dict = {'No scaling' : attention_no_scaling, 'Scaling' : attention_scaling}\n",
                "plot_histogram(attn_results_dict)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oY12_djps1I4"
            },
            "source": [
                "# Exercise 2: Additive attention\n",
                "\n",
                "In this case, the attention mechanism uses an MLP to learn the similarity function $e(k,q) = tanh(k W_k + q W_q) w^T$ where $W_k \\in \u211d^{h\ud835\ude1fd_k}$, $W_q \\in \u211d^{h\ud835\ude1fd_q}$ and $w \\in \u211d^{h}$ are trainable parameters.\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/additive_attention.png?raw=true\" class=\"center\" title=\"Additive attention\" width=\"700\"/>\n",
                "</p><br>\n",
                "\n",
                "Here we don't assume that the queries and keys have the same dimensionality, because we can project them into a space of a fixed dimension through $W_k$ and $W_q$. Then this is our first layer of the MLP. And then we have the second layer with is just a linear output layer. \n",
                "\n",
                "Notice that we are adding projected versions of the key and query and applying a 2-layer MLP. Both projections and the output layer are trainable.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdditiveAttention(nn.Module):\n",
                "    \"\"\"\n",
                "     Implements the additive attention as proposed in \"Neural Machine Translation by Jointly Learning to Align and Translate\".\n",
                "     Args:\n",
                "         q_dim (int): dimesion of the queries\n",
                "         k_dim (int): dimesion of the keys\n",
                "         attn_dim (int): dimension of intermediate vectors\n",
                "\n",
                "     Inputs: query, key, value\n",
                "        query (torch.FloatTensor):  Query Tensor   (... x T_q x d_q)\n",
                "        key (torch.FloatTensor):  Key Tensor     (... x T_k x d_k)\n",
                "        value (torch.FloatTensor):  Value Tensor   (... x T_v x d_v)\n",
                "\n",
                "    Returns:\n",
                "        torch.FloatTensor: Result of the Attention Mechanism  (... x T_q x d_v)\n",
                "        torch.FloatTensor: Attention map       (... x T_q x T_k)\n",
                "\n",
                "    \"\"\"\n",
                "    def __init__(self, q_dim: int, k_dim: int, attn_dim: int) -> None:\n",
                "        super(AdditiveAttention, self).__init__()\n",
                "        \n",
                "        # TODO: Create projections of queries and keys\n",
                "        self.proj_q = ...\n",
                "        self.proj_k = ...\n",
                "        \n",
                "        self.bias = nn.Parameter(torch.rand(attn_dim).uniform_(-0.1, 0.1))\n",
                "        self.w = nn.Linear(attn_dim, 1)\n",
                "\n",
                "    def forward(self, query, key, value):\n",
                "        q_ = self.proj_q(query) # (... x T_q x q_dim) -> (... x T_q x attn_dim)\n",
                "        k_ = self.proj_k(key)   # (... x T_k x k_dim) -> (... x T_k x attn_dim)\n",
                "\n",
                "        # Prepare for Broadcasting Semantics\n",
                "        q_ = q_.unsqueeze(-2)   # (... x T_q x attn_dim) -> (... x T_q x  1  x attn_dim)\n",
                "        k_ = k_.unsqueeze(-3)   # (... x T_k x attn_dim) -> (... x  1  x T_k x attn_dim)\n",
                "\n",
                "        #\u00a0Sum thanks to Broadcasting Semantics\n",
                "        attn_hid = torch.tanh(q_ + k_ + self.bias) # (... x T_q x  1  x attn_dim) + (... x  1  x T_k x attn_dim) + (attn_dim) -> (... x T_q x T_k x attn_dim)\n",
                "        \n",
                "        attn_logits = self.w(attn_hid)        #\u00a0(... x T_q x T_k x attn_dim) -> (... x T_q x T_k x 1)\n",
                "        attn_logits = attn_logits.squeeze(-1) # (... x T_q x T_k x 1) -> (... x T_q x T_k)\n",
                "\n",
                "        attn_weights = F.softmax(attn_logits, dim=-1)\n",
                "\n",
                "        # TODO: Compute the output of the attention\n",
                "        output = ...\n",
                "\n",
                "        return output, attn_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_attn_weights = torch.Tensor([[0.1, 0.2, 0.1, 0.3, 0.0, 0.2, 0.0, 0.1]])\n",
                "\n",
                "attn_module = AdditiveAttention(q_dim=dim, k_dim=dim, attn_dim=dim//2)\n",
                "\n",
                "output, attn_weights = dummy_train(attn_module, q, k, v, target_attn_weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot attention weights of trained attention\n",
                "output, attention = attn_module(q,k,v)\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "shjNIItaVNNc"
            },
            "source": [
                "# Attention with matrices\n",
                "\n",
                "In the previous excercises we have used only one query. However, in reality, we deal with $m$ queries simultaneously. We can define a stack of queries as a matrix $Q \\in \u211d^{m\ud835\ude1fd_q}$.\n",
                "\n",
                "Similarly, we'll stack the keys and values in their own matrices $K \\in \u211d^{n\ud835\ude1fd_k}$ $V \\in \u211d^{n\ud835\ude1fd_v}$\n",
                "\n",
                "Then we can compute the attention weights for all the queries in parallel:\n",
                "\n",
                "$B = \\frac{1}{\\sqrt{d}}QK^T$\n",
                "\n",
                "$A = \\text{softmax}(B)$, all the weights in each row sum 1\n",
                "\n",
                "$O = AV \\in \u211d^{m\ud835\ude1fd_v}$\n",
                "\n",
                "Just like before, the output is still the weighted sum of the values scaled by the attention. \n",
                "This is computed by each query, so for each of the $m$ input queries we have an output value, a row in $O$.\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/attention_matrices.png?raw=true\" class=\"center\" title=\"Attention with matrices\" width=\"500\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vuFWtumJC1j9"
            },
            "source": [
                "Let's now define a query matrix Q, with three queries, one per row."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dim = 128\n",
                "q = torch.randn(3, dim)\n",
                "k = torch.randn(8, dim)\n",
                "v = k"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_tvik6xCDDO8"
            },
            "source": [
                "Our dummy task now has to predict one `target` vector per each query:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_attn_weights = torch.Tensor([[0.1, 0.2, 0.1, 0.3, 0.0, 0.2, 0.0, 0.1],\n",
                "                                    [0.4, 0.1, 0.1, 0.2, 0.0, 0.2, 0.0, 0.0],\n",
                "                                    [0.0, 0.6, 0.0, 0.1, 0.0, 0.2, 0.0, 0.1]])\n",
                "\n",
                "attn_module = AdditiveAttention(q_dim=dim, k_dim=dim, attn_dim=dim//2)\n",
                "\n",
                "output, attn_weights = dummy_train(attn_module, q, k, v, target_attn_weights)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot attention weights of trained attention\n",
                "output, attention = attn_module(q,k,v)\n",
                "\n",
                "plot_attention(\n",
                "    attention,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XKD6UER-tNJV"
            },
            "source": [
                "# Exercise 3: Neural Machine Translation with RNNs and attention"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qfnBy3LPto2S"
            },
            "source": [
                "#\u00a0Sequence-to-sequence Models\n",
                "\n",
                "To better motivate the use of attention, we are going to introduce a new family of models, the sequence-to-sequence, or se2seq, models. As the name suggests, seq2seq models map sequences to sequences, but both of arbitrary length.\n",
                "\n",
                "This systems usually use an encoder-decoder architecture, composed of:\n",
                "\n",
                "*   Encoder: processes the input sequence and compresses information into a context vector of fixed length. This embedding is a summary of the whole source sentence.\n",
                "*   Decoder: it takes the context vector and produces the transformed output. \n",
                "\n",
                "A classic example of seq2seq models are Machine Translation (MT) models. In this task, we deal with an input sequence corresponding to a sentence in the *source* language, and we aim to produce a translation in a *target* language:\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/enc_dec_gif_mt.gif?raw=true\" class=\"center\" title=\"NMT decoding gift\" width=\"500\"/>\n",
                "</p><br>\n",
                "\n",
                "Note that we generate the target sentence *autoregressively*, i.e. at each decoding step we produce a single word.\n",
                "\n",
                "In order to correctly translate, at each decoding step humans usually pay attention to different parts of the source sentence, leaving the rest aside. This is the intuition behind the attention mechanism.\n",
                "\n",
                "### Bottleneck Problem\n",
                "The seq2seq architecture can be implemented with RNNs. However there is a main issue with this models: the decoder only uses the **last** state of the encoder as a context vector of the source sentence.\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/bottleneck_problem.png?raw=true\" class=\"center\" title=\"Bottleneck problem\" width=\"500\"/>\n",
                "</p><br>\n",
                "\n",
                "Why is this a problem? Since the last state of the encoder is of fixed size, it is not able to store the information of long sentences, and it often forgets the first parts once it completes processing the whole input.\n",
                "\n",
                "Rather than building a single context vector out of the encoder's last hidden state, the attention mechanism creates shortcuts between each hidden state in the decoder and the entire source input. And this shortcuts are customizable for each output element."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "pwdPLWokuT9I"
            },
            "source": [
                "In this lab, we are going to build a seq2seq model for translating text from English to Catalan, and will eventually visualize the attention patterns discovered by the model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "oGsTFfL7yvUb"
            },
            "source": [
                "## Download dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!wget http://www.manythings.org/anki/cat-eng.zip\n",
                "!unzip cat-eng.zip && rm cat-eng.zip\n",
                "!mkdir data\n",
                "!mv \"cat.txt\" \"data/eng-cat.txt\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "SOS_token = 0\n",
                "EOS_token = 1\n",
                "\n",
                "class Lang:\n",
                "    def __init__(self, name):\n",
                "        self.name = name\n",
                "        self.word2index = {}\n",
                "        self.word2count = {}\n",
                "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
                "        self.n_words = 2  # Count SOS and EOS\n",
                "\n",
                "    def addSentence(self, sentence):\n",
                "        for word in sentence.split(' '):\n",
                "            self.addWord(word)\n",
                "\n",
                "    def addWord(self, word):\n",
                "        if word not in self.word2index:\n",
                "            self.word2index[word] = self.n_words\n",
                "            self.word2count[word] = 1\n",
                "            self.index2word[self.n_words] = word\n",
                "            self.n_words += 1\n",
                "        else:\n",
                "            self.word2count[word] += 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iBxJSbT9y9jo"
            },
            "source": [
                "## Data pre-processing\n",
                "\n",
                "First we need to pre-process the raw text data. We need to make sure to lowercase all of it, remove all non-letter characters, and limit the maximum lenght of the sentences to MAX_LENGTH. Eventually, we create ```pairs```, a list where each component has a sentence and its translation, both pre-processed.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Turn a Unicode string to plain ASCII, thanks to\n",
                "# http://stackoverflow.com/a/518232/2809427\n",
                "def unicodeToAscii(s):\n",
                "    return ''.join(\n",
                "        c for c in unicodedata.normalize('NFD', s)\n",
                "        if unicodedata.category(c) != 'Mn'\n",
                "    )\n",
                "\n",
                "# Lowercase, trim, and remove non-letter characters\n",
                "def normalizeString(s):\n",
                "    s = unicodeToAscii(s.lower().strip())\n",
                "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
                "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
                "    return s\n",
                "    \n",
                "def readLangs(lang1, lang2, reverse=False):\n",
                "    print(\"Reading lines...\")\n",
                "\n",
                "    # Read the file and split into lines\n",
                "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
                "        read().strip().split('\\n')\n",
                "\n",
                "    # Remove extra info\n",
                "    new_lines = []\n",
                "    for line in lines:\n",
                "        new_lines.append(line[0:(line.find('CC-BY')-1)])\n",
                "\n",
                "\n",
                "    # Split every line into pairs and normalize\n",
                "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in new_lines]\n",
                "\n",
                "    # Reverse pairs, make Lang instances\n",
                "    if reverse:\n",
                "        pairs = [list(reversed(p)) for p in pairs]\n",
                "        input_lang = Lang(lang2)\n",
                "        output_lang = Lang(lang1)\n",
                "    else:\n",
                "        input_lang = Lang(lang1)\n",
                "        output_lang = Lang(lang2)\n",
                "\n",
                "    return input_lang, output_lang, pairs\n",
                "\n",
                "MAX_LENGTH = 20\n",
                "\n",
                "def filterPair(p):\n",
                "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
                "        len(p[1].split(' ')) < MAX_LENGTH \n",
                "\n",
                "def filterPairs(pairs):\n",
                "    return [pair for pair in pairs if filterPair(pair)]\n",
                "\n",
                "def prepareData(lang1, lang2, reverse=False):\n",
                "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
                "    print(\"Read %s sentence pairs\" % len(pairs))\n",
                "    pairs = filterPairs(pairs)\n",
                "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
                "    print(\"Counting words...\")\n",
                "    for pair in pairs:\n",
                "        input_lang.addSentence(pair[0])\n",
                "        output_lang.addSentence(pair[1])\n",
                "    print(\"Counted words:\")\n",
                "    print(input_lang.name, input_lang.n_words)\n",
                "    print(output_lang.name, output_lang.n_words)\n",
                "    return input_lang, output_lang, pairs\n",
                "\n",
                "\n",
                "input_lang, output_lang, pairs = prepareData('eng', 'cat', False)\n",
                "print(random.choice(pairs))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "V29yklurzHIo"
            },
            "source": [
                "We have a total of 1306 pairs of sentences containing 1436 english words and 1773 catalan words. We use a very small dataset to reduce training time, but a larger dataset would be necessary to learn something meaningful."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VZedNSCNzQ2g"
            },
            "source": [
                "## Building the seq2seq model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "vxLIx4S0mUA_"
            },
            "source": [
                "We proceed to build our seq2seq model with attention. First we define our encoder RNN:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderRNN(nn.Module):\n",
                "    def __init__(self, input_size, hidden_size):\n",
                "        super(EncoderRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "        # TODO: Define the Embedding matrix (use nn.Embedding)\n",
                "        self.embedding = ...\n",
                "\n",
                "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
                "\n",
                "    def forward(self, input, hidden):\n",
                "        embedded = self.embedding(input)\n",
                "        output = embedded\n",
                "        output, hidden = self.gru(output, hidden)\n",
                "        return output, hidden\n",
                "\n",
                "    def initHidden(self):\n",
                "        return torch.zeros(1, 1, self.hidden_size, device=device)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uxm-SsMxZfCz"
            },
            "source": [
                "Now we define our decoder, with an attention mechanism (```self.attn```) built inside. Try to map the code with the following diagram:\n",
                "\n",
                "<p align=\"center\"><br>\n",
                "<img src=\"https://github.com/telecombcn-dl/labs-all/raw/main/labs/attention/images/attention_tensor_dance_last.jpeg?raw=true\" class=\"center\" title=\"Decoding with attention\" width=\"800\"/>\n",
                "</p><br>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AttnDecoderRNN(nn.Module):\n",
                "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
                "        super(AttnDecoderRNN, self).__init__()\n",
                "        self.hidden_size = hidden_size\n",
                "        self.output_size = output_size\n",
                "        self.dropout_p = dropout_p\n",
                "        self.max_length = max_length\n",
                "\n",
                "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
                "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, batch_first=True)\n",
                "        self.dropout = nn.Dropout(self.dropout_p)\n",
                "        # Here we define the attention we use\n",
                "        self.attn = AdditiveAttention(self.hidden_size, self.hidden_size, self.hidden_size // 2)\n",
                "        #self.attn = MultiplicativeAttention(self.hidden_size, self.hidden_size)\n",
                "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
                "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
                "\n",
                "    def forward(self, input, hidden, encoder_outputs):\n",
                "        embedded = self.embedding(input)\n",
                "        embedded = self.dropout(embedded)\n",
                "\n",
                "        x, hidden = self.gru(embedded, hidden)\n",
                "        context, attn_weights = self.attn(query=x, key=encoder_outputs, value=encoder_outputs)\n",
                "\n",
                "        # TODO: compute concatenation of the decoder hidden states and the context vectors\n",
                "        x_w_context = ...\n",
                "\n",
                "        x_w_context = self.attn_combine(x_w_context)\n",
                "        output = F.log_softmax(self.out(x_w_context), dim=-1)\n",
                "        return output, hidden, attn_weights\n",
                "\n",
                "    def initHidden(self):\n",
                "        return torch.zeros(1, 1, self.hidden_size, device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def indexesFromSentence(lang, sentence):\n",
                "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
                "\n",
                "\n",
                "def tensorFromSentence(lang, sentence):\n",
                "    indexes = indexesFromSentence(lang, sentence)\n",
                "    indexes.append(EOS_token)\n",
                "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
                "\n",
                "\n",
                "def tensorsFromPair(pair):\n",
                "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
                "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
                "    return (input_tensor, target_tensor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
                "    encoder_hidden = encoder.initHidden()\n",
                "\n",
                "    encoder_optimizer.zero_grad()\n",
                "    decoder_optimizer.zero_grad()\n",
                "\n",
                "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
                "\n",
                "    # First hidden state used by the decoder is the last encoder hidden state\n",
                "    decoder_hidden = encoder_hidden\n",
                "\n",
                "    # We feed the decoder with the whole target sentence (teacher forcing),\n",
                "    # first we append SOS_token\n",
                "    decoder_input = torch.cat([\n",
                "        torch.tensor([[SOS_token]], device=device),\n",
                "        target_tensor[:, :-1]\n",
                "    ], dim=1)\n",
                "\n",
                "    \n",
                "    decoder_output, decoder_hidden, decoder_attention = decoder(\n",
                "                decoder_input, decoder_hidden, encoder_outputs)\n",
                "            \n",
                "    loss = criterion(\n",
                "        decoder_output.view(-1, decoder_output.size(-1)),\n",
                "        target_tensor.view(-1)\n",
                "    )\n",
                "    loss.backward()\n",
                "\n",
                "    encoder_optimizer.step()\n",
                "    decoder_optimizer.step()\n",
                "\n",
                "    return loss.item() / target_tensor.numel()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "def asMinutes(s):\n",
                "    m = math.floor(s / 60)\n",
                "    s -= m * 60\n",
                "    return '%dm %ds' % (m, s)\n",
                "\n",
                "\n",
                "def timeSince(since, percent):\n",
                "    now = time.time()\n",
                "    s = now - since\n",
                "    es = s / (percent)\n",
                "    rs = es - s\n",
                "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
                "\n",
                "def showPlot(points):\n",
                "    plt.figure()\n",
                "    fig, ax = plt.subplots()\n",
                "    # this locator puts ticks at regular intervals\n",
                "    loc = ticker.MultipleLocator(base=0.2)\n",
                "    ax.yaxis.set_major_locator(loc)\n",
                "    plt.plot(points)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
                "    start = time.time()\n",
                "    plot_losses = []\n",
                "    print_loss_total = 0  # Reset every print_every\n",
                "    plot_loss_total = 0  # Reset every plot_every\n",
                "\n",
                "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
                "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
                "\n",
                "    # Random sample of n_iters pairs\n",
                "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
                "                      for i in range(n_iters)]\n",
                "    criterion = nn.NLLLoss()\n",
                "\n",
                "    for iter in range(1, n_iters + 1):\n",
                "        training_pair = training_pairs[iter - 1]\n",
                "        input_tensor = training_pair[0]\n",
                "        target_tensor = training_pair[1]\n",
                "\n",
                "        loss = train(input_tensor, target_tensor, encoder,\n",
                "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
                "        print_loss_total += loss\n",
                "        plot_loss_total += loss\n",
                "\n",
                "        if iter % print_every == 0:\n",
                "            print_loss_avg = print_loss_total / print_every\n",
                "            print_loss_total = 0\n",
                "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
                "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
                "\n",
                "        if iter % plot_every == 0:\n",
                "            plot_loss_avg = plot_loss_total / plot_every\n",
                "            plot_losses.append(plot_loss_avg)\n",
                "            plot_loss_total = 0\n",
                "\n",
                "    showPlot(plot_losses)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hidden_size = 256\n",
                "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
                "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
                "\n",
                "trainIters(encoder1, attn_decoder1, 50000, print_every=3000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
                "    with torch.no_grad():\n",
                "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
                "        encoder_hidden = encoder.initHidden()\n",
                "\n",
                "        encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
                "\n",
                "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
                "\n",
                "        decoder_hidden = encoder_hidden\n",
                "\n",
                "        decoded_words = []\n",
                "        decoder_attentions = []\n",
                "\n",
                "        for di in range(max_length):\n",
                "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
                "                decoder_input, decoder_hidden, encoder_outputs)\n",
                "            decoder_attentions.append(decoder_attention.squeeze(0).data)\n",
                "            topv, topi = decoder_output.data.topk(1)\n",
                "            if topi.squeeze().item() == EOS_token:\n",
                "                decoded_words.append('<EOS>')\n",
                "                break\n",
                "            else:\n",
                "                decoded_words.append(output_lang.index2word[topi.squeeze().item()])\n",
                "            decoder_input = topi.detach().squeeze(1)\n",
                "\n",
                "        return decoded_words, torch.cat(decoder_attentions)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluateRandomly(encoder, decoder, n=10):\n",
                "    for i in range(n):\n",
                "        pair = random.choice(pairs)\n",
                "        print('Source:', pair[0])\n",
                "        print('Reference:', pair[1])\n",
                "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
                "        output_sentence = ' '.join(output_words[:-1])\n",
                "        print('Model:', output_sentence)\n",
                "        print('')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "evaluateRandomly(encoder1, attn_decoder1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def showAttention(input_sentence, output_words, attentions):\n",
                "    import pandas as pd\n",
                "    # Set up figure with colorbar\n",
                "    fig = plt.figure()\n",
                "    ax = fig.add_subplot(111)\n",
                "    #cax = ax.matshow(attentions.numpy(), cmap='Blues')\n",
                "    #fig.colorbar(cax)\n",
                "    sns.set(rc={'figure.figsize':(12, 8)})\n",
                "    input_sentence_list = input_sentence.split(' ') + ['<EOS>']\n",
                "\n",
                "    df = pd.DataFrame(attentions, columns = input_sentence_list, index = output_words)\n",
                "    ax = sns.heatmap(\n",
                "        df.astype(float),\n",
                "        linewidth=0.5,\n",
                "        cmap=\"Blues\",\n",
                "        square=True)\n",
                "\n",
                "    \n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def evaluateAndShowAttention(input_sentence):\n",
                "    output_words, attentions = evaluate(\n",
                "        encoder1, attn_decoder1, input_sentence)\n",
                "    print('input =', input_sentence)\n",
                "    print('output =', ' '.join(output_words))\n",
                "    showAttention(input_sentence, output_words, attentions.detach().cpu())\n",
                "\n",
                "\n",
                "evaluateAndShowAttention(\"your son is a genius .\")\n",
                "\n",
                "evaluateAndShowAttention(\"i can t remember which is my racket .\")\n",
                "\n",
                "evaluateAndShowAttention(\"please circle the right answer .\")\n",
                "\n",
                "evaluateAndShowAttention(\"i d like to reserve a table for two .\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "gpuClass": "standard",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
