{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zeQHvio65L7S"
            },
            "source": [
                "# Diffusion Models (DDPM)\n",
                "Notebook created by [Pol Caselles Rico](https://www.linkedin.com/in/pcaselles/) for the Postgraduate course in artificial intelligence with deep learning in UPC School (2023). Minor contributions by [Laia Tarr\u00e9s](https://www.linkedin.com/in/laia-tarres-9a5369138/) during 2023.\n",
                "\n",
                "In this notebook you will learn about Diffusion Models by implementing a DDPM to generate images from noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "import torchvision\n",
                "import torchvision.transforms as transforms\n",
                "from torchvision.utils import make_grid\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "import math\n",
                "import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import torch.nn.functional as F\n",
                "from torch import nn\n",
                "from typing import Dict, List, Tuple\n",
                "import tqdm\n",
                "from torchvision.datasets import video_utils"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 22\n",
                "torch.manual_seed(seed)\n",
                "torch.cuda.manual_seed_all(seed)\n",
                "torch.backends.cudnn.deterministic = True\n",
                "torch.backends.cudnn.benchmark = False\n",
                "np.random.seed(seed)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "rZJh7OW5Y68W"
            },
            "source": [
                "# Understanding Diffusion Models\n",
                "\n",
                "A diffusion model transforms noise sampled from a known distribution into a data sample. The network is trained to gradually remove noise, starting from an initial state of pure noise.\n",
                "\n",
                "<p align=\"center\">\n",
                "    <img src=\"https://drive.google.com/uc?id=12tpb3-3KwP4IVaRh7iQTOjPDc8AeJl3x\" width=\"300\" />\n",
                "</p>\n",
                "\n",
                "Denoising diffusion models involve two essential processes:\n",
                "\n",
                "* **Forward Diffusion Process:** This process incrementally introduces noise to the input data.\n",
                "* **Reverse Denoising Process:** The model learns to generate data by effectively denoising the input.\n",
                "\n",
                "For the sake of simplicity, we often choose to model the noise as isotropic Gaussians. It's important to recall that the sum of Gaussians remains a Gaussian.\n",
                "\n",
                "\n",
                "<p align=\"center\">\n",
                "    <img src=\"https://drive.google.com/uc?id=1t5dUyJwgy2ZpDAqHXw7GhUAp2FE5BWHA\" width=\"600\" />\n",
                "</p>\n",
                "\n",
                "\n",
                "Transitioning from \\\\(\\mathbf{x}_{t-1}\\\\) to \\\\(\\mathbf{x}_{t}\\\\) is straightforward, since it involves the addition of Gaussian noise computed in closed form. However, the challenge is in the reverse direction: learning to distinguish between noise and the original structure requires an understanding of the underlying data distribution.\n",
                "\n",
                "To illustrate this point, consider the example of modeling faces. When presented with a noised image, identifying the modified pixels necessitates knowledge of facial features.\n",
                "\n",
                "Given the inherent difficulty, we use the capabilities of deep learning models to discern this underlying structure. The core concept involves taking a dataset of ground truth (GT) images, applying the forward diffusion process (adding noise), and self-supervising our model to effectively reverse this process. For clarity, we specify the number of steps (or states) \\\\(T\\\\) in our diffusion process. The workflow involves:\n",
                "\n",
                "\n",
                "1.   sampling a noised image,\n",
                "2.   forwarding it to our denoiser model,\n",
                "3.   removing the noise,\n",
                "3.   and repeating steps (2) and (3) until arriving at the final image.\n",
                "\n",
                "\n",
                "\n",
                "Conceptually, we model this intricate process as a Markov process, emphasizing its dependency solely on the current state. This implies that predicting the applied noise relies only on the information within the noised image.\n",
                "\n",
                "Given the data-intensive nature of diffusion models, we opt to train it on the MNIST dataset for simplicity:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download and dataset preparation\n",
                "transforms = transforms.Compose([\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.5],[0.5])\n",
                "    ])\n",
                "\n",
                "train_data: Dataset = torchvision.datasets.MNIST(\n",
                "    root='./content/data/',\n",
                "    train= True,\n",
                "    transform=transforms,\n",
                "    download= True\n",
                "  )\n",
                "\n",
                "test_data: Dataset = torchvision.datasets.MNIST(\n",
                "    root='./content/data/',\n",
                "    train= False,\n",
                "    transform=None,\n",
                "    download= True,\n",
                "\n",
                "  )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MGsZ1nZWaC9k"
            },
            "source": [
                "# Forward Diffusion and Variance Schedules\n",
                "\n",
                "The forward diffusion process introduces noise to an image sampled from the real distribution over a defined number of time steps $T$. This noise addition is orchestrated through a variance schedule. Thus, starting from \\\\(\\mathbf{x}_0\\\\), the progression unfolds as\n",
                "\\\\(\\mathbf{x}_1,  ..., \\mathbf{x}_t, ..., \\mathbf{x}_T\\\\)\n",
                " , culminating with \\\\(\\mathbf{x}_T\\\\) representing pure Gaussian noise when the schedule is appropriately configured.\n",
                "\n",
                "To streamline computation and prevent the need for continuous reparameterization of betas during processing, we compute them once. These computed values are referred to as \"ddpm_schedules.\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ddpm_schedules(betas: torch.tensor) -> Dict[str,torch.tensor]:\n",
                "    \"\"\"\n",
                "    Returns pre-computed schedules for DDPM sampling, training process.\n",
                "    \"\"\"\n",
                "    alphas = 1. - betas\n",
                "    alphas_cumprod = torch.cumprod(alphas, dim=-1)\n",
                "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
                "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
                "\n",
                "    return {\n",
                "        \"alphas\": alphas,\n",
                "        \"betas\": betas,\n",
                "        \"alphas_cumprod\": alphas_cumprod,\n",
                "        \"sqrt_alphas_cumprod\": sqrt_alphas_cumprod,\n",
                "        \"sqrt_one_minus_alphas_cumprod\": sqrt_one_minus_alphas_cumprod,\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "SMmdv1GIsj7P"
            },
            "source": [
                "The beta schedulers are responsible for defining the amout of noise applied at each timestep \\\\(t\\\\) during the forward process.\n",
                "\n",
                "The quantity of noise introduced at each step influences the generation process. The original authors of [DDPM](https://proceedings.neurips.cc/paper/2020/file/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf) utilized a linear schedule for this purpose. However, in this lab, we will also implement the cosine beta scheduler to explore and compare the differences between the two approaches.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def linear_beta_schedule(\n",
                "      T: int = 500,\n",
                "      beta1: float = 1e-4,\n",
                "      beta2: float = 0.02\n",
                "      ) -> torch.tensor:\n",
                "    \"\"\"\n",
                "    linear schedule, proposed in original ddpm paper\n",
                "    \"\"\"\n",
                "    timesteps = T\n",
                "    scale = 1000 / timesteps\n",
                "    beta_start = scale * 0.0001\n",
                "    beta_end = scale * 0.02\n",
                "    betas = torch.linspace(\n",
                "        beta_start,\n",
                "        beta_end,\n",
                "        timesteps,\n",
                "        dtype=torch.float32)\n",
                "    return betas\n",
                "\n",
                "\n",
                "def cosine_beta_schedule(\n",
                "      T: int,\n",
                "      s: float = 0.008\n",
                "      ) -> torch.tensor:\n",
                "    \"\"\"\n",
                "    cosine schedule\n",
                "    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n",
                "    \"\"\"\n",
                "    steps=torch.linspace(0, T, steps=T+1, dtype=torch.float32)\n",
                "    f_t=torch.cos(((steps/T+s)/(1.0+s))*math.pi*0.5)**2\n",
                "    betas=torch.clip(1.0-f_t[1:]/f_t[:T],0.0,0.999)\n",
                "    return betas"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "a44FqSzXt1i_"
            },
            "source": [
                "## Forward Diffusion Process\n",
                "\n",
                "To maintain simplicity, we'll skip the majority of the mathematical formulation.\n",
                "\n",
                "The `ForwardDiffusionProcess` class is responsible for generating noisy samples based on the true signal \\\\(\\mathbf{x}_0\\\\). This process is deterministic, meaning there are no parameters being trained at this stage. However, we will use this class to train our denoiser model, our network. Typically, for a given true sample \\\\(\\mathbf{x}_0\\\\), we uniformly sample a timestep \\\\(t\\\\) and apply Gaussian noise.\n",
                "\n",
                "Note:\n",
                "Remember that a normal distribution (also known as a Gaussian distribution) is defined by two parameters: a mean \\\\(\\mu\\\\) and a variance \\\\(\\sigma^2 \\geq 0\\\\). Essentially, each new (slightly noisier) image at timestep \\\\(t\\\\) is drawn from a conditional Gaussian distribution with \\\\(\\mathbf{\\mu}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1}\\\\) and \\\\(\\sigma^2_t = \\beta_t\\\\). This is achieved by sampling \\\\(\\mathbf{\\epsilon} \\sim \\mathbf{N}(\\mathbf{0}, \\mathbf{I})\\\\) and then setting \\\\(\\mathbf{x}_t = \\sqrt{1 - \\beta_t} \\mathbf{x}_{t-1} +  \\sqrt{\\beta_t} \\mathbf{\\epsilon}\\\\).\n",
                "\n",
                "\n",
                "**Exercise 1:**\n",
                "Your task is to complete the forward method to sample timesteps between 0 and \\\\(T\\\\) from a Uniform distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ForwardDiffusionProcess(nn.Module):\n",
                "    def __init__(\n",
                "          self,\n",
                "          ddpm_schedules: Dict[str,torch.tensor],\n",
                "          device: torch.device) -> None:\n",
                "\n",
                "        super().__init__()\n",
                "\n",
                "        # Register buffers with ddpm schedules\n",
                "        for k, v in ddpm_schedules.items():\n",
                "            self.register_buffer(k, v)\n",
                "\n",
                "        self.n_T = self.alphas.shape[0]\n",
                "        self.device = device\n",
                "\n",
                "    def apply_noise(self, x, noise, ts):\n",
                "        # This is the x_t, which is sqrt(alphabar) x_0 + sqrt(1-alphabar) * eps\n",
                "\n",
                "        a = self.sqrt_alphas_cumprod.gather(-1, ts).reshape(x.shape[0], 1, 1, 1)\n",
                "        b = self.sqrt_one_minus_alphas_cumprod.gather(-1, ts).reshape(x.shape[0], 1, 1, 1)\n",
                "        x_t = a * x + b * noise\n",
                "\n",
                "        return x_t\n",
                "\n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        this method is used in training, so samples t and noise randomly\n",
                "        \"\"\"\n",
                "\n",
                "        # t ~ Uniform(0, n_T)\n",
                "        # TODO: sample uniformly a timestep\n",
                "        _ts = torch.randint(...,...,...).to(self.device)\n",
                "\n",
                "        # eps ~ N(0, 1)\n",
                "        # TODO: get the random noise\n",
                "        noise = torch...\n",
                "\n",
                "        x_t = self.apply_noise(x, noise, _ts)\n",
                "\n",
                "        return x_t, noise, _ts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AUjMTv648BRA"
            },
            "source": [
                "To start with, we will establish the linear and cosine schedules for \\\\(T=100\\\\) time steps. Additionally, we will define several essential variables derived from \\\\(\\beta_t\\\\), including the cumulative product of variances \\\\(\\bar{\\alpha}_t\\\\). Each of the following variables is represented as a 1-dimensional tensor, storing values from \\\\(t\\\\) to \\\\(T\\\\)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define number of steps\n",
                "n_T=100\n",
                "\n",
                "# Define beta schedulers\n",
                "betas_linear = linear_beta_schedule(T=n_T)\n",
                "betas_cosine = cosine_beta_schedule(T=n_T)\n",
                "\n",
                "\n",
                "# Generate ddpm schedulers based on the given betas\n",
                "ddpm_linear = ddpm_schedules(betas_linear)\n",
                "ddpm_cosine = ddpm_schedules(betas_cosine)\n",
                "\n",
                "\n",
                "# Create the ForwardDiffusionProcess objects\n",
                "forward_diffusion_process_linear = ForwardDiffusionProcess(\n",
                "    ddpm_schedules=ddpm_linear,\n",
                "    device=device\n",
                ").to(device) # Linear\n",
                "\n",
                "forward_diffusion_process_cosine = ForwardDiffusionProcess(\n",
                "    ddpm_schedules=ddpm_cosine,\n",
                "    device=device\n",
                ").to(device) # Cosine"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jl5eCPJEyO9J"
            },
            "source": [
                "The beta parameter plays a crucial role in regulating the extent of noise introduced at each timestep relative to the original sample in diffusion models. In the subsequent plot, we illustrate the noise levels applied at each timestep for different schedulers.\n",
                "\n",
                "The Y-axis denotes the ratio of the original image \\\\(\\mathbf{x}_0\\\\), while the X-axis represents the step \\\\(t\\\\) in the diffusion process. Notably, the linear scheduler exhibits a more pronounced drop at the onset, whereas the cosine scheduler preserves a higher fidelity to the true signal across all timesteps.\n",
                "\n",
                "The precise control of noise levels during training directly influences the learning trajectory of the model. When a substantial number of steps deviate significantly from the target distribution, the network tends to counteract Gaussian noise, potentially leading to inaccuracies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize = (8, 4))\n",
                "plt.plot(forward_diffusion_process_linear.alphas_cumprod.cpu().numpy(), label='linear')\n",
                "plt.plot(forward_diffusion_process_cosine.alphas_cumprod.cpu().numpy(), label='cosine')\n",
                "plt.legend(loc=\"upper right\")\n",
                "\n",
                "plt.title(\"Linear combination between a sample and noise\")\n",
                "plt.xlabel(\"Timesteps\")\n",
                "plt.ylabel(\"Amount of original image\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Kmtic-P3-vGA"
            },
            "source": [
                "To visually ilustrate the differences, we will now pick a sample from MNIST dataset, and we will apply both of our defined schedulers at different time steps:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pick a MNIST sample\n",
                "image = train_data[0][0].clone().unsqueeze(0).to(device) # Shape: [1, 1, 28, 28]\n",
                "plt.axis('off')\n",
                "plt.imshow(image.permute(0,2,3,1)[0].cpu().numpy(), cmap='Greys')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Return Forward Diffusion Process\n",
                "def get_image_from_linear(noise: torch.tensor, image: torch.tensor) -> torch.tensor:\n",
                "    image_linear = forward_diffusion_process_linear.apply_noise(\n",
                "      image,\n",
                "      noise,\n",
                "      torch.tensor(t).to(device)\n",
                "    )\n",
                "    return image_linear\n",
                "\n",
                "\n",
                "def get_image_from_cosine(noise: torch.tensor, image: torch.tensor) -> torch.tensor:\n",
                "    image_cosine = forward_diffusion_process_cosine.apply_noise(\n",
                "        image,\n",
                "        noise,\n",
                "        torch.tensor(t).to(device)\n",
                "    )\n",
                "    return image_cosine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# noise the image over timesteps using the two schedulers\n",
                "image_linear = image.clone()\n",
                "image_cosine = image.clone()\n",
                "\n",
                "images_linear = []\n",
                "images_cosine = []\n",
                "\n",
                "for t in tqdm.tqdm(range(n_T)):\n",
                "    noise = torch.randn_like(image_linear)\n",
                "\n",
                "    # Add noise from linear scheduler\n",
                "    image_linear = get_image_from_linear(noise, image_linear)\n",
                "    images_linear.append(image_linear.detach().cpu())\n",
                "\n",
                "    # Add noise from cosine scheduler\n",
                "    image_cosine = get_image_from_cosine(noise, image_cosine)\n",
                "    images_cosine.append(image_cosine.detach().cpu())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ygCZ6zADBXIu"
            },
            "source": [
                "Here, we present the evolution of samples over various timesteps. The top row corresponds to the linear scheduler, while the bottom row corresponds to the cosine scheduler:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize = (16, 8))\n",
                "plt.axis('off')\n",
                "indices = [0, 5,10,15,20,25, 30, 35, 40, 45, 50]\n",
                "img_row_1 = torch.cat([images_linear[i] for i in indices], dim=3).permute(0,2,3,1)[0] # Linear beta scheduler\n",
                "img_row_2 = torch.cat([images_cosine[i] for i in indices], dim=3).permute(0,2,3,1)[0] # Cosine beta scheduler\n",
                "img_row_1_2 = torch.cat([img_row_1, img_row_2])\n",
                "np.clip(img_row_1_2, 0, 1)\n",
                "plt.imshow(img_row_1_2, interpolation='nearest', cmap='Greys')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Xr17SXuUHLY6"
            },
            "source": [
                "Notably, the cosine scheduler exhibits the ability to retain recognition of the original image across more distant timesteps compared to the linear scheduler.\n",
                "\n",
                "The intuition is the following: When we train, if we have a lot of timesteps where there is almost no information of the original image, the network learning a mapping between a random noise and another random noise is irrelevant. so we are wasting valuable GPU energy. So it is preferred to use the cosine schedule."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "t8h7w7vf0lmU"
            },
            "source": [
                "# Defining the Reverse Denoising Process\n",
                "\n",
                "While the comprehensive understanding of the mathematics behind diffusion models isn't the primary focus of this lab, we do need to define certain methods for training and sampling our model. Here, we introduce two essential methods:\n",
                "\n",
                "- `ReverseDiffusionProcess()._reverse_diffusion`: This method manages the reverse process for a **single step**.\n",
                "- `ReverseDiffusionProcess().sample`: This method oversees the **complete reverse process**, guiding the transformation from noise to the final sample."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReverseDiffusionProcess(nn.Module):\n",
                "    def __init__(self, nn_model, ddpm_schedules, device):\n",
                "        super().__init__()\n",
                "\n",
                "        for k, v in ddpm_schedules.items():\n",
                "            self.register_buffer(k, v)\n",
                "\n",
                "        self.nn_model = nn_model\n",
                "        self.n_T = self.alphas.shape[0]\n",
                "        self.device = device\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def _reverse_diffusion(self, x_t, t, noise, c=None):\n",
                "        '''\n",
                "        p(x_{0}|x_{t}),q(x_{t-1}|x_{0},x_{t})->mean,std\n",
                "\n",
                "        pred_noise -> pred_x_0 (clip to [-1.0,1.0]) -> pred_mean and pred_std\n",
                "        '''\n",
                "        pred=self.nn_model(x_t.float(),t.float(),c)\n",
                "        alpha_t=self.alphas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
                "        alpha_t_cumprod=self.alphas_cumprod.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
                "        beta_t=self.betas.gather(-1,t).reshape(x_t.shape[0],1,1,1)\n",
                "\n",
                "        x_0_pred=torch.sqrt(1. / alpha_t_cumprod)*x_t-torch.sqrt(1. / alpha_t_cumprod - 1.)*pred\n",
                "        x_0_pred.clamp_(-1., 1.)\n",
                "\n",
                "        if t.min()>0:\n",
                "            alpha_t_cumprod_prev=self.alphas_cumprod.gather(-1,t-1).reshape(x_t.shape[0],1,1,1)\n",
                "            mean= (beta_t * torch.sqrt(alpha_t_cumprod_prev) / (1. - alpha_t_cumprod))*x_0_pred +\\\n",
                "                 ((1. - alpha_t_cumprod_prev) * torch.sqrt(alpha_t) / (1. - alpha_t_cumprod))*x_t\n",
                "\n",
                "            std=torch.sqrt(beta_t*(1.-alpha_t_cumprod_prev)/(1.-alpha_t_cumprod))\n",
                "        else:\n",
                "            mean=(beta_t / (1. - alpha_t_cumprod))*x_0_pred #alpha_t_cumprod_prev=1 since 0!=1\n",
                "            std=0.0\n",
                "\n",
                "        return mean+std*noise\n",
                "\n",
                "    @torch.no_grad()\n",
                "    def sample(self, n_sample, size, device, c=None):\n",
                "\n",
                "        x_t = torch.randn(n_sample, *size).to(device)  # x_T ~ N(0, 1), sample initial noise\n",
                "        x_ts = [x_t.cpu()]\n",
                "        for i in tqdm.tqdm(range(self.n_T - 1, -1, -1), desc=\"Sampling\"):\n",
                "\n",
                "            noise = torch.randn_like(x_t).to(device)\n",
                "            t = torch.tensor([i for _ in range(n_sample)]).to(device)\n",
                "            x_t = self._reverse_diffusion(x_t, t, noise, c)\n",
                "            x_ts.append(x_t.cpu())\n",
                "\n",
                "        x_t = (x_t + 1.) / 2. #[-1,1] to [0,1]\n",
                "\n",
                "        return x_t, x_ts"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LHVUC6AxeJOw"
            },
            "source": [
                "##### **Excercise 2**:\n",
                "\n",
                "Your task is to complete the forward method of the DDPM class to obtain the ground truth (GT) sample at timestep t and the corresponding noise.\n",
                "\n",
                "The denoiser model should be conditioned on \\\\(\\mathbf{x}_t\\\\) and timestep \\\\(t\\\\)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DDPM(nn.Module):\n",
                "    def __init__(\n",
                "          self,\n",
                "          forward_diffusion_process: ForwardDiffusionProcess,\n",
                "          reverse_diffusion_process: ReverseDiffusionProcess,\n",
                "          nn_model\n",
                "          ) -> None:\n",
                "        super(DDPM, self).__init__()\n",
                "\n",
                "        self.forward_diffusion_process = forward_diffusion_process\n",
                "        self.reverse_diffusion_process = reverse_diffusion_process\n",
                "        self.nn_model = nn_model # denoiser model\n",
                "\n",
                "    def forward(self, x, c=None):\n",
                "        \"\"\"\n",
                "        this method is used in training, so samples t and noise randomly\n",
                "        \"\"\"\n",
                "        # TODO: Call the the forward_diffusion_proces, that given a sample, it returns a sampled timestep, the initial noise and the noisy sample\n",
                "        x_t, noise, _ts = ...\n",
                "        # TODO: Call the denoiser model, so given the noisy sample and the timestep, it returns the predicted initial noise\n",
                "        noise_pred = ...(..., ..., c=c)\n",
                "\n",
                "        return noise, noise_pred\n",
                "\n",
                "    def sample(self, n_sample, size, device, c=None):\n",
                "        \"\"\"\n",
                "        this method is used for inference, and performs the whole process from noise to the final sample, x_i\n",
                "        \"\"\"\n",
                "        x_i, x_i_store = self.reverse_diffusion_process.sample(\n",
                "            n_sample=n_sample,\n",
                "            size=size,\n",
                "            device=device,\n",
                "            c=c\n",
                "        )\n",
                "\n",
                "        return x_i, x_i_store"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fQze-BpQ3lxz"
            },
            "source": [
                "# The neural network (denoiser)\n",
                "\n",
                "For the neural network to effectively denoise an image at a specific time step, it must generate a prediction for the noise. It's important to note that the predicted noise is a tensor with the same size/resolution as the input image. Various approaches exist for predicting noise, and in this lab, we opt to predict the mean while maintaining a fixed variance.\n",
                "\n",
                "To achieve this, we require a model capable of performing image-to-image transformations. For this purpose, we will utilize an adapted version of the UNET architecture.\n",
                "\n",
                "<p align=\"center\">\n",
                "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"400\" />\n",
                "</p>\n",
                "\n",
                "The adapted UNET architecture consists of multiple stages, downsampling, upsampling, self-attention layers and residual connections.\n",
                "\n",
                "In order to predict the noise injected at each timestep, it is crucial to condition the model on the timestep \\\\(\\mathbf{N}_T\\\\). To achieve this, various methods could be employed. However, we have chosen the following approach: We predict two distinct latent representations for each timestep. These representations are then added to the latent features of the model at each of the upsampling stages.\n",
                "This approach is similar to what is done in the Transformer architecture.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SelfAttention(nn.Module):\n",
                "    '''\n",
                "    Similar to the Transformer Architecture, this network has self-attention blocks.\n",
                "    '''\n",
                "    def __init__(self, n_channels):\n",
                "      super().__init__()\n",
                "      n_channels_out = n_channels//4\n",
                "      self.query = nn.Linear(n_channels, n_channels_out, bias=False)\n",
                "      self.key = nn.Linear(n_channels, n_channels_out, bias=False)\n",
                "      self.value = nn.Linear(n_channels, n_channels, bias=False)\n",
                "      self.gamma = nn.Parameter(torch.tensor([0.0]))\n",
                "\n",
                "    def forward(self, x):\n",
                "      B, C, H, W = x.shape\n",
                "\n",
                "      x = x.permute(0, 2, 3, 1).view(B, H * W, C) # Shape: [B, H*W, C]\n",
                "\n",
                "      q = self.query(x) # [B, H*W, C]\n",
                "      k = self.key(x) # [B, H*W, C]\n",
                "      v = self.value(x) # [B, H*W, C]\n",
                "\n",
                "      attn = F.softmax(torch.bmm(q, k.transpose(1,2)), dim=1) # Shape: [B, H*W, H*W]\n",
                "      out = self.gamma * torch.bmm(attn, v) + x # Shape: [B, H*W, C]\n",
                "\n",
                "      out = out.permute(0, 2, 1).view(B, C, H, W).contiguous()\n",
                "\n",
                "      return out\n",
                "\n",
                "\n",
                "class ResidualConvBlock(nn.Module):\n",
                "    '''\n",
                "    The following are resnet block, which consist of convolutional layers, followed by batch normalization and residual connections.\n",
                "    '''\n",
                "    def __init__(\n",
                "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
                "    ) -> None:\n",
                "        super().__init__()\n",
                "        '''\n",
                "        standard ResNet style convolutional block\n",
                "        '''\n",
                "        self.same_channels = in_channels==out_channels\n",
                "        self.is_res = is_res\n",
                "        self.conv1 = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.GELU(),\n",
                "        )\n",
                "        self.conv2 = nn.Sequential(\n",
                "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.GELU(),\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        if self.is_res:\n",
                "            x1 = self.conv1(x)\n",
                "            x2 = self.conv2(x1)\n",
                "            # this adds on correct residual in case channels have increased\n",
                "            if self.same_channels:\n",
                "                out = x + x2\n",
                "            else:\n",
                "                out = x1 + x2\n",
                "            return out / 1.414\n",
                "        else:\n",
                "            x1 = self.conv1(x)\n",
                "            x2 = self.conv2(x1)\n",
                "            return x2\n",
                "\n",
                "\n",
                "class UnetDown(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(UnetDown, self).__init__()\n",
                "        '''\n",
                "        process and downscale the image feature maps\n",
                "        '''\n",
                "        layers = [\n",
                "            ResidualConvBlock(in_channels, in_channels, True),\n",
                "            ResidualConvBlock(in_channels, in_channels, True),\n",
                "            ResidualConvBlock(in_channels, out_channels),\n",
                "            nn.MaxPool2d(2)\n",
                "        ]\n",
                "        self.model = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.model(x)\n",
                "\n",
                "\n",
                "class UnetUp(nn.Module):\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(UnetUp, self).__init__()\n",
                "        '''\n",
                "        process and upscale the image feature maps\n",
                "        '''\n",
                "        layers = [\n",
                "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
                "            ResidualConvBlock(out_channels, out_channels, True),\n",
                "            ResidualConvBlock(out_channels, out_channels, True),\n",
                "        ]\n",
                "        self.model = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, x, skip):\n",
                "        x = torch.cat((x, skip), 1)\n",
                "        x = self.model(x)\n",
                "        return x\n",
                "\n",
                "\n",
                "class EmbedFC(nn.Module):\n",
                "    def __init__(self, input_dim, emb_dim):\n",
                "        super(EmbedFC, self).__init__()\n",
                "        '''\n",
                "        generic one layer FC NN for embedding things\n",
                "        '''\n",
                "        self.input_dim = input_dim\n",
                "        layers = [\n",
                "            nn.Linear(input_dim, emb_dim),\n",
                "            nn.GELU(),\n",
                "            nn.Linear(emb_dim, emb_dim),\n",
                "        ]\n",
                "        self.model = nn.Sequential(*layers)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = x.view(-1, self.input_dim)\n",
                "        return self.model(x)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yf_dB7oHTOl_"
            },
            "source": [
                "##### **Excercise 3**:\n",
                "\n",
                "Your task is to complete the forward method of the UNET architecture, ensuring it incorporates both timestep and class conditioning.\n",
                "\n",
                "It's noteworthy that there are two Multi-Layer Perceptrons (MLPs), namely timeembed1 and timeembed2, responsible for generating two time embeddings. Additionally, two other MLPs, classembed1 and classembed2, are employed to handle class conditioning information."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Unet(nn.Module):\n",
                "    def __init__(self, in_channels, n_feat = 256, num_classes: int = 10):\n",
                "        super(Unet, self).__init__()\n",
                "\n",
                "        self.in_channels = in_channels\n",
                "        self.n_feat = n_feat\n",
                "\n",
                "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
                "\n",
                "        self.down1 = UnetDown(n_feat, n_feat)\n",
                "        self.attn1 = SelfAttention(n_feat)\n",
                "\n",
                "        self.down2 = UnetDown(n_feat, 2 * n_feat)\n",
                "        self.attn2 = SelfAttention(2 * n_feat)\n",
                "\n",
                "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
                "\n",
                "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
                "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
                "\n",
                "        self.classembed1 = EmbedFC(num_classes, 2*n_feat)\n",
                "        self.classembed2 = EmbedFC(num_classes, 1*n_feat)\n",
                "\n",
                "        self.up0 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, 7, 7),\n",
                "            nn.GroupNorm(8, 2 * n_feat),\n",
                "            nn.ReLU(),\n",
                "        )\n",
                "\n",
                "        self.up1 = UnetUp(4 * n_feat, n_feat)\n",
                "        self.attn1up = SelfAttention(n_feat)\n",
                "        self.up2 = UnetUp(2 * n_feat, n_feat)\n",
                "        self.attn2up = SelfAttention(n_feat)\n",
                "        self.out = nn.Sequential(\n",
                "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
                "            nn.GroupNorm(8, n_feat),\n",
                "            nn.ReLU(),\n",
                "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
                "        )\n",
                "        self.n_classes=10\n",
                "\n",
                "    def forward(self, x, t, c=None):\n",
                "        # x is (noisy) image, t is timestep,\n",
                "\n",
                "        # Downsampling\n",
                "        x = self.init_conv(x)\n",
                "        down1 = self.attn1(self.down1(x))\n",
                "        down2 = self.attn2(self.down2(down1))\n",
                "\n",
                "        hiddenvec = self.to_vec(down2)\n",
                "\n",
                "        # get the embeddings corresponding to the time step\n",
                "        temb1 = self.timeembed1(t).view(-1, self.n_feat * 2, 1, 1)\n",
                "        temb2 = self.timeembed2(t).view(-1, self.n_feat, 1, 1)\n",
                "\n",
                "        # class condition embeddings\n",
                "        if c is not None:\n",
                "            c = torch.nn.functional.one_hot(c, num_classes=self.n_classes).type(torch.float)\n",
                "            # TODO: Get the class embeddings\n",
                "            cemb1 = ...\n",
                "            cemb2 = ...\n",
                "\n",
                "        # Upsampling\n",
                "        up1 = self.up0(hiddenvec)\n",
                "\n",
                "        if c is not None:\n",
                "           condition = up1*cemb1 + temb1\n",
                "        else:\n",
                "            condition = up1 + temb1\n",
                "        up2 = self.attn1up(self.up1(condition, down2))\n",
                "\n",
                "\n",
                "        if c is not None:\n",
                "           condition = up2*cemb2 + temb2\n",
                "        else:\n",
                "            condition = up2 + temb2\n",
                "\n",
                "        up3 = self.attn2up(self.up2(condition, down1))\n",
                "        out = self.out(torch.cat((up3, x), 1))\n",
                "        return out\n",
                "\n",
                "m = Unet(in_channels=1, n_feat=32)\n",
                "inp = torch.randn(2,1,28,28) #As a sanity check, we define a random tensor with the same shape as an input image, to make sure that we can forward it to our network\n",
                "t = torch.tensor([0]).float()\n",
                "out = m(inp,t)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "P_Q7DF0v30Ae"
            },
            "source": [
                "# Training the neural network (denoiser)\n",
                "\n",
                "We now train our denoiser model. You can select which beta scheduler to use by commenting out your choice!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Model parameters\n",
                "h_params = {\n",
                "    'n_feat': 64,\n",
                "    'n_T': 500,\n",
                "    'lr': 0.001,\n",
                "    'epochs': 25,\n",
                "}\n",
                "\n",
                "\n",
                "# Betas scheduler. Uncomment your choice!\n",
                "\n",
                "#betas = linear_beta_schedule(beta1=1e-4, beta2=0.02, T=h_params['n_T'])\n",
                "betas = cosine_beta_schedule(T=h_params['n_T'])\n",
                "\n",
                "\n",
                "ddpm_schedules_dict = ddpm_schedules(betas=betas)\n",
                "\n",
                "# Forwards Difussion Process\n",
                "forward_diffusion_process = ForwardDiffusionProcess(\n",
                "    ddpm_schedules=ddpm_schedules_dict,\n",
                "    device=device\n",
                ").to(device)\n",
                "\n",
                "\n",
                "# Reverse Diffusion Process\n",
                "denoiser_model = Unet(in_channels=1, n_feat=h_params['n_feat'])\n",
                "reverse_diffusion_process = ReverseDiffusionProcess(\n",
                "    nn_model=denoiser_model,\n",
                "    ddpm_schedules=ddpm_schedules_dict,\n",
                "    device=device\n",
                ").to(device)\n",
                "\n",
                "\n",
                "# Full Model (Only for training)\n",
                "model = DDPM(\n",
                "    forward_diffusion_process=forward_diffusion_process,\n",
                "    reverse_diffusion_process=reverse_diffusion_process,\n",
                "    nn_model=denoiser_model,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create Dataloaders\n",
                "d = torch.utils.data.Subset(train_data, indices=torch.tensor(list(range(10000))))\n",
                "train_dataloader = DataLoader(d, batch_size=128, shuffle=True, drop_last=True, num_workers=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "stz7Itxxfami"
            },
            "source": [
                "Our model follows a standard image-to-image training.\n",
                "\n",
                "For optimization, we employ the AdamW optimizer to update our parameters. The Mean Squared Error (MSE) serves as the objective function for training the UNET.\n",
                "\n",
                "If results are not satisfactory after 50 epochs, feel free to rerun the subsequent cells until the results align with expectations!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Script for training the denoiser model\n",
                "def training_diffusion(\n",
                "      model: torch.nn.Module,\n",
                "      train_dataloader,\n",
                "      epochs: int,\n",
                "      lr: float,\n",
                "      plot_every_n: int = 50,\n",
                "      add_class_condition: bool = False) -> None:\n",
                "    import time\n",
                "\n",
                "    optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
                "    loss_fn = F.mse_loss\n",
                "\n",
                "    model.to(device)\n",
                "\n",
                "    for epoch in range(epochs):\n",
                "      # Adjunst lr during trianing\n",
                "      optim.param_groups[0]['lr'] = lr * (1-epoch / epochs)\n",
                "\n",
                "      for batch_idx, (images, labels) in enumerate(train_dataloader):\n",
                "        optim.zero_grad()\n",
                "\n",
                "        images = images.to(device)\n",
                "\n",
                "        # predict mean noise, we use the forward function of the DDPM model,  which if you remember, returned both the original noise used and the predicted noise\n",
                "        if add_class_condition:\n",
                "            labels = labels.to(device)\n",
                "            noise, noise_pred = model(images.float(), labels.to(torch.int64))\n",
                "        else:\n",
                "            noise, noise_pred = model(images.float())\n",
                "\n",
                "        #MSE loss\n",
                "        loss = loss_fn(noise, noise_pred, reduction='mean')\n",
                "\n",
                "        loss.backward()\n",
                "        optim.step()\n",
                "\n",
                "        if batch_idx % plot_every_n == 0:\n",
                "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "              epoch, batch_idx * len(images), len(train_dataloader.dataset),\n",
                "              100. * batch_idx / len(train_dataloader), loss.item()))\n",
                "      t2 = time.perf_counter()\n",
                "      torch.cuda.synchronize()\n",
                "\n",
                "    return model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = training_diffusion(\n",
                "      model=model,\n",
                "      train_dataloader=train_dataloader,\n",
                "      epochs=h_params['epochs'],\n",
                "      lr=h_params['lr'],\n",
                "      plot_every_n=25\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "sQxUtPX-gJRx"
            },
            "source": [
                "### Testing the Denoiser Model\n",
                "\n",
                "The denoiser model has undergone independent training, implying that updates to the network were not based on sampling all timesteps.\n",
                "\n",
                "During testing, the procedure begins by sampling a noisy image from a Gaussian distribution. The trained denoiser model is then employed iteratively to denoise the image. It's worth noting that, unlike Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), the sampling process in Diffusion models tends to be comparatively slower. This is due to the necessity of multiple forward passes for generating a single sample.\n",
                "\n",
                "We now sample 56 different numbers!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test time:\n",
                "\n",
                "reverse_diffusion_process.eval()\n",
                "with torch.no_grad():\n",
                "    x_gen, x_gen_store = reverse_diffusion_process.sample(56, (1, 28, 28), device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final result\n",
                "\n",
                "img_grid = make_grid(x_gen.detach().cpu())\n",
                "\n",
                "plt.figure(figsize = (10, 10))\n",
                "plt.axis('off')\n",
                "plt.imshow(img_grid.permute(1, 2, 0), interpolation='nearest')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "J7YnMcgvhNvT"
            },
            "source": [
                "### Visualizing Intermediate Outputs of the Reverse Process\n",
                "\n",
                "We next show the intermediate outputs of our reverse process. For better accessibility, we've generated a gif/video in which each frame corresponds to the denoised images, transitioning from noise to the desired sample:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Gif to visualize the reverse diffusion process\n",
                "\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.animation as animation\n",
                "\n",
                "fig = plt.figure(figsize=(8, 8))\n",
                "ims = []\n",
                "for xx in x_gen_store[::16]:\n",
                "    iim = make_grid(xx).permute(1, 2, 0)\n",
                "    iim = torch.clip(iim, 0, 1)\n",
                "    im = plt.imshow(iim.numpy(), cmap=\"gray\", animated=True)\n",
                "    plt.axis('off')\n",
                "    ims.append([im])\n",
                "\n",
                "animate = animation.ArtistAnimation(fig, ims, interval=100, blit=True, repeat_delay=5000)\n",
                "animate.save('diffusion_56.gif')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# If the above cell does not show the gif, run this cell:\n",
                "from IPython.display import Image\n",
                "Image(open('diffusion_56.gif','rb').read())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Image(open('diffusion_56.gif','rb').read())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "TZjW-Xj5M755"
            },
            "source": [
                "# Extra 1: Conditional Generation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IPuM-nHuD-FD"
            },
            "source": [
                "Conditional Diffusion Models extend traditional Diffusion Models by enabling the generation process to be conditioned on additional information, incorporating context or auxiliary variables during both training and generation.\n",
                "\n",
                "In the following cell we train the same model as before, but adding the class information to the denoiser (U-Net) model. This give us the ability to control the generation process to obtain class specific images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model parameters\n",
                "h_params = {\n",
                "    'n_feat': 64,\n",
                "    'n_T': 500,\n",
                "    'lr': 0.001,\n",
                "    'epochs': 35,\n",
                "}\n",
                "\n",
                "# Betas scheduler. Uncomment your choice!\n",
                "\n",
                "betas = linear_beta_schedule(beta1=1e-4, beta2=0.02, T=h_params['n_T'])\n",
                "#betas = cosine_beta_schedule(T=h_params['n_T']) #/100\n",
                "\n",
                "\n",
                "ddpm_schedules_dict = ddpm_schedules(betas=betas)\n",
                "\n",
                "# Forwards Difussion Process\n",
                "forward_diffusion_process = ForwardDiffusionProcess(\n",
                "    ddpm_schedules=ddpm_schedules_dict,\n",
                "    device=device\n",
                ").to(device)\n",
                "\n",
                "\n",
                "# Reverse Diffusion Process\n",
                "denoiser_model = Unet(in_channels=1, n_feat=h_params['n_feat'])\n",
                "reverse_diffusion_process = ReverseDiffusionProcess(\n",
                "    nn_model=denoiser_model,\n",
                "    ddpm_schedules=ddpm_schedules_dict,\n",
                "    device=device\n",
                ").to(device)\n",
                "\n",
                "\n",
                "# Full Model (Only for training)\n",
                "model = DDPM(\n",
                "    forward_diffusion_process=forward_diffusion_process,\n",
                "    reverse_diffusion_process=reverse_diffusion_process,\n",
                "    nn_model=denoiser_model,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model = training_diffusion(\n",
                "      model=model,\n",
                "      train_dataloader=train_dataloader,\n",
                "      epochs=h_params['epochs'],\n",
                "      lr=h_params['lr'],\n",
                "      plot_every_n=25,\n",
                "      add_class_condition=True #this is the difference between the uncoditional and conditional\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_and_show(condition=0) -> None:\n",
                "    reverse_diffusion_process.eval()\n",
                "    with torch.no_grad():\n",
                "        x_gen, x_gen_store = reverse_diffusion_process.sample(\n",
                "            56,\n",
                "            (1, 28, 28),\n",
                "            device,\n",
                "            c=torch.tensor([condition]*56).to(torch.int64).to(device)\n",
                "        )\n",
                "\n",
                "    # Final result\n",
                "    img_grid = make_grid(x_gen.detach().cpu())\n",
                "\n",
                "    plt.figure(figsize = (5, 5))\n",
                "    plt.axis('off')\n",
                "    plt.imshow(img_grid.permute(1, 2, 0), interpolation='nearest')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generate_and_show(condition=0)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generate_and_show(condition=1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8yC_zYMfBvTD"
            },
            "source": [
                "# Bonus: Generate State-of-the-Art Text-to-Image Samples\n",
                "\n",
                "Many recent models, including DreamFusion, DreamBooth, StableDiffusion, DALL\u00b7E, or ControlNet, are built upon diffusion models. Typically, these models are trained on billions of samples, making it challenging to train them from scratch. Fortunately, some of them can be executed at no cost!\n",
                "\n",
                "We now present examples demonstrating the generation of images from a text prompt using the [\ud83e\udd17 Diffusers](https://huggingface.co/docs/diffusers/index) library."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install diffusers\n",
                "from diffusers import AutoPipelineForText2Image"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
                "\t\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, variant=\"fp16\"\n",
                ").to(\"cuda\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "l1zzO43ZDc5i"
            },
            "source": [
                "Once we have downloaded the model, we can generate new images by changing the text promt:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "promt = \"Shiba inu with a black hat, 8k \"\n",
                "image = pipeline(promt).images[0]\n",
                "image"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ex-5XO337Xz1"
            },
            "source": [
                "If you look inside the library, you will find many more examples. Dive deep and surprise yourself with your own generations!\n",
                "\n",
                "One example specially fun uses image-to-image pipeline, and it allows editing an input image through the information of the text prompt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from diffusers import AutoPipelineForImage2Image\n",
                "import torch\n",
                "\n",
                "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
                "    \"runwayml/stable-diffusion-v1-5\",\n",
                "    torch_dtype=torch.float16,\n",
                "    use_safetensors=True,\n",
                ").to(\"cuda\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from PIL import Image\n",
                "from io import BytesIO\n",
                "prompt = \"a portrait of a dog wearing a pearl earring\"\n",
                "\n",
                "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"\n",
                "response = requests.get(url)\n",
                "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
                "image.thumbnail((768, 768))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]\n",
                "image"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "0091df7aec2e4001802281fad436f7d3": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "02db2628441a47cb91ec72d71d184978": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "1179a1e8e5e84b46916b2e69478a8fa2": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "3220fd4e2ccf4aa7b2124026a83272f4": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_e923a2756e924d39ad035b96869ae094",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_f1a9d272564d415088ac22cde730ff74",
                        "value": "scheduler/scheduler_config.json: 100%"
                    }
                },
                "36f3b4f218b24d6f8e5b05752d58f9e3": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_0091df7aec2e4001802281fad436f7d3",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_a20815154c0d45089fe3484e197f0d3a",
                        "value": " 541/541 [00:00&lt;00:00, 39.0kB/s]"
                    }
                },
                "3728c2930bf349a886f420fc5f7611e9": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "3a612378a35f4619827ec6353f6e9372": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "46728fe08e7f42bf9a1bab1eb8369e80": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "4f920d0496324a68a4c6222799760ab8": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_7cf449ea09b8460982e50d602a9e8eb3",
                            "IPY_MODEL_98cecce366a5483d9d626562bba7f794",
                            "IPY_MODEL_ac36ac451a25436c9e53251ecfd76341"
                        ],
                        "layout": "IPY_MODEL_1179a1e8e5e84b46916b2e69478a8fa2"
                    }
                },
                "591a6d3e7ab049fbbfaedcb563174c47": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "6ba6a6c5012a491fb0570935d6a0e1c0": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_b09510bf7917436cadd689450f24f87e",
                            "IPY_MODEL_e076e9524f64491fb239a1ae8eaaf819",
                            "IPY_MODEL_36f3b4f218b24d6f8e5b05752d58f9e3"
                        ],
                        "layout": "IPY_MODEL_f055dca2287b45d1a92f9e92834bf7e7"
                    }
                },
                "75e63a854b624361ac5c645dc585605b": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "7cf449ea09b8460982e50d602a9e8eb3": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_8caa2f025c0341ae918567f999dced11",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_591a6d3e7ab049fbbfaedcb563174c47",
                        "value": "Fetching 15 files:   0%"
                    }
                },
                "86f874fe15f640779bf6b6ee538634ae": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "8caa2f025c0341ae918567f999dced11": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "8f1f8594f70b45efb5ba97bc316932e8": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_3220fd4e2ccf4aa7b2124026a83272f4",
                            "IPY_MODEL_b6020f14f9224da6bde15f6abd42f1a2",
                            "IPY_MODEL_fe431e4f593248d7963cbf80bd713fe9"
                        ],
                        "layout": "IPY_MODEL_75e63a854b624361ac5c645dc585605b"
                    }
                },
                "98cecce366a5483d9d626562bba7f794": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_f02c73695df84a4aa863f888f6b30a45",
                        "max": 15,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_bdf96d2a519e4332885d90a2125f0057",
                        "value": 0
                    }
                },
                "a20815154c0d45089fe3484e197f0d3a": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "ac36ac451a25436c9e53251ecfd76341": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_02db2628441a47cb91ec72d71d184978",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_3728c2930bf349a886f420fc5f7611e9",
                        "value": " 0/15 [00:00&lt;?, ?it/s]"
                    }
                },
                "b09510bf7917436cadd689450f24f87e": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_c15e33c5c27541b3ae30b592d7be8f84",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_c0052f69b7244eee89f96334ec5da688",
                        "value": "model_index.json: 100%"
                    }
                },
                "b6020f14f9224da6bde15f6abd42f1a2": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_f7c7692673c24c4a9204c62bf537d9e0",
                        "max": 308,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_eced0d836964445eb644211599c9a53e",
                        "value": 308
                    }
                },
                "bc5502ad21a14b71a782867d68286ebf": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "bdf96d2a519e4332885d90a2125f0057": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "c0052f69b7244eee89f96334ec5da688": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "c15e33c5c27541b3ae30b592d7be8f84": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "e076e9524f64491fb239a1ae8eaaf819": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_86f874fe15f640779bf6b6ee538634ae",
                        "max": 541,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_bc5502ad21a14b71a782867d68286ebf",
                        "value": 541
                    }
                },
                "e923a2756e924d39ad035b96869ae094": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "eced0d836964445eb644211599c9a53e": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                },
                "f02c73695df84a4aa863f888f6b30a45": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "f055dca2287b45d1a92f9e92834bf7e7": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "f1a9d272564d415088ac22cde730ff74": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "f7c7692673c24c4a9204c62bf537d9e0": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "fe431e4f593248d7963cbf80bd713fe9": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_46728fe08e7f42bf9a1bab1eb8369e80",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_3a612378a35f4619827ec6353f6e9372",
                        "value": " 308/308 [00:00&lt;00:00, 9.72kB/s]"
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
