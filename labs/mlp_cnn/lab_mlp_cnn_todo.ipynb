{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# Image Classification with a Multi-Layer Perceptron vs a Convolutional Neural Network\n",
                "\n",
                "This notebook is a merged and updated version based on two notebooks initially developed for the [UPC School's Postgraduate in Artificial Intelligence with Deep Learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) (2019). Both notebooks were originally created by [Santi Pascual](https://github.com/santi-pdp) and updated by various contributors.\n",
                "\n",
                "- **Original version in Keras**: The Keras version was created by [Miriam Bellver](https://imatge.upc.edu/web/people/miriam-bellver) for the [Barcelona Technology School](https://barcelonatechnologyschool.com/master/master-in-big-data-solutions/) (BTS) in 2018.\n",
                "- **UPC School Version**: Developed by Santi Pascual in 2019, with subsequent updates by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/), [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro), [Pol Caselles](https://www.linkedin.com/in/pcaselles/), [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/), and [Juan Jos\u00e9 Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/).\n",
                "- **Merging and Final Updates**: Merged and updated by [Laia Albors](https://www.linkedin.com/in/laia-albors-zumel-837a35211) and [\u00c0lex Sol\u00e9](https://www.linkedin.com/in/alex-sole-gomez/) (2024).\n",
                "\n"
            ],
            "metadata": {
                "id": "v_NJVH-Byl1Q"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9). The dataset we will use is the MNIST dataset, a classic dataset in the machine learning community, which has been around for almost as long as the field itself and has been very intensively studied. It's a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can think of \"solving\" MNIST as the \"Hello World\" of deep learning -- it's what you do to verify that your algorithms are working as expected. As you become a machine learning practitioner, you will see MNIST come up over and over again, in scientific papers, blog posts, and so on."
            ],
            "metadata": {
                "id": "95kRFx3hziiY"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import random\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from typing import Tuple, Dict, Any, List\n",
                "from torchvision import datasets, transforms\n",
                "from torchvision.utils import make_grid"
            ]
        },
        {
            "cell_type": "markdown",
            "source": [
                "To ensure reproducibility of the experiments, we can set the seed to a fixed number."
            ],
            "metadata": {
                "id": "PvL6PyFkznOu"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "seed = 123\n",
                "np.random.seed(seed)\n",
                "_ = torch.manual_seed(seed)\n",
                "_ = torch.cuda.manual_seed(seed)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# we select to work on GPU if it is available in the machine, otherwise\n",
                "# will run on CPU\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
                "\n",
                "# whenever we send something to the selected device (X.to(device)) we already use\n",
                "# either CPU or CUDA (GPU). Importantly...\n",
                "# The .to() operation is in-place for nn.Module's, so network.to(device) suffices\n",
                "# The .to() operation is NOT in.place for tensors, so we must assign the result\n",
                "# to some tensor, like: X = X.to(device)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Defining the Hyper-parameters\n",
                "\n",
                "We now define the hyperparameters that are going to be used throughout the notebook\n",
                "to define the network, the data `batch_size`, the training `learning_rate`, and others."
            ],
            "metadata": {
                "id": "hRjAXXlizwCz"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's define some hyper-parameters\n",
                "hparams = {\n",
                "    'batch_size':64,\n",
                "    'num_epochs':10,\n",
                "    'test_batch_size':64,\n",
                "    'hidden_size':128,\n",
                "    'num_classes':10,\n",
                "    'num_inputs':784,\n",
                "    'learning_rate_mlp':1e-4,\n",
                "    'learning_rate_cnn': 1e-3,\n",
                "    'log_interval':100,\n",
                "}"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Defining the PyTorch Dataset and the DataLoader\n",
                "\n",
                "The [PyTorch Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) is an inheritable `class` that helps us defining what source of data do we have (image, audio, text, ...) and how to load it (overriding the `__getitem__` function). The MNIST dataset is easible accessible from it.\n",
                "\n",
                "The [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is an iterator that provides batching and shuffling capabilities, among others."
            ],
            "metadata": {
                "id": "9Iq14FlUz92s"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "Each of the datasets, `mnist_trainset` and `mnist_testset`, is composed by images and labels. The model will be trained with the former and evaluated with the latter. Our images are encoded as Numpy arrays, and the labels are simply an array of digits, ranging from 0 to 9. There is a one-to-one correspondence between the images and the labels."
            ],
            "metadata": {
                "id": "B3zQqphnz-qh"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "transforms = transforms.Compose([\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.1307,), (0.3081,))\n",
                "    ])\n",
                "\n",
                "\n",
                "# Dataset initializations\n",
                "\n",
                "mnist_trainset = datasets.MNIST(\n",
                "    root='data',\n",
                "    train=True,\n",
                "    download=True,\n",
                "    transform=transforms)\n",
                "\n",
                "mnist_testset = datasets.MNIST(\n",
                "    root='data',\n",
                "    train=False,\n",
                "    download=True,\n",
                "    transform=transforms\n",
                ")\n",
                "\n",
                "# Dataloders initialization\n",
                "\n",
                "train_loader = torch.utils.data.DataLoader(\n",
                "    dataset=mnist_trainset,\n",
                "    batch_size=hparams['batch_size'],\n",
                "    shuffle=True,\n",
                "    drop_last=True,\n",
                ")\n",
                "\n",
                "test_loader = torch.utils.data.DataLoader(\n",
                "    dataset=mnist_testset,\n",
                "    batch_size=hparams['test_batch_size'],\n",
                "    shuffle=False,\n",
                "    drop_last=True,\n",
                ")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# We can retrieve a sample from the dataset by simply indexing it\n",
                "img, label = mnist_trainset[0]\n",
                "print('Img shape: ', img.shape)\n",
                "print('Label: ', label)\n",
                "\n",
                "# Similarly, we can sample a BATCH from the dataloader by running over its iterator\n",
                "iter_ = iter(train_loader)\n",
                "bimg, blabel = next(iter_)\n",
                "print('Batch Img shape: ', bimg.shape)\n",
                "print('Batch Label shape: ', blabel.shape)\n",
                "print(f'The Batched tensors return a collection of {bimg.shape[0]} grayscale images ({bimg.shape[1]} channel, {bimg.shape[2]} height pixels, {bimg.shape[3]} width pixels)')\n",
                "print(f'In the case of the labels, we obtain {blabel.shape[0]} batched integers, one per image')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "And now let's look at the kind of images we are dealing with:"
            ],
            "metadata": {
                "id": "idc4EOF90VCJ"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# make_grid is a function from the torchvision package that transforms a batch\n",
                "# of images to a grid of images\n",
                "img_grid = make_grid(bimg)\n",
                "\n",
                "plt.figure(figsize = (8, 8))\n",
                "plt.imshow(img_grid.permute(1, 2, 0), interpolation='nearest')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### BAAAAM, we've got some numbers there"
            ],
            "metadata": {
                "id": "N3XM3Tij0ZC4"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Training a Multi-Layer Perceptron (MLP)\n",
                "\n",
                "Now that we have the dataset loaded and prepared, let's get some deep stuff spinning.\n",
                "\n",
                "Our workflow will be as follow: first we will train our neural network with the training data, loaded from the constructed `train_loader`. The network will then learn to associate images and labels. Finally, we will ask the network to produce predictions for `test_loader` images, and we will verify if these predictions match the labels from `test_loader`."
            ],
            "metadata": {
                "id": "BK51Q4sS0eSe"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 1\n",
                "\n",
                "For the time being, we will use a very simple network. It consists of a sequence of two `nn.Linear` layers, which are densely-connected (also called \"fully-connected\") neural layers. The last layer is a 10-way `nn.LogSoftmax` layer, which means it will return an array of 10 log-probability scores. Each score will be the probability that the current digit image belongs to one of our 10 digit classes. Please fill in the network definition below."
            ],
            "metadata": {
                "id": "AmAiWJG00e1h"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TODO: Define a variable 'network' by instantiating a PyTorch sequential model\n",
                "# https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\n",
                "# Add a (1) nn.Linear hidden layer with 128 neurons, (2) a nn.ReLU,\n",
                "# (3) the output nn.Linear and (4) the output nn.LogSoftmax\n",
                "# NOTE: Consider the 'num_inputs', 'hidden_size', and 'num_classes' parameters\n",
                "# defined above as hyper-params\n",
                "network = ...\n",
                "\n",
                "network.to(device)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Now we can check which is the architecture of the network, and the number of parameters of each layer with the following helper function:"
            ],
            "metadata": {
                "id": "Lpekfq2L03_0"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def get_nn_nparams(net: torch.nn.Module) -> int:\n",
                "  \"\"\"\n",
                "  Function that returns all parameters regardless of the require_grad value.\n",
                "  https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/6\n",
                "  \"\"\"\n",
                "  return sum([torch.numel(p) for p in list(net.parameters())])\n",
                "\n",
                "\n",
                "print(network)\n",
                "print('Num params: ', get_nn_nparams(network))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 2\n",
                "\n",
                "To make our network ready for training, we need to pick three more things:\n",
                "\n",
                "*    **A loss function**: this is how the network will be able to measure how good a job works on its training data, and thus how it will be able to steer itself in the right direction. **Check the [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) for the negative log likelihood loss for multi-class classification.**\n",
                "*   **An optimizer**: this is the mechanism through which the network will update itself based on the data it sees and its loss function. **Check the [PyTorch documentation](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop) and find the RMSprop optimizer to use it.**\n",
                "*   **Metrics to monitor during training and testing**. Here we will only care about accuracy (the fraction of the images that were correctly classified). **Define the accuracy function to return, for a batch, the count of correct predictions (hence same prediction as the label).**"
            ],
            "metadata": {
                "id": "0QE3osHi07-6"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
                "# TODO: Specify the loss function\n",
                "criterion = ...\n",
                "\n",
                "\n",
                "# https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop\n",
                "# TODO: Create the optimizer\n",
                "optimizer = ...\n",
                "\n",
                "\n",
                "def compute_accuracy(predicted_batch: torch.Tensor, label_batch: torch.Tensor) -> int:\n",
                "    \"\"\"\n",
                "    Define the Accuracy metric in the function below by:\n",
                "      (1) obtain the maximum for each predicted element in the batch to get the\n",
                "        class (it is the maximum index of the num_classes array per batch sample)\n",
                "        (look at torch.argmax in the PyTorch documentation)\n",
                "      (2) compare the predicted class index with the index in its corresponding\n",
                "        neighbor within label_batch\n",
                "      (3) sum up the number of affirmative comparisons and return the summation\n",
                "\n",
                "    Parameters:\n",
                "    -----------\n",
                "    predicted_batch: torch.Tensor shape: [BATCH_SIZE, N_CLASSES]\n",
                "        Batch of predictions\n",
                "    label_batch: torch.Tensor shape: [BATCH_SIZE, 1]\n",
                "        Batch of labels / ground truths.\n",
                "    \"\"\"\n",
                "    pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
                "    acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
                "    return acum"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 3\n",
                "Now, we will define the training and testing functions. In each of them we will iterate over the corresponding data_loader.\n",
                "\n",
                "Since the input of the network requires a input vector of shape [BATCH_SIZE, H*W], we must rearrange the dimensions of the images. We will use `reshape()` to flatten the 2D images into 1D vectors."
            ],
            "metadata": {
                "id": "FcIFVB2F2RTH"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def train_epoch_mlp(\n",
                "        train_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        optimizer: torch.optim,\n",
                "        criterion: torch.nn.functional,\n",
                "        log_interval: int,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Activate the train=True flag inside the model\n",
                "    network.train()\n",
                "\n",
                "    avg_loss = []\n",
                "    acc = 0.\n",
                "    avg_weight = 0.1\n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "\n",
                "        # Move input data and labels to the device\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        # Set network gradients to 0.\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # TODO: rearrange the data dimension\n",
                "        data = ...\n",
                "\n",
                "        # Forward batch of images through the network\n",
                "        output = network(data)\n",
                "\n",
                "        # Compute loss\n",
                "        loss = criterion(output, target)\n",
                "\n",
                "        # Compute backpropagation\n",
                "        loss.backward()\n",
                "\n",
                "        # Update parameters of the network\n",
                "        optimizer.step()\n",
                "\n",
                "        # Compute metrics\n",
                "        acc += compute_accuracy(output, target)\n",
                "        avg_loss.append(loss.item())\n",
                "\n",
                "        if batch_idx % log_interval == 0:\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss.item()))\n",
                "    avg_acc = 100. * acc / len(train_loader.dataset)\n",
                "\n",
                "    return np.mean(avg_loss), avg_acc"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "@torch.no_grad() # decorator: avoid computing gradients\n",
                "def test_epoch_mlp(\n",
                "        test_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Dectivate the train=True flag inside the model\n",
                "    network.eval()\n",
                "\n",
                "    test_loss = []\n",
                "    acc = 0\n",
                "    for data, target in test_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        # TODO\n",
                "        data = ...\n",
                "\n",
                "        output = network(data)\n",
                "\n",
                "        # Apply the loss criterion and accumulate the loss\n",
                "        test_loss.append(criterion(output, target).item())  # sum up batch loss\n",
                "\n",
                "        # compute number of correct predictions in the batch\n",
                "        acc += compute_accuracy(output, target)\n",
                "\n",
                "    # Average accuracy across all correct predictions batches now\n",
                "    test_acc = 100. * acc / len(test_loader.dataset)\n",
                "    test_loss = np.mean(test_loss)\n",
                "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
                "        ))\n",
                "    return test_loss, test_acc"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 4\n",
                "\n",
                "Now that the `network` and the training and test functions are good to go epoch by epoch, build the loop to make as many as `hparams['num_epochs']` epochs by alternating the train and test phases. Check what these functions return and store the resulting values in lists that will be used to plot the results in the later cell."
            ],
            "metadata": {
                "id": "t5rAhRbd2h0E"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Init lists to save the evolution of the training & test losses/accuracy.\n",
                "train_losses_mlp = []\n",
                "test_losses_mlp = []\n",
                "train_accs_mlp = []\n",
                "test_accs_mlp = []\n",
                "\n",
                "# For each epoch\n",
                "for epoch in range(hparams['num_epochs']):\n",
                "\n",
                "    # Compute & save the average training loss for the current epoch\n",
                "    train_loss, train_acc = train_epoch_mlp(train_loader, network, optimizer, criterion, hparams[\"log_interval\"])\n",
                "    train_losses_mlp.append(train_loss)\n",
                "    train_accs_mlp.append(train_acc)\n",
                "\n",
                "    # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
                "    # HELP: Review the functions previously defined to implement the train/test epochs\n",
                "    test_loss, test_accuracy = ...\n",
                "\n",
                "    test_losses_mlp.append(test_loss)\n",
                "    test_accs_mlp.append(test_accuracy)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot the plots of the learning curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('NLLLoss')\n",
                "plt.plot(train_losses_mlp, label='train')\n",
                "plt.plot(test_losses_mlp, label='test')\n",
                "plt.legend()\n",
                "plt.subplot(2,1,2)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy [%]')\n",
                "plt.plot(train_accs_mlp, label='train')\n",
                "plt.plot(test_accs_mlp, label='test')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Training a Convolution Neural Network (CNN)\n",
                "\n",
                "Previously, we trained a network using fully connected layers to classify images. However, in the lectures we learned that when dealing with images, Convolutional Neural Networks (CNNs) are more convenient because they deal better with local correlations in the data (as with images). So now we are going to train a CNN for multiclass classification.\n",
                "\n",
                "The following lines show what a basic convnet looks like. It's a stack of Conv2D and MaxPooling2D layers.  "
            ],
            "metadata": {
                "id": "fcsiyg6P3l5B"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's first define a 2D convolutional layer with 1 input channel, 3 output channels\n",
                "# and (height=3, width=3) kernel size\n",
                "conv = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=(3,3))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Importantly, a convnet takes input tensors of shape `(batch, num_channels, image_height, image_width)`. In our case, we will configure our convnet to process inputs of size `(1, H, W)`, which is the format of MNIST images.\n",
                "Let's try with some random image."
            ],
            "metadata": {
                "id": "OvlY2WTL4DU_"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "x = torch.rand(1, 1, 28, 28)\n",
                "y = conv(x)\n",
                "print(f\"Output shape: {y.shape} = conv({x.shape})\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "> Alice: Note that the only actual number we specify from the input data is the number of channels! No image dimensions are given to the convolutional layer. These are just used to shape the data `x`.\n",
                ">\n",
                "> Bob: Why is that?\n",
                ">\n",
                "> <p>Alice: Because of PyTorch magic &#128526;</p>\n",
                ">\n",
                "> <p>Bob: No, seriously. Why? &#128565;</p>\n",
                ">\n",
                "> Alice: Because of the dynamic computational graph (DCG)!\n",
                "\n",
                "**Do you remember all that stuff about a certain dynamic computational graph? Well here it goes in action. Do we need to specify a fixed size for the images as in other frameworks (such as Keras, TensorFlow, etc.) ? Nope. Because we can forward any image size at any time through the same convolutional layer!**\n",
                "\n",
                "As another example. Let's forward an image of size (11, 11)."
            ],
            "metadata": {
                "id": "BQbfTMWv4Ho4"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "x = torch.rand(1, 1, 11, 11)\n",
                "y = conv(x)\n",
                "print(f\"Output shape: {y.shape} = conv({x.shape})\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Tad\u00e1aaa**, it did accept the input data, and give a corresponding output shape. **The only argument related to the data required in the convolutional definition is the number of channels.**\n",
                "\n",
                "### Exercise 5\n",
                "\n",
                "Why do the output spatial dimensions (`dim=2` and `dim=3`) differ from the input ones? Re-define the `conv` layer below setting the appropriate property such that the output spatial shape is the same as the input one. Also configure it to have three output channels. Look at the PyTorch documentation (`https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d`) for more reference."
            ],
            "metadata": {
                "id": "wsx6wvbr4LuY"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TODO: Define the conv layer below and ensure that the output tensor shape in\n",
                "# dimensions {H, W} ( as in [1, channels, H, W] ) will be the same as the input in both cases.\n",
                "conv = ...\n",
                "\n",
                "x = torch.rand(1, 1, 20, 20)\n",
                "y = conv(x)\n",
                "print(f\"Output shape: {y.shape} = conv({x.shape})\")\n",
                "\n",
                "x = torch.rand(1, 1, 11, 11)\n",
                "y = conv(x)\n",
                "print(f\"Output shape: {y.shape} = conv({x.shape})\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### About Pooling\n",
                "\n",
                "Pooling refers to a block where downsampling happens. In the case of CNNs, as they process full images throughout a certain stack of layers that can get quite deep, they occupy a lot of memory to store the so called feature maps. Feature maps are the intermediate hidden activations of a CNN. The next image ([from a Quora response](https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks)) is very self-explainatory of what **Max Pooling** does applied to images and spatial feature maps.\n",
                "\n",
                "![](https://qph.fs.quoracdn.net/main-qimg-40cdeb3b43594f4b1b1b6e2c137e80b7)\n",
                "\n",
                "As you see, it decimates neighboring regions by picking the max value within that region. And that happens for every channel in the feature map (or the image, if it is grayscale/RGB).\n",
                "\n",
                "The are also other pooling methods, like [`AvgPool2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.AvgPool2d), strided Convolutions (simply incrasing the `stride > 1` of the `torch.nn.Conv2d` layer, etc. Each one has its advantages and drawbacks, which are so far out of the scope of this study on how to define a CNN in PyTorch.\n",
                "\n",
                "In any case, a good question for now is: **what are the advantages of using pooling of any kind?**\n",
                "\n",
                "Let's define a small CNN without pooling and another one with pooling, and let's check the amount of memory used by each in terms of feature map usage and the time it takes to forward an image of `512x512` pixels with just `1` input channel (hence greyscale)."
            ],
            "metadata": {
                "id": "sjK6icT44Tta"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "NUM_BITS_FLOAT32 = 32\n",
                "\n",
                "\"\"\"\n",
                "Let's define a class that encapsulates a collection of layers we pass in\n",
                "for each forwarded layer, it retains the amount of consumed memory for\n",
                "the returned feature map. It also displays the total amount used after\n",
                "all blocks are ran.\n",
                "\"\"\"\n",
                "class CNNMemAnalyzer(nn.Module):\n",
                "\n",
                "    def __init__(self, layers: nn.Module) -> None:\n",
                "        super().__init__()\n",
                "        self.layers = layers\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> Tuple[float, List[int]]:\n",
                "        tot_mbytes = 0\n",
                "        spat_res = []\n",
                "        for layer in self.layers:\n",
                "            h = layer(x)\n",
                "            mem_h_bytes = np.cumprod(h.shape)[-1] * NUM_BITS_FLOAT32 // 8\n",
                "            mem_h_mb = mem_h_bytes / 1e6\n",
                "            print('-' * 30)\n",
                "            print(f'New feature map of shape: {h.shape}')\n",
                "            print(f'Mem usage: {mem_h_mb} MB')\n",
                "            x = h\n",
                "            if isinstance(layer, nn.Conv2d):\n",
                "                # keep track of the current spatial width for conv layers\n",
                "                spat_res.append(h.shape[-1])\n",
                "            tot_mbytes += mem_h_mb\n",
                "        print('=' * 30)\n",
                "        print('Total used memory: {:.2f} MB'.format(tot_mbytes))\n",
                "        return tot_mbytes, spat_res"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Forwarding the 512x512 image through a non-pooled CNN"
            ],
            "metadata": {
                "id": "aw7RLYNR4bQB"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# First, make a plain stack of convlayers\n",
                "cnn = CNNMemAnalyzer(\n",
                "    nn.ModuleList([\n",
                "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3)),\n",
                "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3)),\n",
                "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3)),\n",
                "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
                "        nn.Conv2d(in_channels=128, out_channels=512, kernel_size=(3,3)),\n",
                "    ])\n",
                ")\n",
                "\n",
                "# Let's work with a realistic 512x512 image size\n",
                "# Also, keep track of time to make forward\n",
                "beg_t = time.perf_counter()\n",
                "nopool_mbytes, nopool_res = cnn(torch.randn(1, 1, 512, 512))\n",
                "\n",
                "# https://pytorch.org/docs/stable/generated/torch.cuda.synchronize.html\n",
                "# Waits for all kernels in all streams on a CUDA device to complete.\n",
                "torch.cuda.synchronize(device=device)\n",
                "\n",
                "end_t = time.perf_counter()\n",
                "nopool_time = end_t - beg_t\n",
                "print('Total inference time for non-pooled CNN: {:.2f} s'.format(nopool_time))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "#### Forwarding the 512x512 image through a pooled CNN"
            ],
            "metadata": {
                "id": "XxLlorlN4hwD"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Now, let's make a stack of convlayers combined with MaxPoolings\n",
                "cnn = CNNMemAnalyzer(\n",
                "    nn.ModuleList([\n",
                "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3)),\n",
                "        nn.MaxPool2d(kernel_size=(2,2)),\n",
                "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3,3)),\n",
                "        nn.MaxPool2d(kernel_size=(2,2)),\n",
                "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3,3)),\n",
                "        nn.MaxPool2d(kernel_size=(2,2)),\n",
                "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3,3)),\n",
                "        nn.MaxPool2d(kernel_size=(2,2)),\n",
                "        nn.Conv2d(in_channels=128, out_channels=512, kernel_size=(3,3)),\n",
                "        nn.MaxPool2d(kernel_size=(2,2)),\n",
                "    ])\n",
                ")\n",
                "\n",
                "beg_t = time.perf_counter()\n",
                "pool_mbytes, pool_res = cnn(torch.randn(1, 1, 512, 512))\n",
                "torch.cuda.synchronize(device=device)\n",
                "end_t = time.perf_counter()\n",
                "pool_time = end_t - beg_t\n",
                "print('Total inference time for pooled CNN: {:.2f} s'.format(pool_time))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "mem_ratio = 1. - pool_mbytes / nopool_mbytes\n",
                "print('Total saved memory with poolings: ', 100. * mem_ratio)\n",
                "\n",
                "time_ratio = nopool_time / pool_time\n",
                "print('Total inference speed increase with poolings: x{:.1f}'.format(time_ratio))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's plot the width of each feature map as we get deeper into the network\n",
                "plt.plot(nopool_res, label='No pooling')\n",
                "plt.plot(pool_res, label='Pooling')\n",
                "plt.xticks(list(range(len(pool_res))))\n",
                "plt.xlabel('Layer index')\n",
                "plt.ylabel('Img dimension [pixels]')\n",
                "plt.legend()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Key Observations\n",
                "\n",
                "* We save 91.2% of memory having a model which is pooling after the first couple of conv layers.\n",
                "* The model that contains pooling runs 15.7 times faster in inference than the other one.\n",
                "* The width dimension decreases exponentially when inserting the poolings, compared to the one without those poolings.\n",
                "\n",
                "The convolutional operator works by sweeping the kernel filters through the input image. If we pool `x2` in a couple layers, from the 3rd convlayer onwards we have a `x4` smaller spatial resolution. This means it has to run through by far less pixels to process the whole feature map in those layers. Also, each feature occupies much less memory for the reduced resolution. Pooling is hence a practical downsampling to make our nets fit in memory, and also to get the salient features from the previous incoming feature maps (it gets the maximum activation and forwards only that one within a window). In general, for classification, it is usual to have pooling to condense spatial dimensions into less-and-more-abstract ones. This is done by finish processing the last reduced spatial feature map with some fully connected layer that mixes it all up."
            ],
            "metadata": {
                "id": "khk8WUCz4jbj"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Grand Finale: Building a (pseudo) LeNet model\n",
                "\n",
                "Here we will gather the puzzle pieces we have so far (tensor manipulations, convs, poolings, fully connected layers, etc.) and we will define a pseudo-LeNet model ([LeNet ref](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)). We say \"pseudo\" because we will obviate the type of actual pooling that the authors proposed by then, or the gaussian connections, or the `Tanh` activations. We will instead use `MaxPooling`s, `Fully connected` layers all the way through, and `ReLU` activations.\n"
            ],
            "metadata": {
                "id": "4PqNjEwx4r5r"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 6\n",
                "\n",
                "Make the `ConvBlock` class to properly do: `Conv2d`, `ReLU`, and `MaxPool2d`. Ensure that for an input of size `1x32x32` you obtain an output feature map of size `6x14x14` as shown in the figure above for layer `S2`."
            ],
            "metadata": {
                "id": "iBhCJq7O40un"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class ConvBlock(nn.Module):\n",
                "\n",
                "    def __init__(\n",
                "            self,\n",
                "            num_inp_channels: int,\n",
                "            num_out_fmaps: int,\n",
                "            kernel_size: int,\n",
                "            pool_size: int=2) -> None:\n",
                "\n",
                "        super().__init__()\n",
                "\n",
                "        # TODO: define the 3 modules needed\n",
                "        self.conv = ...\n",
                "        self.relu = ...\n",
                "        self.maxpool = ...\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        return self.maxpool(self.relu(self.conv(x)))\n",
                "\n",
                "\n",
                "\n",
                "model = ConvBlock(\n",
                "    num_inp_channels=1,\n",
                "    num_out_fmaps=6,\n",
                "    kernel_size=5,\n",
                "    pool_size=2)\n",
                "\n",
                "# run forward pass\n",
                "x = torch.randn(1, 1, 32, 32)\n",
                "y = model(x)\n",
                "\n",
                "assert y.shape[1] == 6, 'The amount of feature maps is not correct!'\n",
                "assert y.shape[2] == 14 and y.shape[3] == 14, 'The spatial dimensions are not correct!'\n",
                "print(f'Input shape: {x.shape}')\n",
                "print(f'ConvBlock output shape (S2 level in Figure): {y.shape}')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 7\n",
                "\n",
                "Finish the `PseudoLeNet` class by including the following:\n",
                "1. As the input images from MNIST are 28x28, add padding to make them 32x32 with the `torch.nn.ConstantPad2d` (https://pytorch.org/docs/stable/nn.html#torch.nn.ConstantPad2d).\n",
                "2. Build the `mlp` classifier as a `nn.Sequential` stack of fully connected layers and ReLU activations, with the sizes shown in the figure above: [120, 84, 10]. Plug the appropriate output activation in the end to do multi-class classification.\n",
                "3. Remember to \"flatten\" the feature maps coming out of the second `ConvBlock` and connect them to the output `mlp` to build the classifier in the `forward` function. This has to be done because fully connected layers (`Linear`) only accept features without any spatial dimension. Hence, all these spatial dimensions and channels are unrolled into single vectors, one per batch sample. **HINT: Remember the `.reshape()` operator to change tensors shape!**"
            ],
            "metadata": {
                "id": "1D1kY9pv47l2"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "class PseudoLeNet(nn.Module):\n",
                "\n",
                "    def __init__(self) -> None:\n",
                "        super().__init__()\n",
                "        # TODO: Define the zero-padding\n",
                "        self.pad = ...\n",
                "\n",
                "        self.conv1 = ConvBlock(num_inp_channels=1, num_out_fmaps=6, kernel_size=5)\n",
                "        self.conv2 = ConvBlock(num_inp_channels=6, num_out_fmaps=16, kernel_size=5)\n",
                "\n",
                "        # TODO: Define the MLP at the deepest layers\n",
                "        self.mlp = nn.Sequential(...)\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        x = self.pad(x)\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        # Obtain the parameters of the tensor in terms of:\n",
                "        # 1) batch size\n",
                "        # 2) number of channels\n",
                "        # 3) spatial \"height\"\n",
                "        # 4) spatial \"width\"\n",
                "        bsz, nch, height, width = x.shape\n",
                "        # TODO: Flatten the feature map with the reshape() operator\n",
                "        # within each batch sample\n",
                "        x = ...\n",
                "\n",
                "        y = self.mlp(x)\n",
                "        return y"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's forward a toy example emulating the MNIST image size\n",
                "plenet = PseudoLeNet()\n",
                "y = plenet(torch.randn(1, 1, 28, 28))\n",
                "print(f\"Output shape: {y.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Now it's time to train and test the model.\n",
                "\n",
                "We use the previous training and test code from the MLP section, only changing the way we input the data into the model, as with CNNs, we do not need to flatten the images beforehand. Execute the cells below without further hesitation."
            ],
            "metadata": {
                "id": "NXQ23F-o5KAX"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def train_epoch_cnn(\n",
                "        train_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        optimizer: torch.optim,\n",
                "        criterion: torch.nn.functional,\n",
                "        log_interval: int,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Activate the train=True flag inside the model\n",
                "    network.train()\n",
                "\n",
                "    train_loss = []\n",
                "    acc = 0.\n",
                "    avg_weight = 0.1\n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "\n",
                "        # Move input data and labels to the device\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        # Set network gradients to 0.\n",
                "        optimizer.zero_grad()\n",
                "\n",
                "        # Forward batch of images through the network\n",
                "        output = network(data)\n",
                "\n",
                "        # Compute loss\n",
                "        loss = criterion(output, target)\n",
                "\n",
                "        # Compute backpropagation\n",
                "        loss.backward()\n",
                "\n",
                "        # Update parameters of the network\n",
                "        optimizer.step()\n",
                "\n",
                "        # Compute metrics\n",
                "        acc += compute_accuracy(output, target)\n",
                "        train_loss.append(loss.item())\n",
                "\n",
                "        if batch_idx % log_interval == 0:\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss.item()))\n",
                "    avg_acc = 100. * acc / len(train_loader.dataset)\n",
                "\n",
                "    return np.mean(train_loss), avg_acc"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "@torch.no_grad() # decorator: avoid computing gradients\n",
                "def test_epoch_cnn(\n",
                "        test_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Dectivate the train=True flag inside the model\n",
                "    network.eval()\n",
                "\n",
                "    test_loss = []\n",
                "    acc = 0\n",
                "    for data, target in test_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        output = network(data)\n",
                "\n",
                "        # Apply the loss criterion and accumulate the loss\n",
                "        test_loss.append(criterion(output, target).item())\n",
                "\n",
                "        # compute number of correct predictions in the batch\n",
                "        acc += compute_accuracy(output, target)\n",
                "\n",
                "    # Average accuracy across all correct predictions batches now\n",
                "    test_acc = 100. * acc / len(test_loader.dataset)\n",
                "    test_loss = np.mean(test_loss)\n",
                "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
                "        ))\n",
                "    return test_loss, test_acc"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "train_losses_cnn = []\n",
                "test_losses_cnn = []\n",
                "train_accs_cnn = []\n",
                "test_accs_cnn = []\n",
                "network = PseudoLeNet()\n",
                "network.to(device)\n",
                "\n",
                "optimizer = torch.optim.RMSprop(network.parameters(), lr=hparams['learning_rate_cnn'])\n",
                "criterion = nn.NLLLoss(reduction='mean')\n",
                "\n",
                "for epoch in range(hparams['num_epochs']):\n",
                "\n",
                "    # Compute & save the average training loss for the current epoch\n",
                "    train_loss, train_acc = train_epoch_cnn(train_loader, network, optimizer, criterion, hparams[\"log_interval\"])\n",
                "    train_losses_cnn.append(train_loss)\n",
                "    train_accs_cnn.append(train_acc)\n",
                "\n",
                "    # TODO: Compute & save the average test loss & accuracy for the current epoch\n",
                "    # HELP: Review the functions previously defined to implement the train/test epochs\n",
                "    test_loss, test_accuracy = ...\n",
                "\n",
                "    test_losses_cnn.append(test_loss)\n",
                "    test_accs_cnn.append(test_accuracy)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot the plots of the learning curves\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('NLLLoss')\n",
                "plt.plot(train_losses_cnn, label='train')\n",
                "plt.plot(test_losses_cnn, label='test')\n",
                "plt.legend()\n",
                "plt.subplot(2,1,2)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy [%]')\n",
                "plt.plot(train_accs_cnn, label='train')\n",
                "plt.plot(test_accs_cnn, label='test')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Let's now compare both types of models\n",
                "\n",
                "The testing accuracy using the CNN model should be slightly below 99%, better than the MLP model for a comparable amount of training."
            ],
            "metadata": {
                "id": "cjmEjzOG6bb9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Test NLLLoss')\n",
                "plt.plot(test_losses_mlp, label='mlp')\n",
                "plt.plot(test_losses_cnn, label='cnn')\n",
                "plt.legend()\n",
                "plt.subplot(2,1,2)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Test Accuracy [%]')\n",
                "plt.plot(test_accs_mlp, label='mlp')\n",
                "plt.plot(test_accs_cnn, label='cnn')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "# References\n",
                "\n",
                "[1] https://github.com/pytorch/examples/blob/master/mnist/main.py"
            ],
            "metadata": {
                "id": "MBCjWp3z3Xvo"
            }
        }
    ]
}
