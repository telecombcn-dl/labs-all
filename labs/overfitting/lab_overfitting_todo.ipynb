{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "5G3_R4Rk_g0O"
            },
            "source": [
                "# Fighting Overfitting \n",
                "\n",
                "Lab created by [Santi Pascual](https://github.com/santi-pdp) and [Xavier Giro-i-Nieto](https://imatge.upc.edu/web/people/xavier-giro) for the [UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) (2019) and updated by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) and [Juan Jos\u00e9 Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/) in 2022.\n",
                "\n",
                "Based on an original version in Keras created by [Miriam Bellver](https://imatge.upc.edu/web/people/miriam-bellver) for the [Barcelona Technology School](https://barcelonatechnologyschool.com/master/master-in-big-data-solutions/) (BTS) in 2018."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dbaCSnLW_gKp"
            },
            "source": [
                "### Advice: Select the GPU Hardware Acceleration in the Runtime environment Menu to train the network fast."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xIlbdMD2_oA_"
            },
            "source": [
                "We will start replicating the imports, Dataset and training/evaluation routines from the last labs (MLPs and CNNs). Nonetheless, we will work with the [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html). This dataset contains 50.000 training images of 10 classes and 10.000 test images. Does it sound familiar? Yes! MNIST follows a similar structure, but CIFAR contains natural images (planes, dogs, cars, ...) whereas MNIST has quite simplistic patterns that would not allow us to appreciate the challenges that appear when we transfer from training data to evaluation data. \n",
                "\n",
                "We will refer to evaluation data to the so called validation data. Typically we have two splits during model development and a final split to test it to report results. The former splits are training (obviously) and validation/evaluation. Validation is useful to check how our model performs out of the training corpus. If the model is good at both training and validation, then our model is prone to \"generalize\" to the test data (real world data). If our model is very good at training and very bad at validation... well you will see later on in this lab the concept... but obviously the model must be changed to get better at validation data as well as training data. Once all this development is done and we match the performance on both splits, we can test the model with data that is non-related at all to the training process.\n",
                "\n",
                "**NOTE:** in this lab we do not have test data. We consider the CIFAR test split as our validation set to check how model changes affect generalization. Do not let this misguide you. This is just for learning purposes. So we emulate the validation set with the CIFAR test set for convenience only, do not forget."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import random\n",
                "import numpy as np\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "from random import shuffle\n",
                "from typing import Tuple, Dict, Any, List\n",
                "from torchvision import datasets, transforms\n",
                "from torchvision.utils import make_grid\n",
                "\n",
                "import matplotlib\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 123\n",
                "random.seed(seed)\n",
                "np.random.seed(seed)\n",
                "_ = torch.manual_seed(seed)\n",
                "_ = torch.cuda.manual_seed(seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# we select to work on GPU if it is available in the machine, otherwise\n",
                "# will run on CPU\n",
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ma9PKiw5_uhq"
            },
            "source": [
                "### Defining the Hyper-parameters\n",
                "\n",
                "We now define the hyperparameters that are going to be used throughout the notebook\n",
                "to define the network, the data `batch_size`, the training `learning_rate`, and others."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's define some hyper-parameters\n",
                "hparams = {\n",
                "    'batch_size': 100,\n",
                "    'num_epochs': 12,\n",
                "    'val_batch_size': 100,\n",
                "    'num_classes': 10,\n",
                "    'learning_rate': 1e-3,\n",
                "    'log_interval': 100,\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zxATQ1upVZ1z"
            },
            "source": [
                "## Defining the training and validation sets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "])\n",
                "\n",
                "# Dataset initializations\n",
                "\n",
                "trainset = datasets.CIFAR10(\n",
                "    root='data', \n",
                "    train=True, \n",
                "    download=True,\n",
                "    transform=transform\n",
                ")\n",
                "\n",
                "evalset = datasets.CIFAR10(\n",
                "    root='data', \n",
                "    train=False, \n",
                "    download=True,\n",
                "    transform=transform\n",
                ")\n",
                "\n",
                "# Pick only the first 10k samples\n",
                "trainset.data = trainset.data[:10000]\n",
                "trainset.targets = trainset.targets[:10000]\n",
                "\n",
                "\n",
                "# Dataloders initialization\n",
                "\n",
                "train_loader = torch.utils.data.DataLoader(\n",
                "    dataset=trainset,\n",
                "    batch_size=hparams['batch_size'],\n",
                "    shuffle=True,\n",
                "    drop_last=True,\n",
                ")\n",
                "\n",
                "test_loader = torch.utils.data.DataLoader(\n",
                "    dataset=evalset,\n",
                "    batch_size=hparams['val_batch_size'],\n",
                "    shuffle=False,\n",
                "    drop_last=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We can retrieve a sample from the dataset by simply indexing it\n",
                "img, label = trainset[0]\n",
                "print('Img shape: ', img.shape)\n",
                "print('Label: ', label)\n",
                "\n",
                "# Similarly, we can sample a BATCH from the dataloader by running over its iterator\n",
                "iter_ = iter(train_loader)\n",
                "bimg, blabel = next(iter_)\n",
                "print('Batch Img shape: ', bimg.shape)\n",
                "print('Batch Label shape: ', blabel.shape)\n",
                "print(f'The Batched tensors return a collection of {bimg.shape[0]} RGB images \\\n",
                "({bimg.shape[1]} channel, {bimg.shape[2]} height pixels, {bimg.shape[3]} width \\\n",
                "pixels)')\n",
                "print(f'In the case of the labels, we obtain {blabel.shape[0]} batched integers, one per image')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xbNNX5rwh_cI"
            },
            "source": [
                "Let's make a small function to show some CIFAR10 example images. You can see the type of `32x32x3` images we are dealing with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainset = datasets.CIFAR10(\n",
                "    root='data', \n",
                "    train=True, \n",
                "    download=True,\n",
                "    transform=transforms.ToTensor()\n",
                ")\n",
                "loader = torch.utils.data.DataLoader(\n",
                "    dataset=trainset,\n",
                "    batch_size=32,\n",
                ")\n",
                "# make_grid is a function from the torchvision package that transforms a batch\n",
                "# of images to a grid of images\n",
                "img, _ = next(iter(loader))\n",
                "img_grid = make_grid(img) \n",
                "\n",
                "plt.figure(figsize = (8, 8))\n",
                "plt.imshow(img_grid.permute(1, 2, 0), interpolation='nearest')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_accuracy(predicted_batch: torch.Tensor, label_batch: torch.Tensor) -> float:\n",
                "    \"\"\"\n",
                "    Define the Accuracy metric in the function below by:\n",
                "      (1) obtain the maximum for each predicted element in the batch to get the\n",
                "        class (it is the maximum index of the num_classes array per batch sample)\n",
                "        (look at torch.argmax in the PyTorch documentation)\n",
                "      (2) compare the predicted class index with the index in its corresponding\n",
                "        neighbor within label_batch\n",
                "      (3) sum up the number of affirmative comparisons and return the summation\n",
                "\n",
                "    Parameters:\n",
                "    -----------\n",
                "    predicted_batch: torch.Tensor shape: [BATCH_SIZE, N_CLASSES]\n",
                "        Batch of predictions\n",
                "    label_batch: torch.Tensor shape: [BATCH_SIZE, 1]\n",
                "        Batch of labels / ground truths.\n",
                "    \"\"\"\n",
                "    pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
                "    acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
                "    return acum"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VA0ygPzoq6-2"
            },
            "source": [
                "Now let's define the training and evaluation functions, which will be called later to train different models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_epoch(\n",
                "        train_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        optimizer: torch.optim,\n",
                "        criterion: torch.nn.functional,\n",
                "        epoch: int,\n",
                "        log_interval: int,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Activate the train=True flag inside the model\n",
                "    network.train()\n",
                "\n",
                "    train_loss = []\n",
                "    acc = 0.\n",
                "    avg_weight = 0.1\n",
                "    for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        output = network(data)\n",
                "        loss = criterion(output, target)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        # Compute metrics\n",
                "        acc += compute_accuracy(output, target)\n",
                "        train_loss.append(loss.item())\n",
                "\n",
                "        if batch_idx % log_interval == 0 or batch_idx >= len(train_loader):\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss.item()))\n",
                "    avg_acc = 100. * acc / len(train_loader.dataset)\n",
                "\n",
                "    return np.mean(train_loss), avg_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.no_grad() # decorator: avoid computing gradients\n",
                "def eval_epoch(\n",
                "        test_loader: torch.utils.data.DataLoader,\n",
                "        network: torch.nn.Module,\n",
                "        criterion: torch.nn.functional,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    # Dectivate the train=True flag inside the model\n",
                "    network.eval()\n",
                "    \n",
                "    test_loss = 0\n",
                "    acc = 0\n",
                "    for data, target in test_loader:\n",
                "        data, target = data.to(device), target.to(device)\n",
                "\n",
                "        output = network(data)\n",
                "\n",
                "        # Apply the loss criterion and accumulate the loss\n",
                "        test_loss += criterion(output, target).item()\n",
                "\n",
                "        # compute number of correct predictions in the batch\n",
                "        acc += compute_accuracy(output, target)\n",
                "\n",
                "    test_loss /= len(test_loader)\n",
                "    # Average accuracy across all correct predictions batches now\n",
                "    test_acc = 100. * acc / len(test_loader.dataset)\n",
                "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "        test_loss, acc, len(test_loader.dataset), test_acc,\n",
                "        ))\n",
                "    return test_loss, test_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_net(\n",
                "        network: torch.nn.Module,\n",
                "        train_loader: torch.utils.data.DataLoader,\n",
                "        eval_loader: torch.utils.data.DataLoader,    \n",
                "        optimizer: torch.optim,\n",
                "        num_epochs: int,\n",
                "        plot: bool=True,\n",
                "        ) -> Dict[str, List[float]]:\n",
                "    \n",
                "    \"\"\" Function that trains and evals a network for num_epochs,\n",
                "      showing the plot of losses and accs and returning them.\n",
                "    \"\"\"\n",
                "    tr_losses = []\n",
                "    tr_accs = []\n",
                "    te_losses = []\n",
                "    te_accs = []\n",
                "\n",
                "    network.to(device)\n",
                "    criterion = nn.NLLLoss(reduction='mean')\n",
                "\n",
                "    for epoch in range(1, num_epochs + 1):\n",
                "        tr_loss, tr_acc = train_epoch(train_loader, network, optimizer, criterion, epoch, hparams[\"log_interval\"])\n",
                "        te_loss, te_acc = eval_epoch(eval_loader, network, criterion)\n",
                "        te_losses.append(te_loss)\n",
                "        te_accs.append(te_acc)\n",
                "        tr_losses.append(tr_loss)\n",
                "        tr_accs.append(tr_acc)\n",
                "    rets = {'tr_losses':tr_losses, 'te_losses':te_losses,\n",
                "          'tr_accs':tr_accs, 'te_accs':te_accs}\n",
                "    if plot:\n",
                "        plt.figure(figsize=(10, 8))\n",
                "        plt.subplot(2,1,1)\n",
                "        plt.xlabel('Epoch')\n",
                "        plt.ylabel('NLLLoss')\n",
                "        plt.plot(tr_losses, label='train')\n",
                "        plt.plot(te_losses, label='eval')\n",
                "        plt.legend()\n",
                "        plt.subplot(2,1,2)\n",
                "        plt.xlabel('Epoch')\n",
                "        plt.ylabel('Eval Accuracy [%]')\n",
                "        plt.plot(tr_accs, label='train')\n",
                "        plt.plot(te_accs, label='eval')\n",
                "        plt.legend()\n",
                "    return rets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q_ZCpq7SqlA9"
            },
            "source": [
                "We now define a `ConvBlock` layer that is composed by a convolution, a relu and the possibility to do a striding > 1. If we `stride > 1`, as commented in the previous lab about `CNN` classification, we perform a spatial decimation (hence we downsample the feature map resolution). This increases speed, reduces memory usage and condenses spatial features into non-spatial-but-abstract ones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConvBlock(nn.Module):\n",
                "\n",
                "    def __init__(\n",
                "            self, \n",
                "            num_inp_channels: int, \n",
                "            num_out_fmaps: int,\n",
                "            kernel_size: int, \n",
                "            stride: int=1,\n",
                "            ) -> None:\n",
                "\n",
                "        super().__init__()\n",
                "\n",
                "        self.kernel_size = kernel_size\n",
                "        self.stride = stride\n",
                "        self.conv = nn.Conv2d(\n",
                "            in_channels=num_inp_channels, \n",
                "            out_channels=num_out_fmaps,\n",
                "            kernel_size=(kernel_size, kernel_size),\n",
                "            stride=(stride, stride),\n",
                "        )\n",
                "        self.relu = nn.ReLU(inplace=True)\n",
                "  \n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        P_ = self.kernel_size // 2\n",
                "        if self.stride > 1:\n",
                "            P = (P_ - 1, P_, P_  - 1, P_ )\n",
                "        else:\n",
                "            P = (P_, P_, P_, P_)\n",
                "        x = self.conv(nn.functional.pad(x, P, mode='constant'))\n",
                "        return self.relu(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4QDUS6RlrI3k"
            },
            "source": [
                "We now define a helper function that tells us the amount of parameters contained in our model. This will help us understand possible issues of our design in terms of model magnitude."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_nn_nparams(net: torch.nn.Module) -> int:\n",
                "  \"\"\"\n",
                "  Function that returns all parameters regardless of the require_grad value.\n",
                "  https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/6\n",
                "  \"\"\"\n",
                "  return sum([torch.numel(p) for p in list(net.parameters())])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kFBq8zFbrFAY"
            },
            "source": [
                "### Our Model Design\n",
                "\n",
                "We will work with a `CNN` for obvious reasons: image data. In this case note that our number of input channels are 3 because CIFAR has RGB format, not greyscale as with MNIST. Also, we define a deep network with 2 convolutional layers and a simple `MLP` on top that builds the classifier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BigNet(nn.Module):\n",
                "\n",
                "    def __init__(self) -> None:\n",
                "        \n",
                "        super().__init__()\n",
                "        self.conv1 = ConvBlock(\n",
                "            num_inp_channels=3, \n",
                "            num_out_fmaps=512, \n",
                "            kernel_size=3, \n",
                "            stride=4)\n",
                "        \n",
                "        self.conv2 = ConvBlock(\n",
                "            num_inp_channels=512, \n",
                "            num_out_fmaps=1024, \n",
                "            kernel_size=3, \n",
                "            stride=4)\n",
                "        \n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(\n",
                "                in_features=1024 * 2 * 2, \n",
                "                out_features=1024\n",
                "            ),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(\n",
                "                in_features=1024, \n",
                "                out_features=hparams['num_classes']\n",
                "            ),\n",
                "            nn.LogSoftmax(dim=-1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        x = self.conv1(x)\n",
                "        x = self.conv2(x)\n",
                "        bsz, nch, height, width = x.shape\n",
                "        x = x.reshape(bsz, -1)\n",
                "        y = self.mlp(x)\n",
                "        return y"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bignet = BigNet()\n",
                "# let's check the num of params\n",
                "nparams = get_nn_nparams(bignet)\n",
                "print('BigNet number of params: ', nparams)\n",
                "optimizer = torch.optim.Adam(bignet.parameters(), lr=hparams['learning_rate'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2lNnffigtewe"
            },
            "source": [
                "Our model has approximately 9M params, not bad honestly (but not as large as many models nowadays). Nonetheless, it will serve our purpose to see the mismatching fit between train and eval sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "bigmodel_log = train_net(bignet, train_loader, test_loader, optimizer, hparams['num_epochs'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8Ib0YzMDu44k"
            },
            "source": [
                "The blue lines reflect the training loss and accuracy, while the orange lines are the evaluation (or validaton) loss and accuracy. Note that your own results may vary slightly due to a different random initialization of your network.\n",
                "\n",
                "As you can see, the training loss decreases with every epoch and the training accuracy increases with every epoch. That's what you would expect when running gradient descent optimization -- the quantity you are trying to minimize should get lower with every iteration. But that isn't the case for the validation loss and accuracy: they seem to peak at the fourth epoch. This is an example of what we were warning against earlier: a model that performs better on the training data isn't necessarily a model that will do better on data it has never seen before. In precise terms, what you are seeing is \"overfitting\": after the second epoch, we are over-optimizing on the training data, and we ended up learning representations that are specific to the training data and do not generalize to data outside of the training set.\n",
                "\n",
                "In this case, to prevent overfitting, we could simply stop training after three epochs. In general, there is a range of techniques you can leverage to mitigate overfitting"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MuFF3v5jtxdX"
            },
            "source": [
                "## Overfitting and underfitting\n",
                "\n",
                "When training networks, we can notice that the performance of our model on the validation data would always peak after a few epochs and would then start degrading, i.e. our model would quickly start to overfit to the training data. Overfitting happens in every single machine learning problem. Learning how to deal with overfitting is essential to mastering machine learning.\n",
                "\n",
                "The fundamental issue in machine learning is the tension between optimization and generalization. \"Optimization\" refers to the process of adjusting a model to get the best performance possible on the training data (the \"learning\" in \"machine learning\"), while \"generalization\" refers to how well the trained model would perform on data it has never seen before. The goal of the game is to get good generalization, of course, but you do not control generalization; you can only adjust the model based on its training data.\n",
                "\n",
                "At the beginning of training, optimization and generalization are correlated: the lower your loss on training data, the lower your loss on test data. While this is happening, your model is said to be under-fit: there is still progress to be made; the network hasn't yet modeled all relevant patterns in the training data. But after a certain number of iterations on the training data, generalization stops improving, validation metrics stall then start degrading: the model is then starting to over-fit, i.e. is it starting to learn patterns that are specific to the training data but that are misleading or irrelevant when it comes to new data.\n",
                "\n",
                "To prevent a model from learning misleading or irrelevant patterns found in the training data, the best solution is of course to get more training data. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to modulate the quantity of information that your model is allowed to store, or to add constraints on what information it is allowed to store. If a network can only afford to memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns, which have a better chance of generalizing well.\n",
                "\n",
                "The processing of fighting overfitting in this way is called regularization. Let's review some of the most common regularization techniques, and let's apply them in practice to improve our model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8TqkpnR7uNFI"
            },
            "source": [
                "## Fighting overfitting\n",
                "\n",
                "### Reducing the network's size\n",
                "\n",
                "The simplest way to prevent overfitting is to reduce the size of the model, i.e. the number of learnable parameters in the model (which is determined by the number of layers and the number of units per layer). In deep learning, the number of learnable parameters in a model is often referred to as the model's \"capacity\". Intuitively, a model with more parameters will have more \"memorization capacity\" and therefore will be able to easily learn a perfect dictionary-like mapping between training samples and their targets, a mapping without any generalization power. For instance, a model with 500,000 binary parameters could easily be made to learn the class of every digits in the MNIST training set: we would only need 10 binary parameters for each of the 50,000 digits. Such a model would be useless for classifying new digit samples. Always keep this in mind: deep learning models tend to be good at fitting to the training data, **but the real challenge is generalization, not fitting**.\n",
                "\n",
                "On the other hand, if the network has limited memorization resources, it will not be able to learn this mapping as easily, and thus, in order to minimize its loss, it will have to resort to learning compressed representations that have predictive power regarding the targets -- precisely the type of representations that we are interested in. At the same time, keep in mind that **you should be using models that have enough parameters that they won't be underfitting: your model shouldn't be starved for memorization resources**. There is a compromise to be found between \"too much capacity\" and \"not enough capacity\".\n",
                "\n",
                "Unfortunately, there is no magical formula to determine what the right number of layers is, or what the right size for each layer is. You will have to evaluate an array of different architectures (on your validation set, not on your test set, of course) in order to find the right model size for your data.\n",
                "\n",
                "Let's try to reduce our original network size with the following design:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SmallNet(BigNet):\n",
                "\n",
                "    def __init__(self) -> None:\n",
                "        super().__init__()\n",
                "        self.conv1 = ConvBlock(num_inp_channels=3, num_out_fmaps=128, kernel_size=3, stride=4)\n",
                "        self.conv2 = ConvBlock(num_inp_channels=128, num_out_fmaps=256, kernel_size=3, stride=4)\n",
                "        self.mlp = nn.Sequential(\n",
                "            nn.Linear(in_features=256 * 2 * 2, out_features=512),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Linear(in_features=512, out_features=hparams['num_classes']),\n",
                "            nn.LogSoftmax(dim=-1)\n",
                "        )\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "smallnet = SmallNet()\n",
                "# let's check the num of params\n",
                "nparams = get_nn_nparams(smallnet)\n",
                "print('SmallNet number of params: ', nparams)\n",
                "optimizer = torch.optim.Adam(smallnet.parameters(), lr=hparams['learning_rate'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "smallmodel_log = train_net(smallnet, train_loader, test_loader, optimizer, hparams['num_epochs'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "24-6s7cDw3eP"
            },
            "source": [
                "Let's plot the overlay of both networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.plot(bigmodel_log['te_losses'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_losses'], label='smallmodel')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Loss')\n",
                "plt.legend()\n",
                "plt.subplot(2,1,2)\n",
                "plt.plot(bigmodel_log['te_accs'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_accs'], label='smallmodel')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Accs')\n",
                "plt.legend()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "astnI-qrzV5X"
            },
            "source": [
                "As you can see, the smaller network starts overfitting later than the reference one (after 6 epochs rather than 4) and its performance degrades much more slowly once it starts overfitting. In terms of accuracy, however, there is not much difference, and can even get a bit worse (depending on the seed). In general, reducing the network size is a viable strategy but might not be a good one, as we might be impeding a needed set of parameters by the model that we can instead \"guide\" to learn in a better way. In other words, reducing the model size can lead to capacity limitation, so that the model overfits slowly but reaches worse quotas. On the other hand, regularizing the big model might also slow down overfitting appearance, and reach better quotas than both the small model and the big non-regularized one."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kc-3O_FVzvEp"
            },
            "source": [
                "##Exercise 1: Adding L2 weight regularization\n",
                "\n",
                "You may be familiar with Occam's Razor principle: given two explanations for something, the explanation most likely to be correct is the \"simplest\" one, the one that makes the least amount of assumptions. This also applies to the models learned by neural networks: given some training data and a network architecture, there are multiple sets of weights values (multiple models) that could explain the data, and simpler models are less likely to overfit than complex ones.\n",
                "\n",
                "A \"simple model\" in this context is a model where the distribution of parameter values has less entropy (or a model with fewer parameters, as we saw in the section above). Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to only take small values, which makes the distribution of weight values more \"regular\". This is called \"weight regularization\", and it is done by adding to the loss function of the network a cost associated with having large weights. This cost comes in two flavors:\n",
                "\n",
                "* L1 regularization, where the cost added is proportional to the absolute value of the weights coefficients (i.e. to what is called the \"L1 norm\" of the weights).\n",
                "* L2 regularization, where the cost added is proportional to the square of the value of the weights coefficients (i.e. to what is called the \"L2 norm\" of the weights). L2 regularization is also called weight decay in the context of neural networks. Don't let the different name confuse you: weight decay is mathematically the exact same as L2 regularization.\n",
                "\n",
                "\n",
                "**TODO:** In PyTorch, L2 weight regularization is added by controling the weight decay argument in the optimizer once this is built. Check the docs about the `Adam` optimizer and add build a `BigNet` where a `weight_decay` of 0.001 is applied (https://pytorch.org/docs/stable/optim.html#torch.optim.Adam). Then, train the model following the `train_net` function call, returning the value to the `bigmodel_l2_log` dictionary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO\n",
                "bignet = ...\n",
                "optimizer = ...\n",
                "bigmodel_l2_log = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.plot(bigmodel_log['te_losses'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_losses'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['te_losses'], label='bigmodel L2')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Loss')\n",
                "plt.legend()\n",
                "plt.subplot(2,1,2)\n",
                "plt.plot(bigmodel_log['te_accs'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_accs'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['te_accs'], label='bigmodel L2')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Accs')\n",
                "plt.legend()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "flidGvZ52Fvi"
            },
            "source": [
                "As you can see, the model with L2 regularization (green) has become much more resistant to overfitting than the other two previous models (big and small). Actually, its robustness to overfitting is even better than that of the small model without regularization!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gQ2aYPZG1qDi"
            },
            "source": [
                "## Exercise 2: Adding dropout\n",
                "\n",
                "Dropout is one of the most effective and most commonly used regularization techniques for neural networks, developed by Hinton and his students at the University of Toronto. Dropout, applied to a layer, consists of randomly \"dropping out\" (i.e. setting to zero) a number of output features of the layer during training. Let's say a given layer would normally have returned a vector [0.2, 0.5, 1.3, 0.8, 1.1] for a given input sample during training; after applying dropout, this vector will have a few zero entries distributed at random, e.g. [0, 0.5, 1.3, 0, 1.1]. The \"dropout rate\" is the fraction of the features that are being zeroed-out; it is usually set between 0.2 and 0.5. At test time, no units are dropped out, and instead the layer's output values are scaled down by a factor equal to the dropout rate, so as to balance for the fact that more units are active than at training time.\n",
                "\n",
                "This technique may seem strange and arbitrary. Why would this help reduce overfitting? Geoff Hinton has said that he was inspired, among other things, by a fraud prevention mechanism used by banks -- in his own words: \"I went to my bank. The tellers kept changing and I asked one of them why. He said he didn\u2019t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting\".\n",
                "\n",
                "The core idea is that introducing noise in the output values of a layer can break up happenstance patterns that are not significant (what Hinton refers to as \"conspiracies\"), which the network would start memorizing if no noise was present.\n",
                "\n",
                "**TODO:** In PyTorch you can introduce dropout in a network via the Dropout layer, which gets applied to the output of layer right before it, e.g.:\n",
                "\n",
                "`nn.Dropout(0.5)`. Make a `BigNetDropout` class that has the same network structure as in `BigNet`, and contains dropouts after each `ConvBlock` and in the middle of the `mlp` sub-network. The class has to accept an argument `dropout` that controls the amount of `Dropout` of the aforementioned layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BigNetDropout(nn.Module):\n",
                "\n",
                "    def __init__(self, dropout: float) -> None:\n",
                "        super().__init__()\n",
                "        # TODO\n",
                "        ...\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        # TODO\n",
                "        ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "KaeWr5QyDLy-"
            },
            "source": [
                "**TODO:** Once the dropout model is defined, build an instance of it and train it, storing its results in `bigmodel_dout_log`. Use the dropout factor defined below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dropout = 0.5\n",
                "# TODO\n",
                "bignet = ...\n",
                "optimizer = ...\n",
                "bigmodel_dout_log = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.plot(bigmodel_log['te_losses'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_losses'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['te_losses'], label='bigmodel L2')\n",
                "plt.plot(bigmodel_dout_log['te_losses'], label='bigmodel Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Loss')\n",
                "plt.subplot(2,1,2)\n",
                "plt.plot(bigmodel_log['te_accs'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['te_accs'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['te_accs'], label='bigmodel L2')\n",
                "plt.plot(bigmodel_dout_log['te_accs'], label='bigmodel Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "_ = plt.ylabel('Eval Accs')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "BDJHYVTuDQ94"
            },
            "source": [
                "We effectively reduced a lot the overfitting, and we cannot even notice it appearing anymore at least for the first 12 epochs. It also seems to yield a better trend in the evaluation accuracy, that seems to have an uprising trend. Something interesting to see, though, is the performance of training losses and accuracy, too...\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.plot(bigmodel_log['tr_losses'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['tr_losses'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['tr_losses'], label='bigmodel L2')\n",
                "plt.plot(bigmodel_dout_log['tr_losses'], label='bigmodel Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Train Loss')\n",
                "plt.subplot(2,1,2)\n",
                "plt.plot(bigmodel_log['tr_accs'], label='bigmodel')\n",
                "plt.plot(smallmodel_log['tr_accs'], label='smallmodel')\n",
                "plt.plot(bigmodel_l2_log['tr_accs'], label='bigmodel L2')\n",
                "plt.plot(bigmodel_dout_log['tr_accs'], label='bigmodel Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "_ = plt.ylabel('Train Accs')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "h_oXlovuDm0_"
            },
            "source": [
                "WHOA! So it looks like the dropout model does really badly. In fact, there is a decreasing performance trend for both loss and accuracy for each ragularization method we introduced. Well, actually this is what we wanted to achieve: our model has to be as good in training as it is in validation. \n",
                "\n",
                "**Important Note:** Once we close the gap between training and validation, then we can tune our model to improve on both simultanously. We could for instance put more conv layers or change the striding factors to less aggressive ones (e.g. 2 instead of 4). We could also add residual connections (something to see in advanced deep material). We could be using batch normalization layers. And we could do plenty of other stuff. But importantly, we could also **POUR IN MORE DATA**."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-gQUajxE5gXW"
            },
            "source": [
                "## Exercise 3: Grand Finale\n",
                "\n",
                "**Train the Big Model with Dropout on the whole CIFAR10 Dataset**, and then check the overlayed evaluation plots for the BigModel, the BigModel with Dropout, and this new BigModel with Dropout on the Big Trainset. To make the exercise follow the steps:\n",
                "\n",
                "1. Defining the CIFAR dataset per se in training mode (look at the beginning of the notebook), but do NOT trim the `.data` and `.targets` lists.\n",
                "2. Make the `DataLoader` out of the dataset.\n",
                "3. Build the big network as shown before, with its optimizer, and call the training routine shown above, storing the results in the `bigmodel_dout_log` dictionary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Load the whole CIFAR10\n",
                "big_trainset = ...\n",
                "\n",
                "# TODO: Create a DataLoader\n",
                "big_loader = ...\n",
                "\n",
                "# TODO: Build the big network with dropout and train it\n",
                "bignet = ...\n",
                "optimizer = ...\n",
                "bigmodel_bigtrainset_dout_log = ..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "plt.subplot(2,1,1)\n",
                "plt.plot(bigmodel_log['te_losses'], label='bigmodel')\n",
                "plt.plot(bigmodel_dout_log['te_losses'], label='bigmodel Dropout')\n",
                "plt.plot(bigmodel_bigtrainset_dout_log['te_losses'], label='bigmodel bigtrainset Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Eval Loss')\n",
                "plt.subplot(2,1,2)\n",
                "plt.plot(bigmodel_log['te_accs'], label='bigmodel')\n",
                "plt.plot(bigmodel_dout_log['te_accs'], label='bigmodel Dropout')\n",
                "plt.plot(bigmodel_bigtrainset_dout_log['te_accs'], label='bigmodel bigtrainset Dropout')\n",
                "plt.legend()\n",
                "plt.xlabel('Epoch')\n",
                "_ = plt.ylabel('Eval Accs')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "04nhXJ9yHBZC",
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "To recap... here the most common ways to prevent overfitting in neural networks:\n",
                "\n",
                "* Reducing the capacity of the network.\n",
                "* Adding weight regularization.\n",
                "* Adding dropout.\n",
                "* Getting more training data.\n"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        },
        "widgets": {
            "application/vnd.jupyter.widget-state+json": {
                "0ccad68beec74c65a8b24302b46a2b13": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "FloatProgressModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "FloatProgressModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "ProgressView",
                        "bar_style": "success",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_69494392dbe845bda2a264dacf065156",
                        "max": 170498071,
                        "min": 0,
                        "orientation": "horizontal",
                        "style": "IPY_MODEL_f7eecc77469e4857965613e555cb9ab2",
                        "value": 170498071
                    }
                },
                "363ba4f0d0ff4cd1baf2457f8b612e31": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "4b3a46ca26be41878146c281d5dd8016": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "DescriptionStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "DescriptionStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "description_width": ""
                    }
                },
                "4fb7c7d796924f86bf8984101d3dab5d": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "58ab0a047e934e9f90a0913b7f81c746": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HBoxModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HBoxModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HBoxView",
                        "box_style": "",
                        "children": [
                            "IPY_MODEL_8df1f0e3137a4b4cacfb45f9bd0e860c",
                            "IPY_MODEL_0ccad68beec74c65a8b24302b46a2b13",
                            "IPY_MODEL_ce4b6945c2bf454b9d94d002f92396d4"
                        ],
                        "layout": "IPY_MODEL_ba9426a7b72042aba80a0920197a3ab8"
                    }
                },
                "5d915fa675cd405dbecd1d458bfac162": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "69494392dbe845bda2a264dacf065156": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "8df1f0e3137a4b4cacfb45f9bd0e860c": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_4fb7c7d796924f86bf8984101d3dab5d",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_4b3a46ca26be41878146c281d5dd8016",
                        "value": "100%"
                    }
                },
                "ba9426a7b72042aba80a0920197a3ab8": {
                    "model_module": "@jupyter-widgets/base",
                    "model_module_version": "1.2.0",
                    "model_name": "LayoutModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/base",
                        "_model_module_version": "1.2.0",
                        "_model_name": "LayoutModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "LayoutView",
                        "align_content": null,
                        "align_items": null,
                        "align_self": null,
                        "border": null,
                        "bottom": null,
                        "display": null,
                        "flex": null,
                        "flex_flow": null,
                        "grid_area": null,
                        "grid_auto_columns": null,
                        "grid_auto_flow": null,
                        "grid_auto_rows": null,
                        "grid_column": null,
                        "grid_gap": null,
                        "grid_row": null,
                        "grid_template_areas": null,
                        "grid_template_columns": null,
                        "grid_template_rows": null,
                        "height": null,
                        "justify_content": null,
                        "justify_items": null,
                        "left": null,
                        "margin": null,
                        "max_height": null,
                        "max_width": null,
                        "min_height": null,
                        "min_width": null,
                        "object_fit": null,
                        "object_position": null,
                        "order": null,
                        "overflow": null,
                        "overflow_x": null,
                        "overflow_y": null,
                        "padding": null,
                        "right": null,
                        "top": null,
                        "visibility": null,
                        "width": null
                    }
                },
                "ce4b6945c2bf454b9d94d002f92396d4": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "HTMLModel",
                    "state": {
                        "_dom_classes": [],
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "HTMLModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/controls",
                        "_view_module_version": "1.5.0",
                        "_view_name": "HTMLView",
                        "description": "",
                        "description_tooltip": null,
                        "layout": "IPY_MODEL_5d915fa675cd405dbecd1d458bfac162",
                        "placeholder": "\u200b",
                        "style": "IPY_MODEL_363ba4f0d0ff4cd1baf2457f8b612e31",
                        "value": " 170498071/170498071 [00:01&lt;00:00, 95616544.65it/s]"
                    }
                },
                "f7eecc77469e4857965613e555cb9ab2": {
                    "model_module": "@jupyter-widgets/controls",
                    "model_module_version": "1.5.0",
                    "model_name": "ProgressStyleModel",
                    "state": {
                        "_model_module": "@jupyter-widgets/controls",
                        "_model_module_version": "1.5.0",
                        "_model_name": "ProgressStyleModel",
                        "_view_count": null,
                        "_view_module": "@jupyter-widgets/base",
                        "_view_module_version": "1.2.0",
                        "_view_name": "StyleView",
                        "bar_color": null,
                        "description_width": ""
                    }
                }
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
