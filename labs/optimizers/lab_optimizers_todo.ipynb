{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DBgw8H_z9lfE"
            },
            "source": [
                "# Optimizers\n",
                "\n",
                "Notebook created in PyTorch by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) (2020).\n\n",
                "Updated by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/).\n",
                "\n",
                "In this lab we will do a deep dive on how to use an optimizer to minimize an arbitrary function, as well as best practices on how to use the optimizers for Deep Learning."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.optim as optim\n",
                "import plotly.graph_objects as go\n",
                "import time\n",
                "import numpy as np"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AJ-uZLL6bfx4"
            },
            "source": [
                "We can use the following functions to view an animation of how the optimizer finds the minimum of a function. These functions use [plotly](https://plotly.com/) to do the plots, which is a great choice thanks to its options to create animations."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def animate_2d_optimization(f, x_range, points):\n",
                "\n",
                "    x = torch.linspace(*x_range, steps=1000)\n",
                "    function_graph = go.Scatter(x=x, y=f(x), mode='lines', name=\"Function\")\n",
                "\n",
                "    def frame_data(p):\n",
                "        return [function_graph, \n",
                "                go.Scatter(x=[p], y=[f(torch.tensor([float(p)])).item()], mode='markers', marker=dict(size=[15]), name=\"Point\")]\n",
                "    \n",
                "    frames = [go.Frame(data=frame_data(p)) for p in points]\n",
                "\n",
                "    fig = go.Figure(\n",
                "        data=frame_data(points[0]),\n",
                "        layout=go.Layout(\n",
                "            title=\"Start optimization\",\n",
                "            updatemenus=[dict(\n",
                "                type=\"buttons\",\n",
                "                buttons=[dict(label=\"Play\",\n",
                "                            method=\"animate\",\n",
                "                            args=[None])])],\n",
                "            showlegend=False\n",
                "        ),\n",
                "        frames=frames,\n",
                "    )\n",
                "    return fig\n",
                "\n",
                "\n",
                "def animate_3d_optimization(f, x_range, y_range, points):\n",
                "\n",
                "    x = torch.linspace(*x_range, steps=100)\n",
                "    y = torch.linspace(*x_range, steps=100)\n",
                "    x, y = torch.meshgrid([x, y])\n",
                "    function_surface = go.Surface(x=x, y=y, z=f(x, y), name=\"Function\")\n",
                "\n",
                "    def frame_data(p):\n",
                "        return [function_surface, \n",
                "                go.Scatter3d(x=[p[0]], y=[p[1]], z=[0.05+f(torch.tensor([p[0]], dtype=float), torch.tensor([p[1]], dtype=float)).item()], mode='markers', marker=dict(size=[15], color=\"white\"), name=\"Point\")]\n",
                "    \n",
                "    frames = [go.Frame(data=frame_data(p)) for p in points]\n",
                "\n",
                "    fig = go.Figure(\n",
                "        data=frame_data(points[0]),\n",
                "        layout=go.Layout(\n",
                "            title=\"Start optimization\",\n",
                "            showlegend=False,\n",
                "            updatemenus=[dict(\n",
                "                            type=\"buttons\",\n",
                "                            buttons=[dict(label=\"Play\",\n",
                "                                        method=\"animate\",\n",
                "                                        args=[None])\n",
                "                            ])]\n",
                "        ),\n",
                "        frames=frames,\n",
                "    )\n",
                "    return fig"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ntbFMhZqIag7"
            },
            "source": [
                "## Optimization of single-input functions\n",
                "Let's start by finding the optimum of a very simple single-parameter function."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def f(x):\n",
                "    return torch.tanh(x-2.5)**2 + 0.3*torch.tanh(x)**2"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "siXMc6T9IwZL"
            },
            "source": [
                "### Exercise 1\n",
                "\n",
                "Many people say that the learning rate is the most important hyperparameter when doing Deep Learning. We will see now the important of a good choice of LR.\n",
                "\n",
                "Complete the `optimize_1d_function` and call it with an `init_value` of 4.5. Use SGD as an optimizer, and optimize the tensor `v`."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def optimize_1d_function(f, init_value, lr=0.1, steps=60):\n",
                "    points = []\n",
                "    v = torch.tensor([float(init_value)], requires_grad=True)\n",
                "\n",
                "    # TODO: Use SGD as an optimizer\n",
                "    optimizer = ...\n",
                "\n",
                "    for step in range(steps):\n",
                "        points.append(v.item())\n",
                "\n",
                "        # TODO\n",
                "        optimizer...\n",
                "\n",
                "        loss = f(v)\n",
                "\n",
                "        # TODO\n",
                "        loss...\n",
                "\n",
                "        # TODO\n",
                "        optimizer...\n",
                "\n",
                "    return animate_2d_optimization(f, x_range=[-10, 10], points=points)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_1d_function(f, 4.5)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "IeYnc77HXyi0"
            },
            "source": [
                "Is the optimizer learning fast enough? We have to tune the hyperparameters. Let's try using a LR of 10."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_1d_function(f, 4.5, lr=10)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "s61JGqMjX1gY"
            },
            "source": [
                "Clearly, this LR was too large. Let's try now a LR of 1."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_1d_function(f, 4.5, lr=1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q1mzL8jrLco8"
            },
            "source": [
                "Great! This was the correct choice. Note that we tried 1e-1, 1e1 and 1e2, and only 1e1 worked. This is a toy example, but this exact fenomenon happens when choosing a LR to optimize our Deep Learning model. Luckily, we have more advanced optimizers that make this choice easier! \n",
                "\n",
                "**Extra:** Could we have used `init_value` = 0? Why?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "AgzRkDkCVVlJ"
            },
            "source": [
                "### Exercise 2\n",
                "\n",
                "Complete the function again, but now use Adam instead of SGD. Then, we will try it with the same `init_value` (4.5) and a lr=0.1 and lr=1."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def optimize_1d_function_adam(f, init_value, lr=0.1, steps=60):\n",
                "    points = []\n",
                "    v = torch.tensor([float(init_value)], requires_grad=True)\n",
                "\n",
                "    # TODO: Use Adam instead of SGD\n",
                "    optimizer = ...\n",
                "\n",
                "    for step in range(steps):\n",
                "        optimizer.zero_grad()\n",
                "        points.append(v.item())\n",
                "        loss = f(v)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "    return animate_2d_optimization(f, x_range=[-10, 10], points=points)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_1d_function_adam(f, 4.5, lr=0.1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_1d_function_adam(f, 4.5, lr=1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Vp3l5aK3avnC"
            },
            "source": [
                "We can see that both learning rates worked! But neither of them worked as well as SGD with lr=1. This is the advantage of ADAM, the choice of the learning rate is much more lenient. The exact same thing passes when training neural networks."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0tnInV5FMhLw"
            },
            "source": [
                "**Key takeaways**\n",
                "\n",
                "* SGD works great, but requires tunning the lr much more. We had to use specifically 1e1, otherwise the optimizer didn't find the optimum. This makes SGD the correct choice when we are willing to try many different learning rates and want the most optimal neural network possible.\n",
                "\n",
                "* Adam is much more lenient than SGD. Any value between 1e-1 and 1e1 works, and gets to an optimum almost as good as SGD. This makes Adam the default choice for most people, since it performs almost as well as SGD but doesn't require nearly as much tunning."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NrC78Td6M74A"
            },
            "source": [
                "## Optimization of multi-input functions\n",
                "\n",
                "Now, we will work on a more realistic case. The function we will try to optimize will have 2 parameters. The main difference between 1d and 2d is that we now can have saddle points, which are points that are a maximum in a direction and a minimum in another direction."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def f(x, y):\n",
                "    return 1e-1*((x)**2 - (y)**2)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def optimize_2d_function(f, init_value, lr=0.1, steps=20):\n",
                "    points = []\n",
                "    v = torch.tensor(init_value, requires_grad=True)\n",
                "    optimizer = optim.SGD([v], lr)\n",
                "\n",
                "    for step in range(steps):\n",
                "        points.append(v.detach().numpy().copy().tolist())\n",
                "        optimizer.zero_grad()\n",
                "        loss = f(v[0], v[1])\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "    return animate_3d_optimization(f, x_range=[-10, 10], y_range=[-10, 10], points=points)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "wLD8qODPNb-8"
            },
            "source": [
                "### Exercise 3\n",
                "\n",
                "Search in a logarithmic scale (0.1, 1, 10, 100...) the minimum value for the learning rate necessary to escape from the saddle point, when starting at [-5, 0.01]."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_2d_function(f, [-5, 0.01], lr=0.1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DU3fFCliaZDN"
            },
            "source": [
                "Did we have to use a large learning rate? Using a value that's too high can hinder the training process. Is there another way to escape from saddle points?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "tZkPDZwVPKlS"
            },
            "source": [
                "### Exercise 4\n",
                "\n",
                "Now use SGD with 0.9 momentum. We will try a learning rate of 1 with the same starting point.\n",
                "\n",
                "**Extra:** Do the same but with Nesterov accelerated momentum."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def optimize_2d_function_momentum(f, init_value, lr=0.1, steps=20):\n",
                "    points = []\n",
                "    v = torch.tensor(init_value, requires_grad=True)\n",
                "\n",
                "    #\u00a0TODO: Use SGD with 0.9 momentum\n",
                "    optimizer = ...\n",
                "\n",
                "    for step in range(steps):\n",
                "        points.append(v.detach().numpy().copy().tolist())\n",
                "        optimizer.zero_grad()\n",
                "        loss = f(v[0], v[1])\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "    return animate_3d_optimization(f, x_range=[-10, 10], y_range=[-10, 10], points=points)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "optimize_2d_function_momentum(f, [-5, -0.01], lr=1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3IDUVygScFyh"
            },
            "source": [
                "By adding momentum, we were able to escape the saddle point with a lower learning rate. That's great!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PMRJ2xvXQhfS"
            },
            "source": [
                "**Key takeaway:** When using SGD, we almost always want to add momentum. It will generally accelerate learning without adding the problems of using a high learning rate. The only negative side is that this adds a new hyperparamter to tune (0.9 is a good reference value for momentum)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OenRX_zNRIQH"
            },
            "source": [
                "## How to find a good initial learning rate? The LR Range Test\n",
                "\n",
                "In 2015, Leslie N. Smith more or less formalized the above trial-and-error into a technique called the LR Range Test. The idea is simple, you just run your model and data for a few iterations, with the learning rate initially start at a very small value and then increase after each iteration. You record the loss for each value of learning rate and plot it up.\n",
                "![lr-test](https://miro.medium.com/max/1280/1*U0Y0HWHhFZu9mHyGf3OoRw.png)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Uxf9wJscZ9JE"
            },
            "source": [
                "We'll implement and try this method to find a good learning rate for training CIFAR10 with a resnet34 and Adam."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "import torchvision.datasets as datasets\n",
                "import torchvision.transforms as transforms\n",
                "import torchvision.models as models\n",
                "device = torch.device(\"cuda\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transform = transforms.Compose([\n",
                "                                transforms.ToTensor(),\n",
                "                                transforms.Normalize(0.5, 0.5)\n",
                "])\n",
                "dataset = datasets.CIFAR10(root=\"data\", download=True, transform=transform)\n",
                "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = models.resnet34(num_classes=10).to(device)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HAD3vOlMZqUx"
            },
            "source": [
                "We will search for the learning rate between 1e-9 and 1e2, using np.logspace."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "lr_range = np.logspace(-9, 2, num=200)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "DxWUFAehUCc_"
            },
            "source": [
                "### Exercise 5\n",
                "\n",
                "Complete the code to do the test. You can modify the learning rate by modifying `optimizer.param_groups[0][\"lr\"] = ...`"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "loss_history = []\n",
                "\n",
                "for lr, (images, targets) in zip(lr_range, dataloader):\n",
                "    # TODO: Set the learning rate to lr\n",
                "    optimizer...\n",
                "\n",
                "    # TODO \n",
                "    optimizer...\n",
                "\n",
                "    images, targets = images.to(device), targets.to(device)\n",
                "\n",
                "    # TODO\n",
                "    output = ...\n",
                "\n",
                "    # TODO\n",
                "    loss = ...\n",
                "\n",
                "    # TODO\n",
                "    loss...\n",
                "\n",
                "    # TODO\n",
                "    optimizer...\n",
                "\n",
                "    loss_history.append(loss.item())"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "UJ4sj9uwZf4M"
            },
            "source": [
                "Now we can check what is the optimal Learning Rate in the plot. Even though it's noisy, we can see a good value for the learning rate. Can you spot it?"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.plot(lr_range, loss_history)\n",
                "plt.ylim([2, 3])\n",
                "plt.xscale(\"log\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "3fmqzwkvXN7n"
            },
            "source": [
                "## I've found a good initial learning rate. Now what? LR schedulers\n",
                "\n",
                "A general good practice is to use a learning rate scheduler. One of the most useful ones is `ReduceLROnPlateau`. This scheduler decay the learning rate every time the loss function gets stuck. The use of an optimizer like this one can help squeeze a bit more performance of your model. We will see how to use it in PyTorch. We will train the same model as before, using a lr of 1e-3 (the largest acceptable value we found using the lr test)."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "transform = transforms.Compose([\n",
                "                                transforms.ToTensor(),\n",
                "                                transforms.Normalize(0.5, 0.5)\n",
                "])\n",
                "dataset = datasets.CIFAR10(root=\"data\", download=True, transform=transform)\n",
                "loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kfiHfoafUMmd"
            },
            "source": [
                "First, let's get a baseline without an scheduler:"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = models.resnet34(num_classes=10).to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "epochs = 2\n",
                "for epoch in range(epochs):\n",
                "    losses = []\n",
                "    for i, (images, targets) in enumerate(loader):\n",
                "        optimizer.zero_grad()\n",
                "        images, targets = images.to(device), targets.to(device)\n",
                "        output = model(images)\n",
                "        loss = criterion(output, targets)\n",
                "        loss.backward()\n",
                "        losses.append(loss.item())\n",
                "        optimizer.step()\n",
                "        loss_history.append(loss.item())\n",
                "        if i%50 == 0:\n",
                "            print(f\"Epoch {epoch} [{i}/{len(loader)}]: loss: {np.mean(losses):.2f}, lr={optimizer.param_groups[0]['lr']}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2fSLqrI1UjkI"
            },
            "source": [
                "### Exercise 6\n",
                "Now we will use a scheduler. First, we have to declare it from the `torch.optim` module. They work very similar to optimizers."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "model = models.resnet34(num_classes=10).to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=150, factor=0.5)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "XLTsrs_EVDLs"
            },
            "source": [
                "Complete the training code with a scheduler. You should add the scheduler step after the optimizer step."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "epochs = 2\n",
                "for epoch in range(epochs):\n",
                "    losses = []\n",
                "    for i, (images, targets) in enumerate(loader):\n",
                "        optimizer.zero_grad()\n",
                "        images, targets = images.to(device), targets.to(device)\n",
                "        output = model(images)\n",
                "        loss = criterion(output, targets)\n",
                "        loss.backward()\n",
                "        losses.append(loss.item())\n",
                "        optimizer.step()\n",
                "        # TODO\n",
                "        scheduler...\n",
                "\n",
                "        loss_history.append(loss.item())\n",
                "        if i%50 == 0:\n",
                "            print(f\"Epoch {epoch} [{i}/{len(loader)}]: loss: {np.mean(losses):.2f}, lr={optimizer.param_groups[0]['lr']}\")"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
