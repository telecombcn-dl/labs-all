{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NubFYOc3B0BW"
            },
            "source": [
                "# Linear and Logistic Regression with PyTorch\n",
                "\n",
                "Notebook created in PyTorch by [Santi Pascual](https://github.com/santi-pdp) for the [UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) (2019) and updated by [Xavier Giro](https://imatge.upc.edu/web/people/xavier-giro) for [UPC TelecomBCN](https://telecombcn-dl.github.io/dlai-2019/) (2019).\n",
                "\n",
                "The linear regression part is based on a previous version for the [Barcelona Technology School](https://barcelonatechnologyschool.com/master/master-in-big-data-solutions/) (BTS) created by [Victor Campos](https://scholar.google.com/citations?user=8fzVqSkAAAAJ&hl=en) (2018) and extended by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) (2019)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R4SE1Ghbqs4U"
            },
            "source": [
                "This lab is about linear and logistic regressions with PyTorch. These two components are fundamental in every machine learning toolbox. Linear regression allows us to predict real valued future trends. Logistic regression, on the other hand, allows us to tell classes apart by learning borders in the feature space that separate point clouds.\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We first import PyTorch and Numpy libraries as fundamental tools to work with arrays and tensors\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "# inline ensures we will automatically see the plots as soon as we operate with plot() calls\n",
                "%matplotlib inline"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ag6qd9toCn7T"
            },
            "source": [
                "\n",
                "## Data loading\n",
                "\n",
                "We will first build a toy dataset. This will be based on the line $y = 5x + 3$, with additive noise $N(0, 3)$ that will add some distorting values. So out of this we know so far:\n",
                "1. We have a line out of which we will get outcomes $y[x]$\n",
                "2. These outcomes will be distorted with some noise that will blur the line, but the line is still the underlying generation process.\n",
                "\n",
                "We will then use a linear model $\\hat{y} = w\\cdot x + b$ to learn the parameters $w$ and $b$ that lead to the predicted responses that have to resemble those of the real data. We will then be able to predict new values for arbitrary $x$ values as long as our regressor is trained properly to approximate the real values of the underlying line $w=5$, $b=3$."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We generate NUM_SAMPLES data points between [0,X_SPAN] \n",
                "# by sampling from a uniform distribution\n",
                "NUM_SAMPLES = 200\n",
                "X_SPAN = 10\n",
                "train_X = np.random.rand(NUM_SAMPLES) * X_SPAN\n",
                "# generate noise\n",
                "noise = np.random.randn(NUM_SAMPLES) * 3\n",
                "train_Y = 5*train_X + 3 + noise\n",
                "n_samples = train_X.shape[0]\n",
                "\n",
                "plt.scatter(train_X, train_Y)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We also build a testset to try new predictions with our model once trained\n",
                "test_X = np.random.rand(NUM_SAMPLES) * X_SPAN\n",
                "test_Y = 5*test_X + 3 + noise"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "69kQmjPqE1jp"
            },
            "source": [
                "## Building the Linear Regressor\n",
                "\n",
                "The model we are pursuing behaves as a learnable line and has only 2 parameters. It is elegant to enclose everything within the `nn.Module` class of PyTorch. This special class only requires us to define the following:\n",
                "\n",
                "1. The parameters of our model: regardless of whether it is a complicated neural layer, a simple scalar parameter, or a pizza (whutt?), everything is declared in the `__init__` of our class (normally).\n",
                "2. The forward function of our model: we declare a signature for the `def forward(self, x)` function, or the `def forward(self, x, conditioning)` function, or the `def forward(self, x, y, z, t, w, o, z2, superdupervariable)`, it is up to us what parameters we pass in. The only matter is to have the skeleton defined in the next cell."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# first, we import the neural net module\n",
                "import torch.nn as nn\n",
                "\n",
                "# Now we define the linear regression class\n",
                "class LinearRegression(nn.Module):\n",
                "  \n",
                "  def __init__(self):\n",
                "    # ALWAYS CALL THE SUPERCLASS INIT OR PYTORCH WILL COMPLAIN\n",
                "    super().__init__()\n",
                "    # we define two parameters, w and b (randomly initialized)\n",
                "    self.w = nn.Parameter(torch.randn(1))\n",
                "    self.b = nn.Parameter(torch.randn(1))\n",
                "    \n",
                "  def forward(self, x):\n",
                "    # we build the forward pass by writing the line equation\n",
                "    y = self.w * x + self.b\n",
                "    return y"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Predict the points of our erratic model for the test set \n",
                "# WATCH OUT FOR THE TORCH CONVERSION =D, You learned something useful in the\n",
                "# previous lab about tensors!\n",
                "lreg = LinearRegression()\n",
                "test_X = torch.FloatTensor(test_X)\n",
                "y_ = lreg(test_X)\n",
                "\n",
                "# Check the parameters of our model\n",
                "print('w initial value: ', lreg.w.item())\n",
                "print('b initial value: ', lreg.b.item())\n",
                "\n",
                "# Plot real data on a new figure (use the one from the training set we built)\n",
                "# together with the predicted output for the test data\n",
                "plt.scatter(train_X, train_Y)\n",
                "plt.scatter(test_X.data.numpy(), y_.data.numpy())\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "USeZWPbzG9Og"
            },
            "source": [
                "I bet the line above looks a bit out of fit with respect to the data as the parameters $w$ and $b$ in our model are initialied randomly. If it fits the training points only after random init, perhaps trying lottery for today might be a cool idea, luck might be on your side."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "N6fw6OR9IYH2"
            },
            "source": [
                "### Exercise 1\n",
                "\n",
                "Can you tell what distribution do $w$ and $b$ follow in our randomly initialized model?\n",
                "\n",
                "Hint: understand the code in the defined `class`, the answer is there!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "929-hpc6TaYB"
            },
            "source": [
                "### Now let's iterate step by step to update the parameters of this line with BACKPROPAGATION and STOCHASTIC GRADIENT DESCENT!\n",
                "\n",
                "<figure>\n",
                "\n",
                "<img src='https://pbs.twimg.com/media/D4cxqsNWkAAB9he?format=png&name=900x900' width=250/>\n",
                "<figcaption>Gradient descent updates the model by optimizing the parameters on pieces of data.</figcaption>\n",
                "</figure>"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from random import shuffle\n",
                "# Shuffle datasets\n",
                "def shuffle_dataset(X, Y):\n",
                "  joint = torch.cat((X, Y), dim=1)\n",
                "  joint = list(torch.chunk(joint, len(joint), dim=0))\n",
                "  shuffle(joint)\n",
                "  joint = torch.cat(joint, dim=0)\n",
                "  return torch.chunk(joint, 2, dim=1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CLUmNI9rzbad"
            },
            "source": [
                "### Exercise 2\n",
                "\n",
                "Finish the following code to succesfully train the linear regressor. To do that, consider that you have to do the forward pass with the training batch, compute the cost with the loss function, do the backpropagation and update your model's parameters.\n",
                "\n",
                "In order to solve this exercise, you may want to review how a neural network was trained in the Backpropagation lab."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "\n",
                "lreg = LinearRegression()\n",
                "\n",
                "# Fit all training data\n",
                "X = torch.FloatTensor(train_X).view(-1, 1)\n",
                "Y = torch.FloatTensor(train_Y).view(-1, 1)\n",
                "NUM_EPOCHS = 10\n",
                "BATCH_SIZE=5\n",
                "LR = 1e-2\n",
                "\n",
                "# define MSE as the cost function\n",
                "cost = F.mse_loss\n",
                "\n",
                "opt = optim.SGD(lreg.parameters(), lr=LR)\n",
                "avg_loss = None\n",
                "avg_weight = 0.1\n",
                "losses = []\n",
                "for epoch in range(1, NUM_EPOCHS + 1):\n",
                "    for (beg_i, end_i) in zip(range(0, len(X) - BATCH_SIZE + 1, BATCH_SIZE),\n",
                "                              range(BATCH_SIZE, len(X), BATCH_SIZE)):\n",
                "      x = X[beg_i:end_i]\n",
                "      y = Y[beg_i:end_i]\n",
                "      \n",
                "      # TODO: finish the training steps here\n",
                "      y_ = ...\n",
                "      loss = ...\n",
                "      loss...\n",
                "      opt...\n",
                "      opt...\n",
                "\n",
                "      # Smooth the loss value that is saved to be plotted later\n",
                "      if avg_loss:\n",
                "        avg_loss = avg_weight * loss.item() + (1 - avg_weight) * avg_loss\n",
                "      else:\n",
                "        avg_loss = loss.item()\n",
                "      losses.append(avg_loss)\n",
                "\n",
                "    # Shuffle the data in the training batch to regularize training  \n",
                "    X, Y = shuffle_dataset(X, Y)\n",
                "    \n",
                "plt.ylabel('Smoothed loss by factor {:.2f}'.format(1 - avg_weight))\n",
                "plt.xlabel('Iteration step')\n",
                "plt.plot(losses)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1tXPioliyBFM"
            },
            "source": [
                "Now that the linear regressor is trained, we can plot again the predicted values compared to those from the training set.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "test_X = torch.FloatTensor(test_X)\n",
                "y_ = lreg(test_X)\n",
                "plt.scatter(train_X, train_Y)\n",
                "plt.scatter(test_X.data.numpy(), y_.data.numpy())\n",
                "print('w trained value: ', lreg.w.item())\n",
                "print('b trained value: ', lreg.b.item())"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "g5CPjZr6kjB3"
            },
            "source": [
                "### Exercise 3:\n",
                "\n",
                "Write the code to predict the values at $x=[0.5, 5, 8.75]$ and plot them overlayed with the previous plot."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# TODO: build the code to predict the y values corresponding to the above vector\n",
                "# now that lreg is trained and plot them overlayed on the plot with scale 200 in \n",
                "# scatter plot (check the doc on scatter function https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)\n",
                "x = ...\n",
                "y = ...\n",
                "\n",
                "print(y.data.numpy())\n",
                "\n",
                "plt.scatter(train_X, train_Y)\n",
                "plt.scatter(test_X.data.numpy(), y_.data.numpy())\n",
                "# This is the scatter you have to do with your 3 (x, y) points\n",
                "plt.scatter(x.data.numpy(), y.data.numpy(), s=200)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "q6wO4G8X0L2N"
            },
            "source": [
                "### Grand Finale, the Logistic regression\n",
                "\n",
                "The **Logistic regression** follows the same principle than linear regression as a linear model. The difference is that we use it to **establish a boundary between two classes**. Hence, it can be used to determine if for an input, we have either one outcome or the other one, so used in binary classification problems. \n",
                "\n",
                "Down bellow we generate a toy dataset with two bi-dimensional Gaussian distributions. Each one will belong to a different class (0 or 1). You have to build a Logistic regressor, following the scheme above but using the proper output activation function (it's non-linear, contrary to the `LinearRegressor`) and the proper loss function (NOPE, MSELoss IS NOT VALID). In this classification problem with two classes, we will use the binary cross-entropy loss (introduced in [these slides](https://github.com/telecombcn-dl/dlai-2019/raw/master/slides/dlai_2019_d04l1_losses.pdf))."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We first generate some training data points\n",
                "NUM_SAMPLES = 1000\n",
                "\n",
                "class_0 = np.random.randn(NUM_SAMPLES, 2)\n",
                "class_1 = np.random.randn(NUM_SAMPLES,2 ) + 2.5\n",
                "\n",
                "train_X = np.concatenate((class_0, class_1), axis=0)\n",
                "train_Y = np.concatenate((np.zeros((NUM_SAMPLES,)), np.ones((NUM_SAMPLES,))), axis=0)\n",
                "\n",
                "\n",
                "_ = plt.scatter(class_0[:, 0], class_0[:, 1], alpha=0.15)\n",
                "_ = plt.scatter(class_1[:, 0], class_1[:, 1], alpha=0.15)\n",
                "_ = plt.scatter([0], [0], s=200, color='blue')\n",
                "_ = plt.scatter([2.5], [2.5], s=200, color='red')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0fM54OHgo4gS"
            },
            "source": [
                "In the image above, two clouds of points are shown. The blue one corresponds to the zero-class points, whereas the orange ones belong to the one-class. The big blue dot and the big red dot are their respective centroids."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JPBMIW7XBZsq"
            },
            "source": [
                "**Exercise 4: Define the LogisticRegression Module below. We have 2-D data points now, so use the `nn.Linear` layer to do the linear operation (https://pytorch.org/docs/stable/nn.html#torch.nn.Linear).**"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "class LogisticRegression(nn.Module):\n",
                "  \n",
                "  def __init__(self):\n",
                "    super().__init__()\n",
                "    # NOTE: BEWARE WITH THE NUMBER OF FEATURES IN THE INPUT!\n",
                "    \n",
                "    # TODO: Linear projection\n",
                "    # https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
                "    self.proj = nn...\n",
                "\n",
                "    # TODO: Sigmoid activation\n",
                "    self.act = nn...\n",
                "    \n",
                "  def forward(self, x):\n",
                "\n",
                "    # TODO:Combine the linear layer with the sigmoid activation    \n",
                "    y = ...\n",
                "    \n",
                "    return y"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "RbakbhseBb1f"
            },
            "source": [
                "**Exercise 4.2: Complete the training code based on the code for the linear regression. Use the following parameters to have a quick and effective convergence:**\n",
                "\n",
                "* **NUM_EPOCHS=200**\n",
                "* **BATCH_SIZE=15**\n",
                "* **LR=1e-1 (with SGD)**\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "\n",
                "# Create an instance of a logistic regressor\n",
                "loreg = LogisticRegression()\n",
                "\n",
                "# Fit all training data\n",
                "X = torch.FloatTensor(train_X).view(-1, 2)\n",
                "Y = torch.FloatTensor(train_Y).view(-1, 1)\n",
                "NUM_EPOCHS = 100\n",
                "BATCH_SIZE=15\n",
                "LR = 1e-1\n",
                "\n",
                "# define binary cross entropy as the cost function\n",
                "cost = F.binary_cross_entropy\n",
                "\n",
                "# TODO: Complete the rest of the training code to perform the logistic regression\n",
                "# You can use as a reference the one previously used for linear regression\n",
                "# WARNING: If you copy & paste, update the reference of the optimizer to the\n",
                "# logistic regressor (loreg) instead of the linear one (lreg).\n",
                "...\n",
                "    \n",
                "# Plot the trainig curve of the loss function    \n",
                "plt.ylabel('Smoothed loss by factor {:.2f}'.format(1 - avg_weight))\n",
                "plt.xlabel('Iteration step')\n",
                "plt.plot(losses)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_AzgpbQMqA31"
            },
            "source": [
                "In the following, the `make_logistic_surface` function is provided. We will pass in a random logistic regression model and your trained logistic regression model. If all works fine, you should see a good fit for your logistic probability surface over the 2 centroid classes: zero-class should have probability close to zero (floor), and one-class should have probability close to one (elevated).\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "from matplotlib import cm\n",
                "%matplotlib inline\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "\n",
                "\n",
                "def make_logistic_surface(logistic):\n",
                "  fig = plt.figure()\n",
                "  ax = Axes3D(fig)\n",
                "  ax.set_title('Logistic regression probability surface. Blue dot: zero-class centroid. Orange dot: one-class centroid.')\n",
                "  X = []\n",
                "  y_coords = np.linspace(-4, 6, 100)\n",
                "  x_coords = np.linspace(-4, 6, 100)\n",
                "  for n in y_coords:\n",
                "    for m in x_coords:\n",
                "      X.append([n, m])\n",
                "  X = torch.FloatTensor(X)\n",
                "  Y_ = logistic(X)\n",
                "  Y_ = Y_.data.numpy()\n",
                "  sidx = 0\n",
                "  surface = np.zeros((100, 100))\n",
                "  xc, yc = np.meshgrid(x_coords, y_coords)\n",
                "  for n in range(100):\n",
                "    for m in range(100):\n",
                "      surface[n, m] = Y_[sidx]\n",
                "      sidx += 1\n",
                "\n",
                "  surf = ax.plot_surface(xc, yc, surface, cmap=cm.coolwarm,\n",
                "                         linewidth=0, antialiased=True,\n",
                "                         alpha=0.5)\n",
                "  _ = ax.scatter([0], [0], [1], s=200)\n",
                "  _ = ax.plot([0, 1e-3], [0, 1e-3], [0, 1], linewidth=3, color='blue')\n",
                "  _ = ax.scatter([2.5], [2.5], [1], s=200)\n",
                "  _ = ax.plot([2.5, 2.5 + 1e-3], [2.5, 2.5 + 1e-3], [0, 1], linewidth=3, color='red')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fyZrb0wXqcRw"
            },
            "source": [
                "**Random logistic regression**\n",
                "\n",
                "We use a random seed that we know can give a very bad initialization completely giving zeros to class 1, and vice-versa with class 0.\n",
                "\n",
                "**Advice:** play a bit with the seed value and re-run the function call to see how the surface moves randomly"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "_ = torch.manual_seed(5)\n",
                "loreg_nt = LogisticRegression()\n",
                "make_logistic_surface(loreg_nt)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hC5l7C45qo2i"
            },
            "source": [
                "**Trained logistic regression**\n",
                "\n",
                "Now we plot the resulting probability weights we learned."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "make_logistic_surface(loreg)"
            ],
            "execution_count": null,
            "outputs": []
        }
    ]
}
