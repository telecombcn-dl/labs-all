{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MbRVQ73oRVAO"
            },
            "source": [
                "# DCGAN + conditional DCGAN\n",
                "\n",
                "\n",
                "Notebook created by [Daniel Fojo](https://www.linkedin.com/in/daniel-fojo/) for the [Postgraduate course in artificial intelligence with deep learning](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) in [UPC School](https://www.talent.upc.edu/ing/) (2020). Updated by\u00a0[Mariona Car\u00f3s](https://es.linkedin.com/in/mariona-c-a7bb91105) in 2022.\n",
                "Minor contributions by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) and [Juan Jos\u00e9 Nieto](https://www.linkedin.com/in/juan-jose-nieto-salas/) during 2022."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VTdegfQgzqSZ"
            },
            "source": [
                "In this notebook you will learn about Generative Adversarial Networks by implementing a DCGAN to generate images from noise.\n",
                "\n",
                "**Important:** Set the Colab environment to run on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn, optim\n",
                "from torchvision import transforms, datasets, utils\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import math\n",
                "from IPython.display import display\n",
                "from tqdm import tqdm\n",
                "from itertools import cycle\n",
                "from typing import Tuple"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "seed = 123\n",
                "np.random.seed(seed)\n",
                "_ = torch.manual_seed(seed)\n",
                "_ = torch.cuda.manual_seed(seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "S_wpixTJ9fde"
            },
            "source": [
                "**Defining the Hyper-parameters**\n",
                "\n",
                "We now define the hyperparameters that are going to be used throughout the notebook to define the network.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "hparams = {\n",
                "    'batch_size':128,\n",
                "    'num_epochs':20,\n",
                "    'learning_rate':0.0002,\n",
                "    'betas':(0.5, 0.999),\n",
                "    'noise_size':100,\n",
                "    'num_val_samples':25,\n",
                "    'num_classes':10,\n",
                "    'num_input_channels':1,\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "P1AaRIWR9L5m"
            },
            "source": [
                "### Dataset and Data Loader\n",
                "Download and prepare dataset and datalaoder\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_transforms = transforms.Compose([   \n",
                "    transforms.Resize(32),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize((0.5,), (0.5,))\n",
                "])\n",
                "\n",
                "dataset = datasets.MNIST(\n",
                "    root='data',\n",
                "    train=True,\n",
                "    download=True,\n",
                "    transform=train_transforms,\n",
                ")\n",
                "\n",
                "dataloader = torch.utils.data.DataLoader(\n",
                "    dataset, \n",
                "    batch_size=hparams['batch_size'], \n",
                "    shuffle=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "--drGQMaGeop"
            },
            "source": [
                "# DCGAN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MFEBhsZb96T7"
            },
            "source": [
                "First, lets define our simple generator network."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dD8ms_VzNmWn"
            },
            "source": [
                "### Exercise 1: Generator\n",
                "\n",
                "The generator takes random noise as input and gives an image as output. Your exercise is to create the generator model.\n",
                "\n",
                "It should follow these guidelines:\n",
                "* The input will be a vector with random noise of size `noise_size`\n",
                "* You should first apply a fully connected with output size 512\\*4\\*4 (channels\\*height\\*width)\n",
                "* Then you should apply 3 blocks of:\n",
                "    * TransposedConvolution with kernel size 4, stride 2 and padding 1. For the first 2 blocks, the output channels should be 256 and 128. For the third block, the output channels should be the correct one to generate images of the dataset.\n",
                "    * BatchNorm2d except for the last block.\n",
                "    * ReLU activation for the first 2 blocks and Tanh for the third block.\n",
                "\n",
                "**Hint**: Remember to use reshape where necessary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Generator(torch.nn.Module):\n",
                "    \n",
                "    def __init__(self, noise_size: int, num_input_channels: int):\n",
                "        super().__init__()\n",
                "      \n",
                "        # TODO: Create the Fully connected layer using nn.Linear\n",
                "        self.fc = ...\n",
                "\n",
                "        # TODO: Create the first block\n",
                "        self.convt1 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(..., bias=False), # (B, 256, 8, 8)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.ReLU(),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the second block\n",
                "        self.convt2 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(..., bias=False), # (B, 128, 16, 16)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.ReLU(),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the third block using nn.Sequential with ConvTranspose2d, and activation\n",
                "        self.convt3 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(..., bias=False), # (B, num_input_channels, 32, 32)\n",
                "            nn.Tanh(),\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \n",
                "        # TODO: Define the forward of the generator, x are random noise vectors (B, noise_size)\n",
                "        ...\n",
                "        x = x.reshape(...) # (B, channels, height, width)\n",
                "        ...\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "LnSOu07Q-FLh"
            },
            "source": [
                "Similarly, let's define a simple discriminator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9BQFM-vIPHvj"
            },
            "source": [
                "### Exercise 2: Discriminator\n",
                "\n",
                "The discriminator takes an image as input and classifies it between Real or Fake (1 or 0). Your exercise is to create the discriminator model.\n",
                "\n",
                "It should follow these guidelines:\n",
                "* The input will be an image of size `[num_input_channels, 32, 32]`\n",
                "* You should apply 3 blocks of:\n",
                "    * Convolution with kernel size 4, stride 2 and padding 1. The output channels should be 128, 256 and 512.\n",
                "    * BatchNorm2d except for the first block.\n",
                "    * LeakyReLU activation (alpha=0.2)\n",
                "* Then you should apply a fully connected with input size 512\\*4\\*4 (channels\\*height\\*width) and the correct output size and activation for binary classification\n",
                "\n",
                "\n",
                "**Hint**: Remember to use reshape/flatten where necessary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Discriminator(torch.nn.Module):\n",
                "    \n",
                "    def __init__(self, num_input_channels: int):\n",
                "        super().__init__()\n",
                "        \n",
                "        # TODO: Create the first block\n",
                "        self.conv1 = nn.Sequential(\n",
                "            nn.Conv2d(..., bias=False), # (B, 128, 16, 16)\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the second block\n",
                "        self.conv2 = nn.Sequential(\n",
                "            nn.Conv2d(..., bias=False), # (B, 256, 8, 8)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the third block\n",
                "        self.conv3 = nn.Sequential(\n",
                "            nn.Conv2d(..., bias=False), # (B, 512, 4, 4)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the fully connected block using nn.Sequential with Linear and activation\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(...)\n",
                "            nn.Sigmoid(), # Binary classification (real vs false)\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
                "        \n",
                "        # TODO: Define the forward of the discriminator, x are images (B, num_input_channels, 32, 32)\n",
                "        ...\n",
                "        x = x.reshape(...) # (B, channels * height * width)\n",
                "        ...\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generator = Generator(hparams['noise_size'], hparams['num_input_channels']).to(device)\n",
                "optimizer_g = torch.optim.Adam(generator.parameters(), lr=hparams['learning_rate'], betas=hparams['betas'])\n",
                "\n",
                "discriminator = Discriminator(hparams['num_input_channels']).to(device)\n",
                "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=hparams['learning_rate'], betas=hparams['betas'])\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "\n",
                "def init_weights(m):\n",
                "    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n",
                "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
                "        if m.bias != None:\n",
                "            torch.nn.init.constant_(m.bias, 0.0)\n",
                "    if type(m) == nn.BatchNorm2d:\n",
                "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
                "        nn.init.constant_(m.bias, 0)\n",
                "\n",
                "generator.apply(init_weights)\n",
                "discriminator.apply(init_weights);\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VNKQKA4FfCL7"
            },
            "source": [
                "## Train function"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ryDTioLaSd_t"
            },
            "source": [
                "### Exercise 3: Train\n",
                "\n",
                "Complete the code. Take into account which labels should be used at each step of the training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_batch(\n",
                "        real_samples: torch.Tensor, \n",
                "        generator: torch.nn.Module,\n",
                "        discriminator: torch.nn.Module,\n",
                "        optimizer_g: torch.optim,\n",
                "        optimizer_d: torch.optim,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    generator.train()\n",
                "    discriminator.train()\n",
                "    \n",
                "    bsz = real_samples.shape[0]\n",
                "    \n",
                "    # TODO: Define the labels for the real (ones) and fake (zeros) images of size (bsz, 1)\n",
                "    label_real = ...\n",
                "    label_fake = ...\n",
                "\n",
                "    label_real = label_real.to(device)\n",
                "    label_fake = label_fake.to(device)\n",
                "\n",
                "    ####################\n",
                "    # OPTIMIZE GENERATOR\n",
                "    ####################\n",
                "\n",
                "    # Reset gradients\n",
                "    optimizer_g.zero_grad()\n",
                "\n",
                "    # Generate fake samples\n",
                "    z = torch.randn(bsz, hparams['noise_size'], device=device) \n",
                "    fake_samples = generator(z) \n",
                "    \n",
                "    # Evaluate the generated samples with the discriminator\n",
                "    predictions_g_fake = discriminator(fake_samples) \n",
                "    # Calculate error with respect to what the generator wants\n",
                "    loss_g = criterion(predictions_g_fake, label_real) \n",
                "        \n",
                "    # Backpropagate\n",
                "    loss_g.backward() \n",
                "    \n",
                "    # Update weights (do a step in the optimizer)\n",
                "    optimizer_g.step() \n",
                "    \n",
                "    ########################\n",
                "    # OPTIMIZE DISCRIMINATOR\n",
                "    ########################\n",
                "\n",
                "    fake_samples = fake_samples.detach() # Let's detach them to freeze the generator\n",
                "\n",
                "    # Reset gradients\n",
                "    optimizer_d.zero_grad()\n",
                "\n",
                "    # Calculate discriminator prediction for real samples\n",
                "    predictions_d_real = discriminator(real_samples)\n",
                "\n",
                "    # Calculate error with respect to what the discriminator wants\n",
                "    loss_d_real = criterion(predictions_d_real, label_real) \n",
                "\n",
                "    # Calculate discriminator loss for fake samples\n",
                "    predictions_d_fake = discriminator(fake_samples) \n",
                "\n",
                "    # Calculate error with respect to what the discriminator wants\n",
                "    loss_d_fake = criterion(predictions_d_fake, label_fake) \n",
                "    \n",
                "    # Total discriminator loss\n",
                "    loss_d = (loss_d_real + loss_d_fake) / 2\n",
                "    loss_d.backward()\n",
                "\n",
                "    # Update weights (do a step in the optimizer)\n",
                "    optimizer_d.step() \n",
                "\n",
                "    return loss_g.item(), loss_d.item()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "VbBC3H3j0Yal"
            },
            "source": [
                "## Evaluation function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.no_grad()\n",
                "def evaluate(generator: torch.nn.Module, z_val: torch.Tensor) -> Image.Image:\n",
                "    \n",
                "    generator.eval()\n",
                "    fake_samples = generator(z_val).cpu()\n",
                "    # select a sample or create grid if img is a batch\n",
                "    nrows = int(math.sqrt(fake_samples.shape[0]))\n",
                "    img = utils.make_grid(fake_samples, nrow=nrows)\n",
                "\n",
                "    # unnormalize\n",
                "    img = (img*0.5 + 0.5)*255\n",
                "\n",
                "    # to numpy\n",
                "    image_numpy = img.numpy().astype(np.uint8)\n",
                "    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
                "    \n",
                "    return Image.fromarray(image_numpy)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "iWS0uvRJEHGd"
            },
            "source": [
                "## Train loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_val = torch.randn(hparams['num_val_samples'], hparams['noise_size'], device=device)\n",
                "\n",
                "for epoch in range(hparams['num_epochs']):\n",
                "\n",
                "    for i, (real_samples, labels) in enumerate(dataloader):\n",
                "        real_samples = real_samples.to(device)\n",
                "        loss_g, loss_d = train_batch(real_samples, generator, discriminator, optimizer_g, optimizer_d)\n",
                "\n",
                "        if (i+1) % 200 == 0:\n",
                "            print(f\"\\nEpoch: {epoch+1}/{hparams['num_epochs']}, batch: {i+1}/{len(dataloader)},\"\n",
                "                  +f\" G_loss: {loss_g}, D_loss: {loss_d}\")\n",
                "            \n",
                "            fake_images = evaluate(generator, z_val)\n",
                "            display(fake_images)\n",
                "\n",
                "    print(f\"\\nEpoch: {epoch+1}/{hparams['num_epochs']}, batch: {i+1}/{len(dataloader)},\"\n",
                "          +f\" G_loss: {loss_g}, D_loss: {loss_d}\")\n",
                "    \n",
                "    fake_images = evaluate(generator, z_val)\n",
                "    display(fake_images)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0FgkBXj-9Jgq"
            },
            "source": [
                "# Extra: Conditional GAN"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bbf_CtNvS2uT"
            },
            "source": [
                "### Exercise 4: Generator\n",
                "\n",
                "We will now modify the generator from before to a conditional generator. To do it, we will concatenate the input to the convolutions with an embedding of the label we want to generate.\n",
                "\n",
                "Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n",
                "\n",
                "**Hint**: The embedding is concatenated as a new channel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConditionalGenerator(torch.nn.Module):\n",
                "    \n",
                "    def __init__(\n",
                "            self, \n",
                "            noise_size: int,\n",
                "            num_input_channels: int,\n",
                "            num_classes: int\n",
                "            ) -> None:\n",
                "        \n",
                "        super().__init__()\n",
                "      \n",
                "        # TODO: Create the Fully connected layer using nn.Linear\n",
                "        self.fc = ...\n",
                "\n",
                "        #\u00a0Embedding of the class\n",
                "        self.embedding = nn.Embedding(\n",
                "            num_embeddings=num_classes, \n",
                "            embedding_dim=4*4,\n",
                "        )\n",
                "\n",
                "        # TODO: Create the first block\n",
                "        self.convt1 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(...+1, ..., bias=False), # (B, 256, 8, 8)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.ReLU(),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the second block\n",
                "        self.convt2 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(..., bias=False), # (B, 128, 16, 16)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.ReLU(),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the third block using nn.Sequential with ConvTranspose2d, and activation\n",
                "        self.convt3 = nn.Sequential(\n",
                "            nn.ConvTranspose2d(..., bias=False), # (B, num_input_channels, 32, 32)\n",
                "            nn.Tanh(),\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor, label: int) -> torch.Tensor:\n",
                "        \n",
                "        # TODO: Define the forward of the generator, x are random noise vectors (B, noise_size)\n",
                "        ...\n",
                "        x = x.reshape(...) # (B, channels, height, width)\n",
                "        emb = self.embedding(...).reshape(-1, 1, 4, 4) # (B, 1, height, width)\n",
                "        x = torch.cat(..., dim=1)\n",
                "        ...\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZhtPC283Tbkv"
            },
            "source": [
                "### Exercise 5: Discriminator\n",
                "\n",
                "We will now modify the discriminator from before to a conditional discriminator. To do it, we will concatenate the input image with an embedding of the label we want to generate.\n",
                "\n",
                "Complete the forward method. To do it, use the embedding layer with the label, and then use `torch.cat` to concatenate the label as a channel (after the corresponding `reshape`)\n",
                "\n",
                "**Hint**: The embedding is concatenated as a new channel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConditionalDiscriminator(torch.nn.Module):\n",
                "    \n",
                "    def __init__(self, \n",
                "                 num_input_channels: int,\n",
                "                 num_classes: int):\n",
                "        super().__init__()\n",
                "\n",
                "        self.embedding = nn.Embedding(\n",
                "            num_embeddings=num_classes, \n",
                "            embedding_dim=32*32,\n",
                "        )\n",
                "        \n",
                "        # TODO: Create the first block\n",
                "        self.conv1 = nn.Sequential(\n",
                "            nn.Conv2d(...+1, ..., bias=False), # (B, 128, 16, 16)\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the second block\n",
                "        self.conv2 = nn.Sequential(\n",
                "            nn.Conv2d(..., bias=False), # (B, 256, 8, 8)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the third block\n",
                "        self.conv3 = nn.Sequential(\n",
                "            nn.Conv2d(..., bias=False), # (B, 512, 4, 4)\n",
                "            nn.BatchNorm2d(...),\n",
                "            nn.LeakyReLU(0.2),\n",
                "        )\n",
                "\n",
                "        # TODO: Create the fully connected block using nn.Sequential with Linear and activation\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Linear(...)\n",
                "            nn.Sigmoid(), # Binary classification (real vs false)\n",
                "        )\n",
                "\n",
                "    def forward(self, x: torch.Tensor, label: int) -> torch.Tensor:\n",
                "        # TODO: Define the forward of the discriminator, x are images (B, num_input_channels, 32, 32)\n",
                "        emb = self.embedding(...).reshape(-1, 1, 32, 32) # (B, 1, 32, 32)\n",
                "        x = torch.cat(..., dim=1)\n",
                "        ...\n",
                "        x = x.reshape(...) # (B, channels * height * width)\n",
                "        ...\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "generator = ConditionalGenerator(hparams['noise_size'], hparams['num_input_channels'], \n",
                "                                 hparams['num_classes']).to(device)\n",
                "optimizer_g = torch.optim.Adam(generator.parameters(), lr=hparams['learning_rate'], \n",
                "                               betas=hparams['betas'])\n",
                "\n",
                "discriminator = ConditionalDiscriminator(hparams['num_input_channels'],hparams['num_classes']).to(device)\n",
                "optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=hparams['learning_rate'], betas=hparams['betas'])\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "\n",
                "def init_weights(m):\n",
                "    if type(m) in {nn.Conv2d, nn.ConvTranspose2d, nn.Linear}:\n",
                "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
                "        if m.bias != None:\n",
                "            torch.nn.init.constant_(m.bias, 0.0)\n",
                "    if type(m) == nn.BatchNorm2d:\n",
                "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
                "        nn.init.constant_(m.bias, 0)\n",
                "\n",
                "generator.apply(init_weights)\n",
                "discriminator.apply(init_weights);\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "85_bncGME4VC"
            },
            "source": [
                "## Train function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_batch_conditional(\n",
                "        real_samples: torch.Tensor, \n",
                "        real_classes: torch.Tensor,\n",
                "        generator: torch.nn.Module,\n",
                "        discriminator: torch.nn.Module,\n",
                "        optimizer_g: torch.optim,\n",
                "        optimizer_d: torch.optim,\n",
                "        ) -> Tuple[float, float]:\n",
                "\n",
                "    generator.train()\n",
                "    discriminator.train()\n",
                "\n",
                "    bsz = real_samples.shape[0]\n",
                "\n",
                "    # TODO: Define the labels for the real (ones) and fake (zeros) images of size (bsz, 1)\n",
                "    label_real = ...\n",
                "    label_fake = ...\n",
                "\n",
                "    label_real = label_real.to(device)\n",
                "    label_fake = label_fake.to(device)\n",
                "\n",
                "    ####################\n",
                "    # OPTIMIZE GENERATOR\n",
                "    ####################\n",
                "\n",
                "    # Reset gradients\n",
                "    optimizer_g.zero_grad() \n",
                "\n",
                "    # Generate fake samples\n",
                "    z = torch.randn(bsz, hparams['noise_size'], device=device) \n",
                "    fake_classes = torch.randint(0, 10, size=(bsz, 1), device=device)\n",
                "    fake_samples = generator(z, fake_classes) \n",
                "    \n",
                "    # Evaluate the generated samples with the discriminator\n",
                "    predictions_g_fake = discriminator(fake_samples, fake_classes) \n",
                "\n",
                "    # Calculate error with respect to what the generator wants\n",
                "    loss_g = criterion(predictions_g_fake, label_real) \n",
                "\n",
                "    # Backpropagate\n",
                "    loss_g.backward() \n",
                "    \n",
                "    # Update weights\n",
                "    optimizer_g.step() \n",
                "\n",
                "    ########################\n",
                "    # OPTIMIZE DISCRIMINATOR\n",
                "    ########################\n",
                "\n",
                "    fake_samples = fake_samples.detach()\n",
                "    \n",
                "    # Reset gradients\n",
                "    optimizer_d.zero_grad() \n",
                "\n",
                "    # Calculate discriminator prediction for real samples\n",
                "    predictions_d_real = discriminator(real_samples, real_classes) \n",
                "\n",
                "    # Calculate error with respect to what the discriminator wants\n",
                "    loss_d_real = criterion(predictions_d_real, label_real) \n",
                "\n",
                "    # Calculate discriminator loss for fake samples\n",
                "    predictions_d_fake = discriminator(fake_samples, fake_classes) \n",
                "\n",
                "    # Calculate error with respect to what the discriminator wants\n",
                "    loss_d_fake = criterion(predictions_d_fake, label_fake) \n",
                "    \n",
                "    # Total discriminator loss\n",
                "    loss_d = (loss_d_real + loss_d_fake) / 2\n",
                "\n",
                "    # Backpropagate\n",
                "    loss_d.backward() \n",
                "\n",
                "    # Update weights\n",
                "    optimizer_d.step() \n",
                "\n",
                "    return loss_g.item(), loss_d.item()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "7cZiJBr4E6P_"
            },
            "source": [
                "## Evaluation function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@torch.no_grad()\n",
                "def evaluate_conditional(\n",
                "        generator: torch.nn.Module,\n",
                "        z_val: torch.Tensor,\n",
                "        labels_val: torch.Tensor,\n",
                "        ) -> Image.Image:\n",
                "    \n",
                "    generator.eval()\n",
                "    fake_samples = generator(z_val, labels_val).cpu()\n",
                "    # select a sample or create grid if img is a batch\n",
                "    nrows = int(math.sqrt(fake_samples.shape[0]))\n",
                "    img = utils.make_grid(fake_samples, nrow=nrows)\n",
                "\n",
                "    # unnormalize\n",
                "    img = (img*0.5 + 0.5)*255\n",
                "\n",
                "    # to numpy\n",
                "    image_numpy = img.numpy().astype(np.uint8)\n",
                "    image_numpy = np.transpose(image_numpy, (1, 2, 0))\n",
                "    return Image.fromarray(image_numpy)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "w-g63Js1E9N7"
            },
            "source": [
                "## Train loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "z_val = torch.randn(hparams['num_val_samples'], hparams['noise_size'], device=device)\n",
                "labels_cycle = cycle(range(hparams['num_classes']))\n",
                "labels_val = torch.tensor([next(labels_cycle) for i in range(hparams['num_val_samples'])],\n",
                "                          device=device).unsqueeze(1) #the labels will be a cycle from 0 to 9\n",
                "\n",
                "for epoch in range(hparams['num_epochs']):\n",
                "\n",
                "    for i, (real_samples, real_classes) in enumerate(dataloader):\n",
                "        real_samples = real_samples.to(device)\n",
                "        real_classes = real_classes.unsqueeze(1).to(device)\n",
                "        loss_g, loss_d = train_batch_conditional(\n",
                "            real_samples, \n",
                "            real_classes, \n",
                "            generator, \n",
                "            discriminator, \n",
                "            optimizer_g, \n",
                "            optimizer_d,\n",
                "        )\n",
                "\n",
                "        if (i+1) % 200 == 0:\n",
                "            print(f\"\\nEpoch: {epoch+1}/{hparams['num_epochs']}, batch: {i+1}/{len(dataloader)},\"\n",
                "                  + f\" G_loss: {loss_g}, D_loss: {loss_d}\")\n",
                "            \n",
                "            fake_images = evaluate_conditional(generator, z_val, labels_val)\n",
                "            display(fake_images)\n",
                "\n",
                "    print(f\"\\nEpoch: {epoch+1}/{hparams['num_epochs']}, batch: {i+1}/{len(dataloader)}, \"\n",
                "          + f\"G_loss: {loss_g}, D_loss: {loss_d}\")\n",
                "    \n",
                "    fake_images = evaluate_conditional(generator, z_val, labels_val)\n",
                "    display(fake_images)\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1VmA9OpZzqSu"
            },
            "source": [
                "### Conditional generation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#You can play and visualize different numbers\n",
                "number_chosen = 3\n",
                "z_val = torch.randn(hparams['num_val_samples'], hparams['noise_size'], device=device)\n",
                "number_chosen_torch = torch.tensor([\n",
                "        number_chosen for i in range(hparams['num_val_samples'])\n",
                "    ], \n",
                "    device=device).unsqueeze(1)\n",
                "\n",
                "images_number_chosen = evaluate_conditional(generator, z_val, number_chosen_torch)\n",
                "display(images_number_chosen)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
