{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.5"
        },
        "interpreter": {
            "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "R-tjDhXJeixW"
            },
            "source": [
                "# A World of Tensors and Differentiable Computing\n",
                "Created by [Santiago Pascual](https://scholar.google.es/citations?user=7cVOyh0AAAAJ&hl=ca) ([UPC School](https://www.talent.upc.edu/ing/estudis/formacio/curs/310400/postgrau-artificial-intelligence-deep-learning/) 2019).\n",
                "\n",
                "Updated by [Gerard I. G\u00e1llego](https://www.linkedin.com/in/gerard-gallego/) and [Jos\u00e9 A. R. Fonollosa](https://www.kaggle.com/jarfo1)\n",
                "\n",
                "In these lab exercises we are going to see:\n",
                "\n",
                "1. What are tensors, concretely in the PyTorch framework.\n",
                "2. How to operate with them, and typical operations for deep learning modeling.\n",
                "3. Broadcasting Semantics. Added by [Jose A. R. Fonollosa](https://www.kaggle.com/jarfo1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NwQSQBAt5WL2"
            },
            "source": [
                "## What is a Tensor?\n",
                "\n",
                "\n",
                "A Tensor is the generalization of a vector into k dimensions.\n",
                "\n",
                "![](https://miro.medium.com/max/644/1*SGqhI_WpSaEr17wo8ycUhg.png)\n",
                "\n",
                "Table taken from [1].\n",
                "\n",
                "Because of this, a tensor is any k-dimensional structure, including matrices, vectors and scalars. PyTorch is a deep learning framework (https://pytorch.org) widely used for both research and production. As in any other deep learning framework, its core data structure is the tensor."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We first import PyTorch and Numpy libraries as fundamental tools to work with arrays and tensors\n",
                "import torch\n",
                "import numpy\n",
                "# initialize a random seed such that every execution will raise same random sequences of results\n",
                "torch.manual_seed(1)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MQQFFjMQPT1n"
            },
            "source": [
                "#### Creating tensors with PyTorch"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We can initialize an empty structure with certain dimensions:\n",
                "a = torch.empty(5, 7)\n",
                "# and we can check its dimensionality with the .shape attribute or .size() function\n",
                "print(a.shape)\n",
                "print(a.size())"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "zl28ffg_67z8"
            },
            "source": [
                "Dimensions in PyTorch tensors are indexed from 0 onwards, so the first axis of size 5 is the *dim=0*."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# YAYY we have created a tensor of size 5x7, but what does it contain?\n",
                "print(a)\n",
                "# Rubbish, nonsense, random stuff, it could be zero, it could be nan it could be whatever"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We can easily fill the tensor with some fixed value with the .fill_(val) function\n",
                "a.fill_(10)\n",
                "print(a)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Of course we could go k-dimensional, we just have to put more numbers in the init function\n",
                "a = torch.empty(2,4,6,8)\n",
                "print(a.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_F0pCzMXkEU6"
            },
            "source": [
                "#### There are functions in PyTorch to initialize some special tensors:\n",
                "\n",
                "* **torch.randn** samples from a Gaussian distribution (mean=0, std=1)\n",
                "* **torch.rand** samples from a uniform distribution [0, 1)\n",
                "* **torch.ones** creates a tensor with 1s\n",
                "* **torch.zeros** creates a tensor with 0s"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kC_o44hLkaqy"
            },
            "source": [
                "### Exercise 1\n",
                "\n",
                "Create a tensor *z* drawn from a Gaussian distribution of dimensions (16, 1024)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# TODO: Gaussian tensor 16x1024\n",
                "z = ..."
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "xdGiSnOKmSyv"
            },
            "source": [
                "#### Tensors data type\n",
                "\n",
                "Importantly, tensors have a data type (like numeric variables are int, float, double, etc.). We can check the type\n",
                "with the **tensor.dtype** attribute. We can also change the dtype of our tensor with a very simple cast following the data type name in the form of a function: **tensor.float()**, **tensor.int()**, **tensor.long()**, etc."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "a = torch.ones(5)\n",
                "print(a.dtype)\n",
                "\n",
                "# change to float64 (aka. double)\n",
                "print(a.double().dtype)\n",
                "\n",
                "# change to float16 (aka. half)\n",
                "print(a.half().dtype)\n",
                "\n",
                "# change to int16 (aka. short)\n",
                "print(a.short().dtype)\n",
                "\n",
                "# change to int64 (aka. long)\n",
                "print(a.long().dtype)\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "PQqUN7JGnhYl"
            },
            "source": [
                "And the way to create a tensor with a specific data type at initialization is either by specifying the **dtype=torch.<dtype\\>** during the tensor initialization, or using an explicit tensor constructor like **torch.FloatTensor()**, **torch.LongTensor()**, etc."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Initialize a tensor with type short()\n",
                "a = torch.empty(5, 7, dtype=torch.short)\n",
                "print(a.dtype)\n",
                "\n",
                "# Directly create a short tensor\n",
                "a = torch.ShortTensor(5, 7)\n",
                "print(a.dtype)\n",
                "\n",
                "# Remember: there should be rubbish in these results, we just explicited a data type, not any value yet! (hence random memory is depicted)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0Tw0oG0HsXOf"
            },
            "source": [
                "#### Tips about tensor data types applied to deep learning:\n",
                "\n",
                "Keep in mind the following relations, they might be very useful for your future selves!\n",
                "\n",
                "* Float32 --> Data type for the neural network parameters and GPU operations!\n",
                "* Long (Int64) --> Data type for text inputs (e.g. indexes of words in a dictionary)\n",
                "* Float16 (Half) --> Data type for currently fastest GPU operations (with less precision) on advanced GPU implementations.\n",
                "\n",
                "Remember: only a sticknote for your future selves, in case you have to deal with any of the above mentioned things (embeddings, fastest GPU stuff, etc.).\n",
                "\n",
                "**TO SEE THE FULL SET OF PYTORCH TENSOR DATA TYPES, CHECK THE DOCUMENTATION AT https://pytorch.org/docs/stable/tensors.html**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qK7tJePCnIoC"
            },
            "source": [
                "#### Bringing the tensors from Python and Numpy\n",
                "\n",
                "You may be familiarized with Numpy and Python lists. The former one is a MUST to do any scientific programming in Python, so if you need a refresh it is recommended to have a quick review: https://becominghuman.ai/an-essential-guide-to-numpy-for-machine-learning-in-python-5615e1758301 . The latter, lists, are the inherent mechanism of Python to create a sorted structure of elements (like a k-dimensional array, as we can embed lists in lists and so on).\n",
                "\n",
                "PyTorch is very well integrated with Numpy (actually PyTorch is supposed to be an enhanced Numpy, with algebraic operations also running on GPU!) and Python. We can hence convert our Numpy and lists into PyTorch tensors VERY EASILY!"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Creating a 1-D tensor from the Numpy array [1, 2, 3]\n",
                "a = torch.tensor(numpy.array([1, 2, 3]))\n",
                "\n",
                "# Creating a 1-D tensor from the Python list [1, 2, 3]\n",
                "a = torch.tensor([1, 2, 3])\n",
                "# Values 1, 2, 3\n",
                "print('Tensor a values: ', a)\n",
                "# 1 dimension of size 3\n",
                "print('Tensor a shape: ', a.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# k-dimensional arrays are also turned into PyTorch tensors as easily as that\n",
                "A = torch.tensor(numpy.ones((16, 1024)))\n",
                "print(A.dtype)\n",
                "print(A.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "-J5CGBU3V5gF"
            },
            "source": [
                "#### Converting tensors back to Numpy!\n",
                "\n",
                "Converting back to numpy arrays is as easy as getting the *.data* attribute of the tensor and calling its *.numpy()* casting function\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "A = torch.rand(10, 10)\n",
                "Anpy = A.data.numpy()\n",
                "print('A type: ', type(A)) # torch.Tensor\n",
                "print('Anpy type: ', type(Anpy)) # numpy.ndarray"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "paUskvqytogM"
            },
            "source": [
                "### Exercise 2\n",
                "\n",
                "Create an **int16** *it* tensor in PyTorch (however you want) from the following numpy array *na*"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "na = 10 * numpy.random.rand(8, 8)\n",
                "\n",
                "# TODO: create the short tensor out of 'na'\n",
                "it = ..."
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "GHyxb3EkxHaT"
            },
            "source": [
                "## Operations with tensors\n",
                "\n",
                "The documentation of PyTorch tensors can be found online in: https://pytorch.org/docs/stable/tensors.html\n",
                "\n",
                "This section introduces two important types of operations in PyTorch:\n",
                "\n",
                "1. In-place operations\n",
                "2. Algebraic operations on tensors: transposing, squeezing/unsqueezing, slicing, chunking and concatenating"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ybnrLcAlNeJV"
            },
            "source": [
                "#### In-place operations"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# In-place operations are those whose function name contain an underscore '_' as in fill_(val), add_(val), etc.\n",
                "a = torch.empty(2, 2)\n",
                "a.fill_(1)\n",
                "print(a)\n"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# There are operations where both inplace and normal methods can be applied\n",
                "# For example to sum some value to the tensor\n",
                "a.add_(1)\n",
                "print(a) # prints a tensor of values \"2\"\n",
                "# This, though, takes no effect\n",
                "a.add(1)\n",
                "print(a)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# So yes, you guessed right! We have to actually assign the result to an output tensor to actually\n",
                "# get the outcome of this operation\n",
                "b = a.add(1)\n",
                "print(b) # NOW it prints a tensor of values \"3\"!"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YwDuaN_w8ZQ3"
            },
            "source": [
                "### Exercise 3\n",
                "\n",
                "Do you notice an important difference between these in-place vs normal operations? Perhaps not yet... what if I tell you that I want to apply an operation upon a FloatTensor 10000x10000? Knowing that we have 32 bits per float value, compute the required memory to store that tensor in Megabytes (1 MB = 1.000.000 Bytes)"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# TODO\n",
                "total_mem = ...\n",
                "\n",
                "print(total_mem)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OGXDnygK-adC"
            },
            "source": [
                "### Now we can be more applied to deep learning. But with tensors, raw operations.\n",
                "\n",
                "A Neuron is defined as a linear operation of weighted sums followed by a non-linearity. We thus have a tensor of weights *w*, a scalar with the bias *b*, and a non-linearity (like ReLU *max(0, x)* that just allows the positive components to go forth in the *y* values).\n",
                "\n",
                "![](https://www.researchgate.net/profile/Haroldo_Campos_Velho2/publication/235901708/figure/fig1/AS:669443441049602@1536619162135/Artificial-neuron-Equation-neuron-output.ppm)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# We will have 1 input vector with 100.000 dimensions (features)\n",
                "x = torch.ones(1, 100000)\n",
                "\n",
                "# Our weight tensor is hence, for a neuron, 1 x 100.000\n",
                "w = 0.02 * torch.randn(1, 100000)\n",
                "\n",
                "\n",
                "# Let's define the function that will perform the operation of a neuron\n",
                "def forward_neuron(x, w, b):\n",
                "  v = x.mm(w.T) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n",
                "  y = v.clamp(min=0) # relu is defined as a truncation of the negative activations to zero (clamp function does the trick)\n",
                "  return y\n",
                "\n",
                "# Now we can see examples of operation through the relu\n",
                "\n",
                "# Our bias is just a scalar\n",
                "bp = 10 * torch.ones(1)\n",
                "\n",
                "print(forward_neuron(x, w, bp))\n",
                "\n",
                "# Shifting the bias quite negatively should raise zero\n",
                "bn = -10 * torch.ones(1)\n",
                "print(forward_neuron(x, w, bn))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "A single neuron is a linear combination of inputs followed by a non-linearity. To build more expressive models, we can stack neurons into layers, where each neuron has its own weights and bias.\n",
                "\n",
                "A layer with two neurons means we now have two sets of weights and biases, each producing one output value. Stacking multiple layers allows us to combine these outputs into more complex representations \u2014 this is exactly the principle behind Multi-Layer Perceptrons (MLPs).\n",
                "\n",
                "Let\u2019s see how this works first with one layer of two neurons, and then with two layers of two neurons each."
            ],
            "metadata": {
                "id": "shM0pFZQBbSk"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ONE LAYER OF TWO NEURONS\n",
                "\n",
                "# Let's create the weights for the new neuron\n",
                "w1, b1 = w, bp\n",
                "w2 = 0.02 * torch.randn(1, 100000)\n",
                "b2 = 5 * torch.ones(1)\n",
                "\n",
                "# Let's create the W matrix for the two neurons together\n",
                "W = torch.cat((w1, w2), dim=0)\n",
                "B = torch.cat((b1, b2))\n",
                "\n",
                "print(\"Take a look at the shapes of the weight matrix W and the bias vector B for a layer with 2 neurons and an input of size 100000:\")\n",
                "print('W shape: ', W.shape)\n",
                "print('B shape: ', B.shape)\n",
                "\n",
                "\n",
                "def forward_layer(x, w, b):\n",
                "    v = x.mm(w.T) + b   # (1,100000) @ (100000,2) = (1,2)\n",
                "    y = v.clamp(min=0)  # ReLU activation\n",
                "    return y\n",
                "\n",
                "print(\"\\nOutput of one layer with two neurons:\")\n",
                "print(forward_layer(x, W, B))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# TWO LAYERS, TWO NEURONS EACH\n",
                "\n",
                "# First layer, 2 neurons (100000 inputs)\n",
                "W1 = 0.02 * torch.randn(2, 100000)\n",
                "B1 = 10 * torch.randn(2)\n",
                "\n",
                "# Second layer (2 inputs \u2192 2 neurons)\n",
                "W2 = 0.02 * torch.randn(2, 2)\n",
                "B2 = 10 * torch.randn(2)\n",
                "\n",
                "def forward_two_layers(x, w1, b1, w2, b2):\n",
                "    y1 = forward_layer(x, w1, b1)   # First layer\n",
                "    y2 = forward_layer(y1, w2, b2)  # Second layer\n",
                "    return y2\n",
                "\n",
                "print(\"Output of two layers with two neurons each:\")\n",
                "print(forward_two_layers(x, W1, B1, W2, B2))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "With these examples, we see how the simple operation of a neuron \u2014 a weighted sum followed by a non-linearity \u2014 can be scaled up to layers with multiple neurons, and further stacked into multiple layers. Each new layer transforms the outputs of the previous one, allowing us to build increasingly complex representations of the data. This step-by-step construction is the foundation of a Multi-Layer Perceptron (MLP), which we will explore in the next lab with PyTorch\u2019s higher-level abstractions."
            ],
            "metadata": {
                "id": "oDs-trqSEhvW"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Exercise 3-B\n",
                "\n",
                "Now it\u2019s your turn! We have seen how to extend a single neuron into a layer with multiple neurons, and how to stack two layers together. In this exercise, you will define the weights and biases for a small network with two layers:\n",
                "\n",
                "* The **first layer** has **10 neurons**, each receiving **50,000 input features**.\n",
                "\n",
                "* The **second layer** has **3 neurons**, each receiving the outputs of the 10 neurons from the first layer.\n",
                "\n",
                "\ud83d\udc49 **Your task**: Complete the missing parts of the code below to correctly define the weight matrices (`W1`, `W2`) and bias vectors (`B1`, `B2`) for this network.\n",
                "\n",
                "Remember:\n",
                "\n",
                "* The shape of `W` is `(number_of_neurons, number_of_inputs)`.\n",
                "\n",
                "* The shape of `B` is `(number_of_neurons,)`.\n",
                "\n",
                "When you\u2019re done, run the code and check the output. How many values should the output contain? Why?"
            ],
            "metadata": {
                "id": "R_vdTp-WGdMP"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TWO LAYERS: TEN NEURONS THE FIRST ONE AND THREE NEURONS THE SECOND ONE\n",
                "\n",
                "# input\n",
                "x_50000 = torch.ones(1, 50000)\n",
                "\n",
                "# TODO: Define the first layer with 10 neurons and 50000 inputs\n",
                "W1 = 0.02 * torch.randn(..., ...)\n",
                "B1 = torch.randn(...)\n",
                "\n",
                "# TODO: Define the second layer with 3 neurons and X inputs (you should know the value of X from the definition of the first layer)\n",
                "W2 = 0.02 * torch.randn(..., ...)\n",
                "B2 = torch.randn(...)\n",
                "\n",
                "\n",
                "print(\"Output of two layers with ten neurons and two neurons respectively:\")\n",
                "print(forward_two_layers(x_50000, W1, B1, W2, B2))"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qWXKiv2-xTWF"
            },
            "source": [
                "### Exercise 4\n",
                "\n",
                "Continuing with previous in-place vs normal operations rationale, please change the *forward_neuron* function to apply the ReLU in-place. This is very useful to save memory when constructing very deep nets.\n",
                "\n",
                "\n",
                "\n",
                "**NOTE:** for the record, this will solve the doubt you will have some day \"what is this inplace=True in the *nn.ReLU(inplace=True)* object?\" when you build neural networks with the PyTorch *torch.nn* API."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "def forward_neuron(x, w, b):\n",
                "  v = x.mm(w.T) + b # .mm() is the matmul function (http://pytorch.org/docs/stable/torch.html#torch.mm)\n",
                "  # TODO: make the inplace clamping\n",
                "  return ...\n",
                "\n",
                "print(forward_neuron(x, w, bp))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Y85-XopvNafw"
            },
            "source": [
                "#### Transpositions and beyond\n",
                "\n",
                "Bear in mind the following FUNDAMENTAL operations to work with deep learning:\n",
                "\n",
                "* Tensor transposition: swapping dimensions in the tensor\n",
                "* Tensor chunking: breaking down a tensor into sub-pieces through a certain dimension\n",
                "* Tensor concatenation: merging different tensors into a single one.\n",
                "* Tensor squeezing/unsqueezing for dimension adjustments"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Transposition\n",
                "\n",
                "A = torch.empty(10, 20, 5)\n",
                "\n",
                "# Swap axis 2 and 1\n",
                "A_21 = A.transpose(2, 1)\n",
                "\n",
                "print('{} transposed axis (2, 1) to: {}'.format(A.shape, A_21.shape))\n",
                "\n",
                "# Swap axis 2 and 0\n",
                "A_20 = A.transpose(2, 0)\n",
                "\n",
                "print('{} transposed axis (2, 0) to: {}'.format(A.shape, A_20.shape))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "YiCQwQyTRBiu"
            },
            "source": [
                "**NOTE:** The utlity of transpositions will be seen further when dealing with different neural architecture designs."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Different axis can be merged with the .view() operator\n",
                "B = A.view(200, 5)\n",
                "print('{} axis (0, 1) merged to: {}'.format(A.shape, B.shape))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Note that giving a wrong size in the dimensions for .view() raises an error\n",
                "try:\n",
                "  B = A.view(201, 5)\n",
                "except RuntimeError:\n",
                "  print('Wrong dimension sizes specified in .view()!')"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Chunking the tensor with .chunk() requires to specify how many chunks we want in which dimension\n",
                "# For example for tensor A: (10, 20, 5), we can chunk it into 5 sub-tensors of shape (10, 4, 5) each\n",
                "Achunks = torch.chunk(A, 5, dim=1)\n",
                "for i, achunk in enumerate(Achunks):\n",
                "  print('Chunk {} shape: {}'.format(i, achunk.shape))"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# And tensors can be merged back to a tensor Amerged with .cat() operator, specifying in which dimension do we concatenate\n",
                "# So to go back to the same tensor as we had prior to chunking, we stack on dimension 1\n",
                "\n",
                "Amerged = torch.cat(Achunks, dim=1)\n",
                "print('Amerged shape: ', Amerged.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "sIIVSC4Ez7VF"
            },
            "source": [
                "Finally, we may want to add additional dimensions or remove them from our tensor.\n",
                "We achieve so with [.squeeze()](https://pytorch.org/docs/stable/torch.html#torch.squeeze) or [.unsqueeze()](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# Define an empty tensor to start from\n",
                "A = torch.empty(2, 2)\n",
                "\n",
                "# 1) Add an extra dimension in axis 0 (unsqueeze)\n",
                "A = A.unsqueeze(0)\n",
                "\n",
                "# 2) Add an extra dimension in axis 2\n",
                "A = A.unsqueeze(2)\n",
                "\n",
                "# 3) Add an extra dimension in axis 2 again\n",
                "A = A.unsqueeze(2)\n",
                "\n",
                "print('Current A shape after unsqueezing dimensions=(0, 2, 2): ', A.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 4) Remove the dimension 0 from step (1)\n",
                "\n",
                "A = A.squeeze(0)\n",
                "\n",
                "print('Current A shape after squeezing dim=0: ', A.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 5) We will remove all remaining dimensions of size 1 (\"useless\") when we do not specify the dimension\n",
                "\n",
                "A = A.squeeze()\n",
                "\n",
                "print('Current A after squeezing all remaining dimensions of size 1: ', A.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Ya3h7RTxT3Nm"
            },
            "source": [
                "### Exercise 5\n",
                "\n",
                "Unsqueezing and squeezing dimensions can also be achieved with the [.view()](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view) function. \"View\" the tensor *A* to achieve the same shape as the one after step (3) in the previous section with a single function call to *.view()*"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "A = torch.empty(2, 2)\n",
                "\n",
                "# TODO: use view to unsqueeze dimensions 0, 2 and 2 (as in the previous section)\n",
                "A = ...\n",
                "\n",
                "print('Current A shape: ', A.shape)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1romtEZvf0j9"
            },
            "source": [
                "### Exercise 6 (Broadcasting semantics)\n",
                "\n",
                "Many PyTorch operations support [Broadcasting Semantics](https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics).\n",
                "\n",
                "If a PyTorch operation supports broadcast, then its Tensor arguments can be\n",
                "automatically expanded to be of equal sizes (without making copies of the data).\n",
                "\n",
                "Considering two tensors with at least one dimension, they are \"broadcastable\" if the following conditions are fulfilled, when iterating dimensions jointly from the last one:\n",
                "- Dimensions of both tensors are equal\n",
                "- One of them is 1\n",
                "- One of them does not exist\n",
                "\n",
                "Check these examples:\n",
                "```\n",
                "A.shape = torch.Size([      1])\n",
                "B.shape = torch.Size([3, 1, 7])\n",
                "C = A + B\n",
                "C.shape = torch.Size([3, 1, 7])\n",
                "```\n",
                "\n",
                "```\n",
                "A.shape = torch.Size([5, 1, 4, 1])\n",
                "B.shape = torch.Size([   3, 1, 1])\n",
                "C = A + B\n",
                "C.shape = torch.Size([5, 3, 4, 1])\n",
                "```\n",
                "\n",
                "```\n",
                "A.shape = torch.Size([5, 2, 4, 1])\n",
                "B.shape = torch.Size([   3, 1, 1])\n",
                "C = A + B  # Error, the broadcasting condition is broken in the second dimension\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "a = torch.ones(5, 3, dtype=torch.int16)\n",
                "print(f\"a = {a}\\n\")\n",
                "print(f\"a.shape = {a.shape}\\n\")\n",
                "\n",
                "b = torch.tensor([1, 2, 3])\n",
                "print(f\"b = {b}\\n\")\n",
                "print(f\"b.shape = {b.shape}\\n\")\n",
                "\n",
                "print(f\"a*b = {a*b}\\n\")\n",
                "print(f\"(a*b).shape = {(a*b).shape}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "a = torch.ones(5, 3, dtype=torch.int16)\n",
                "print(f\"a = {a}\\n\")\n",
                "print(f\"a.shape = {a.shape}\\n\")\n",
                "\n",
                "b = torch.tensor([1, 2, 3, 4, 5])\n",
                "print(f\"b = {b}\\n\")\n",
                "print(f\"b.shape = {b.shape}\\n\")\n",
                "\n",
                "try:\n",
                "    # a and b are not broadcastable, because in the trailing dimension 3 != 5\n",
                "    print(f\"a*b = {a*b}\\n\") # Error\n",
                "except Exception as e:\n",
                "    print(f\"ERROR: {e}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# TODO: add a trailing dimension to b to make a and b broadcastable\n",
                "b = ...\n",
                "\n",
                "print(f\"a*b = {a*b}\\n\")\n",
                "print(f\"(a*b).shape = {(a*b).shape}\")"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "80Zvpx366WlU"
            },
            "source": [
                "### Exercise 7 (Grand Finale)\n",
                "\n",
                "Given the tensor *A*, shuffle  each of the elements of the first dimension with the *random.shuffle* Python function.\n",
                "\n",
                "**Clue:** use the functions *torch.chunk*, *random.shuffle* (which acts in-place over Python lists), and *torch.cat*."
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "import torch\n",
                "import random\n",
                "A = torch.rand(4, 2, 4)\n",
                "print('A before shuffling:\\n ', A)\n",
                "\n",
                "# TODO: chunk the tensor, and convert the resulting\n",
                "# tuple into a Python list\n",
                "A = ...\n",
                "\n",
                "# TODO: operate with shuffle over the list\n",
                "...\n",
                "\n",
                "# TODO: concatenate the sub-tensors in list \"A\" back\n",
                "# to tensor \"A\"\n",
                "A = ...\n",
                "\n",
                "print('A after shuffling:\\n ', A)"
            ],
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "dWqua-bDWatW"
            },
            "source": [
                "## Final Words\n",
                "\n",
                "Congrats! You reached the end of this introductory tutorial to PyTorch most fundamental data structure. Managing dimensions, casting dtypes, in-place operations and more are the EVERYDAY to-dos of a deep learner. So get ready to master these before delving into the coolest projects ever in which you'll build deep nets. Being confident with tensor operations is very important to properly design neural networks and avoid bugs!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "BhDWyIL97LZ9"
            },
            "source": [
                "### References\n",
                "\n",
                "[1] https://medium.com/datadriveninvestor/from-scalar-to-tensor-fundamental-mathematics-for-machine-learning-with-intuitive-examples-part-163727dfea8d\n",
                "\n",
                "[2] https://pytorch.org/tutorials/beginner/former_torchies/tensor_tutorial.html\n",
                "\n",
                "[3] https://pytorch.org/docs/stable/tensors.html\n"
            ]
        }
    ]
}
