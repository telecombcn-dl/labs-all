{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# \ud83c\udf99\ufe0f L3: Text-to-Speech (TTS)\n",
                "\n",
                "---\n",
                "Created by Guillermo C\u00e1mbara for the Postgraduate Course in Artificial Intelligence with Deep Learning (UPC School, 2024).\n",
                "\n",
                "Updated by Ariadna Sanchez (2024)\n",
                "\n",
                "---\n",
                "\n",
                "Welcome to the third lab (L3) about **speech synthesis (aka TTS)**! We will get familiarised to how **end-to-end synthesis** works, and how we automatically evaluate it.\n",
                "\n",
                "To do so, we are going to illustrate it with an **open-source model called XTTS**, which is based on one of the latest trends in **generative AI**: modelling speech the same way **big LLMs (ChatGPT, Claude...)** model text!\n",
                "\n",
                "We will:\n",
                "- 1) **Build the synthesis pipeline** of XTTS step by step.\n",
                "- 2) Create functions to **evaluate two quality aspects of our synthesis** - intelligibility (using an ASR) and speaker similarity to reference (using a speaker encoder)\n",
                "- 3) Imagine that we are given the task of building a new voice - we will **do speaker generation** and evaluate its outcomes.\n",
                "- 4) **Play with the TTS** we built... and even try to clone our own voice!\n",
                "\n",
                "\u26a0\ufe0f - Make sure to connect your runtime to a GPU for faster inference! Still, you should be able to run the notebook with CPU, if you have issues with GPU."
            ],
            "metadata": {
                "id": "8VnQrxRz14-x"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## \ud83d\udce6 Imports\n",
                "\n",
                "First, we will use **pip** to install [**coqui-tts**](https://github.com/coqui-ai/TTS), an open-source library that contains many TTS models. Particularly, we use it because it has the implementation and weights of [XTTS](https://docs.coqui.ai/en/latest/models/xtts.html#). Additionally, we set up a particular version of numpy that works with Coqui, and jiwer, to compute WER scores later on."
            ],
            "metadata": {
                "id": "vADgsUIQ042w"
            }
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!sudo apt-get install -q -y timidity libsndfile1\n",
                "!pip install pydub numba music21 librosa jiwer numpy==1.26.4 coqui-tts==0.24.2 torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0"
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Math, tensor and audio backend libs\n",
                "import numpy as np\n",
                "import torch\n",
                "import torchaudio\n",
                "\n",
                "# Word error rate libs\n",
                "from jiwer import wer\n",
                "\n",
                "# Plotting libs\n",
                "import librosa.display\n",
                "import matplotlib.pyplot as plt\n",
                "import IPython\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "# Audio imports\n",
                "import librosa\n",
                "from base64 import b64decode\n",
                "from IPython.display import Audio, Javascript\n",
                "from pydub import AudioSegment\n",
                "from scipy.io import wavfile\n",
                "\n",
                "# Coqui text-to-speech lib\n",
                "from TTS.api import TTS"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## \ud83e\udd16 TTS Model\n",
                "\n",
                "Let's download XTTS v2 and store it into a Python object. Such object contains all the code, modules and model neurons (weights) to run synthesis"
            ],
            "metadata": {
                "id": "Nrk6nZHv2TfG"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Keep this to True if you're using a GPU instance (recommended), otherwise set to False.\n",
                "# If GPU is set, the TTS model will be loaded in the GPU to speed up computation.\n",
                "GPU = True\n",
                "\n",
                "# Sampling rate definitions. In ASR, 16000 Hz is the typical sampling rate.\n",
                "# However, in TTS we normally operate at 24000 Hz, to give extra resolution.\n",
                "ASR_SAMPLING_RATE = 16000\n",
                "TTS_SAMPLING_RATE = 24000\n",
                "\n",
                "# Get the XTTS model. Remember to accept the license by typing \"y\" when it is prompted.\n",
                "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=GPU)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## \u2699\ufe0f XTTS Modules\n",
                "There is a single line function that you can use to synthesize text with XTTS... but if we simply use that, we won't learn so much, right? ;)\n",
                "\n",
                "Let's instead, build the synthesis pipeline ourselves! We're going to get every different part of the XTTS engine, and see how it operates. Being so, let's store in different Python instance every module of XTTS:\n",
                "- \ud83d\udcc3 **Text Tokenizer** - it translates text strings into integer text tokens\n",
                "- \ud83d\udcc3 **Text Embedder** - it encodes text tokens into higher dimensional text embeddings\n",
                "- \ud83d\udc64 **Speaker Encoder & Embeddings** - a neural module that captures the characteristics of a speaker into embeddings, so we can tell the model which identity to recreate\n",
                "- \ud83d\udc44 **Speech Generate-Pretraining Transformer (GPT)** - the main component of XTTS, it uses text and speaker embeddings to generate speech tokens and latents.\n",
                "- \ud83d\udd09 **HiFi-GAN Decoder** - a module that acts as a vocoder: it converts speech latents into a waveform you can listen to."
            ],
            "metadata": {
                "id": "ZtcdK1KX4EYN"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Text tokenizer and embedder\n",
                "tokenizer = tts.synthesizer.tts_model.tokenizer\n",
                "text_embedder = tts.synthesizer.tts_model.gpt.text_embedding\n",
                "\n",
                "# Speaker encoder\n",
                "speaker_encoder = tts.synthesizer.tts_model.hifigan_decoder.speaker_encoder\n",
                "speakers_dict = tts.synthesizer.tts_model.speaker_manager.speakers # <- this contains speaker names and their speaker embeddings\n",
                "\n",
                "# Speech GPT\n",
                "gpt = tts.synthesizer.tts_model.gpt\n",
                "\n",
                "# Vocoder - a HiFi-GAN decoder\n",
                "hifigan_decoder = tts.synthesizer.tts_model.hifigan_decoder"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## \ud83e\udea2 Synthesis Pipeline\n",
                "\n",
                "Now, let's build the synthesis pipeline, extracting the required inputs and passing them through each XTTS module. Remember that the pipeline looks like this:\n",
                "\n",
                "1) text -> **(tokenizer)** -> tokens -> **(embedder)** -> text embeddings\n",
                "\n",
                "2) text embeddings + GPT speaker conditioning latents -> **(GPT)** -> speech tokens -> **(GPT)** -> speech latents\n",
                "\n",
                "3) speech latents + speaker embeddings -> **(HiFi-GAN)** -> waveform\n",
                "\n",
                "\u26a0\ufe0f - Due to certain design choices, both GPT and HiFi-GAN use different types of speaker embeddings. That's why we have \"GPT speaker conditioning latents\" (for GPT) and \"speaker embeddings\" (for HiFi-GAN).\n",
                "\n",
                "\u26a0\ufe0f - Coqui TTS gives GPT and HiFi-GAN speaker embeddings that are already precomputed, so you can choose a particular voice from their voice bank. We will see later how to compute new speaker embeddings from new voices."
            ],
            "metadata": {
                "id": "EkDPLw8wW-LH"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "### \ud83d\udcc3 Embedding text\n",
                "\n",
                "As speech synthesis is called also \"text-to-speech\", it is natural to think that text is the essential input to a TTS model. Text is input as a string, but as neural networks operate with numbers, we need to convert the our string to a sequence of integer tokens. This is the step we call **tokenization**. Let's use the **tokenizer** module to do it."
            ],
            "metadata": {
                "id": "2Wof4dSJY-OX"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "text = \"Hi! I love text to speech synthesis.\" # <- the input text to be synthesised\n",
                "language = \"en\" # <- XTTS needs the language code to be prompted, so it knows which language it has to speak. This will be put in the text token sequence"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "def tokenize_text(text, language, tokenizer):\n",
                "    # Normalize text to lower-case\n",
                "    text = text.strip().lower()\n",
                "\n",
                "    # Get integer tokens with the tokenizer\n",
                "    text_tokens = tokenizer.encode(text, language)\n",
                "\n",
                "    return text_tokens"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 1** - Tokenize the input text."
            ],
            "metadata": {
                "id": "HCvD_5H7ah8z"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TODO: Tokenize the input text\n",
                "text_tokens = ...\n",
                "\n",
                "print(f\"Text tokens = {text_tokens}\")\n",
                "print(f\"Token object type = {type(text_tokens)}\")\n",
                "print(f\"Length of the token sequence = {len(text_tokens)}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 2** - Now you got the token sequence, but remember we're operating with a neural TTS model in PyTorch... This means that you need to convert your text tokens sequence to a PyTorch tensor.\n",
                "\n",
                "Look how you convert a list of integers into a tensor of integers in the PyTorch documentation. Once you've done it, manipulate its shape, so it has (batch_size = 1, text_sequence_length). For example, if your list of tokens has 25 tokens, your integer token tensor should have shape (1, 25)."
            ],
            "metadata": {
                "id": "5xhcPABea5j3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def tokenize_text_to_tensor(text, language, tokenizer, gpu=True):\n",
                "    # Normalize text to lower-case\n",
                "    text = text.strip().lower()\n",
                "\n",
                "    # Get integer tokens with the tokenizer\n",
                "    text_tokens = tokenizer.encode(text, language)\n",
                "\n",
                "    # TODO: Implement here the list to integer tensor conversion\n",
                "    # and save the result to text_tokens\n",
                "    text_tokens = ...\n",
                "\n",
                "    # TODO: Adjust the shape of the tensor so it is (1, text_sequence_length)\n",
                "    # and save the result to text_tokens\n",
                "    text_tokens = ...\n",
                "\n",
                "    if gpu:\n",
                "        text_tokens = text_tokens.cuda()\n",
                "\n",
                "    return text_tokens\n",
                "\n",
                "# TODO: Implement the call to convert your text to tensors\n",
                "text_tokens = ...\n",
                "\n",
                "assert torch.is_tensor(text_tokens), f\"Error! text_tokens is not a torch tensor, is: {type(text_tokens)}.\"\n",
                "assert text_tokens.shape == (1, len(tokenizer.encode(text, language))), f\"Error! The text tokens shape detected is: {text_tokens.shape}\"\n",
                "assert not torch.is_floating_point(text_tokens), f\"Error! The tensor is not integer type, it is: {text_tokens.dtype}\"\n",
                "\n",
                "print(f\"Text tokens tensor = {text_tokens}\")\n",
                "print(f\"Text tokens type = {text_tokens.dtype}\")\n",
                "print(f\"Text tokens shape = {text_tokens.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# You can take a look at how the tokenizer looks from the inside. Let's get the HuggingFace Tokenizer object.\n",
                "huggingface_tokenizer = tokenizer.tokenizer\n",
                "\n",
                "vocab_size = huggingface_tokenizer.get_vocab_size()\n",
                "vocab = huggingface_tokenizer.get_vocab()\n",
                "\n",
                "print(f\"Tokenizer vocabulary size = {vocab_size}\")\n",
                "print(f\"Tokenizer vocabulary = {vocab}\")\n",
                "\n",
                "#  See more here \"https://huggingface.co/docs/tokenizers/api/tokenizer\"."
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "You should see that the vocabulary contains common word pieces from many different languages. That is because it is a Byte-Pair Encoder (BPE) tokenizer, more info [here](https://huggingface.co/learn/nlp-course/chapter6/5).\n",
                "\n",
                "Now, you can actually revert your tokenization operation. Run the following cell to send your text tokens back into strings."
            ],
            "metadata": {
                "id": "stuyBDwCgnkk"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Do the reverse operation\n",
                "text_from_token_list = []\n",
                "for token in text_tokens[0, :]:\n",
                "    text_from_token = huggingface_tokenizer.id_to_token(token)\n",
                "    text_from_token_list.append(text_from_token)\n",
                "\n",
                "# It is not exactly what you wrote in the beginning, you see some strings are substituted by others.\n",
                "# Like whitespace \" \" is codified as \"[SPACE]\". Don't worry about it, this is all internal logic from the library, but you can witness overall how a tokenizer works.\n",
                "# Also noticed how, as mentioned before, the language code has been prepended to the text sequence internally by the tokenizer.\n",
                "print(text_from_token_list)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 3** - As you see, an integer token is a shallow codification of text, with little information. It is useful to expand these tokens to larger embeddings, which codify more information relevant to our synthesis task.\n",
                "\n",
                "Use the **text_embedder** module to convert tokens into embeddings. Simply forward the **text_tokens** through **text_embedder** and examine these."
            ],
            "metadata": {
                "id": "l-PAr_mnetM1"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TODO: Implemented here the forward call of text_tokens through text_embeddings\n",
                "text_embeddings = ...\n",
                "\n",
                "assert text_embeddings.shape == (1, text_tokens.shape[1], 1024)\n",
                "\n",
                "# And visualize. Expected shape is (batch_size, text_sequence_length, dimension).\n",
                "# You see? The length is the same as in the token sequence, but now every token has been expanded to a large embedding of 1024.\n",
                "# This enriches the text knowledge of our model.\n",
                "print(f\"Text tokens shape = {text_tokens.shape}\")\n",
                "print(f\"Text embeddings shape = {text_embeddings.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### \ud83d\udcc3 Getting speaker embeddings\n",
                "\n",
                "Of course, you may input text alone to a TTS but... which kind of voice identity should it output? That is why another essential input for a TTS is an encoding of the speaker identity to replicate.\n",
                "\n",
                "XTTS has two encoders that codify two types of speaker embeddings. One is used to condition GPT, and the other to condition HiFi-GAN. GPT is supposed to output the highest level details of speaker identity, prosody and style. HiFi-GAN is supposed to then add on top fine-level details that yield a nice sounding waveform.\n",
                "\n",
                "As mentioned before, XTTS library has some precomputed GPT and HiFi-GAN speaker embeddings that we can inspect and feed to the model to get different output identities."
            ],
            "metadata": {
                "id": "qnQuP8tWiFEW"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# We can get the speaker names to use from the speakers dict that we extracted before\n",
                "speaker_names = speakers_dict.keys()\n",
                "print(speaker_names)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# And we may use the name of the first speaker, for example, to extract the GPT and HiFi-GAN speaker embeddings\n",
                "gpt_speaker_emb, hifigan_speaker_emb = speakers_dict['Claribel Dervla'].values()\n",
                "print(f\"GPT speaker embedding shape = {gpt_speaker_emb.shape}\")\n",
                "print(f\"HiFi-GAN speaker embedding shape = {hifigan_speaker_emb.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "HiFi-GAN speaker embedding consists of a single embedding of size 512.\n",
                "However, GPT's consists of 32 embeddings of size 1024.\n",
                "\n",
                "This is a bit more advanced information, but if you're curious about it... GPT uses more and bigger speaker embeddings to inpose stronger conditioning, not only to recreate the speaker identity, but also to try to capture their style, accent, emotion... Whereas HiFi-GAN uses a smaller embedding that is supposed to only add some details of the speaker identity. GPT cooks the cake, and HiFi-GAN adds the cherry on top, if you want to think it that way."
            ],
            "metadata": {
                "id": "aWKdoBTwla7L"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 4** - Let's make a plot of the speaker clusters, according to the traits defined in their speaker embeddings. We will use t-SNE to visualize that. Don't worry about the specifics of t-SNE for now, consider it as an algorithm that allows you to downsample a large embedding into a few dimensions that you can plot to visualize your data distribution.\n",
                "\n",
                "We give the function to plot t-SNE here below."
            ],
            "metadata": {
                "id": "cC9PFjARmSlt"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%matplotlib inline\n",
                "\n",
                "def plot_tsne(embeddings, labels=None, perplexity=30, n_iter=1000):\n",
                "    # Convert to numpy if it's a PyTorch tensor\n",
                "    if isinstance(embeddings, torch.Tensor):\n",
                "        embeddings = embeddings.cpu().detach().numpy()\n",
                "\n",
                "    # Perform t-SNE on the embeddings\n",
                "    tsne = TSNE(n_components=2, perplexity=perplexity, n_iter=n_iter, random_state=42)\n",
                "    embeddings_2d = tsne.fit_transform(embeddings)\n",
                "\n",
                "    # Create a scatter plot\n",
                "    plt.figure(figsize=(10, 8))\n",
                "\n",
                "    if labels is not None:\n",
                "        # If labels are provided, color the points based on labels\n",
                "        unique_labels = np.unique(labels)\n",
                "        for label in unique_labels:\n",
                "            indices = np.where(labels == label)\n",
                "            plt.scatter(embeddings_2d[indices, 0], embeddings_2d[indices, 1], label=f\"Speaker {label}\")\n",
                "        plt.legend()\n",
                "    else:\n",
                "        # If no labels, plot all points with the same color\n",
                "        plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1])\n",
                "\n",
                "    plt.title(\"t-SNE of Speaker Embeddings\")\n",
                "    plt.xlabel(\"t-SNE Dimension 1\")\n",
                "    plt.ylabel(\"t-SNE Dimension 2\")\n",
                "    plt.grid(True)\n",
                "    plt.show()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Now, your task is to collect all the HiFi-GAN speaker embeddings from the speakers dict into a list.\n",
                "hifigan_speaker_embeddings_list = []\n",
                "for speaker_name in speaker_names:\n",
                "    # TODO: Extract here the HiFi-GAN speaker embedding and store it in the list above\n",
                "    gpt_speaker_emb, hifigan_speaker_emb = ...\n",
                "\n",
                "    # Remember that the shape of each embedding is (1, 512, 1), right?\n",
                "    # TODO: Modify the shape of the embeddings tensor so it is (512).\n",
                "    hifigan_speaker_emb = ...\n",
                "\n",
                "    hifigan_speaker_embeddings_list.append(hifigan_speaker_emb)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# We may now convert the list of speaker embeddings into a tensor of shape\n",
                "# (number_of_speaker_embeddings, speaker_embedding_dimension)\n",
                "hifigan_speaker_embeddings = torch.stack(hifigan_speaker_embeddings_list)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# And finally, plot the t-SNE. You should notice two clusters, why do you think\n",
                "# there are two main clusters of speaker voices?\n",
                "plot_tsne(hifigan_speaker_embeddings)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "### \ud83d\udc44 Running the GPT\n",
                "\n",
                "We have our text tokens and GPT speaker embeddings ready. That means we can now generate speech tokens and latents/embeddings with the speech GPT."
            ],
            "metadata": {
                "id": "ChF_E_-Hpi66"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Decoding arguments. These condition the characteristics of the output.\n",
                "top_p = 0.8 # <- Lower values mean the decoder produces more \u201clikely\u201d (aka boring) outputs\n",
                "temperature = 0.65 # <- The softmax temperature of the autoregressive model. Lower means more boring, higher might yield more expressive (but more prone to hallucination) speech"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Run this to make sure that GPT and HiFi-GAN speaker embeddings are moved to the GPU if that's available\n",
                "if GPU:\n",
                "    gpt_speaker_emb = gpt_speaker_emb.cuda()\n",
                "    hifigan_speaker_emb = hifigan_speaker_emb.cuda()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 5** - Pass the text tokens and the GPT speaker embeddings to the **generate** call of the **gpt** module. Because this is an autoregressive module, it will generate tokens until it predicts the \"end of sentence\" token, so every run might yield different output length."
            ],
            "metadata": {
                "id": "VK2rm7bHrPx3"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Solution\n",
                "with torch.no_grad():\n",
                "    # TODO: Pass the GPT speaker embeddings and the text tokens (not the embeddings, gpt.generate automatically will get these from the tokens)\n",
                "    speech_tokens = gpt.generate(\n",
                "        cond_latents=...,\n",
                "        text_inputs=...,\n",
                "        input_tokens=None,\n",
                "        do_sample=True,\n",
                "        top_p=top_p,\n",
                "        temperature=temperature,\n",
                "        output_attentions=False,\n",
                "    )\n",
                "\n",
                "# Visualize speech GPT tokens...\n",
                "print(f\"Predicted speech tokens = {speech_tokens}\")\n",
                "print(f\"Predicted speech tokens shape = {speech_tokens.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Speech tokens look actually like text tokens, right? That is the idea between these large GPTs to speech, which is to operate with speech tokens the same way\n",
                "we normally operate with text.\n",
                "\n",
                "However, as we discussed, tokens are semantically rich, but are really low dimensional. Once we've synthesised speech tokens, we want to upsample them to a high dimensional representation that we can listen to.\n",
                "\n",
                "We could try to vocode directly speech tokens into a waveform... But research\n",
                "has shown that it is possible to first convert speech tokens into high dimensional speech latents/embeddings, using GPT itself. Let's forward the extracted speech tokens to GPT to get the last layer embeddings. We shall vocode these afterwards."
            ],
            "metadata": {
                "id": "lmiASVD7tJwo"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# GPT needs to know explicitly which are the expected speech and text tokens\n",
                "# lengths to return the latents.\n",
                "expected_output_len = torch.tensor(\n",
                "    [speech_tokens.shape[-1] * gpt.code_stride_len], device=text_tokens.device\n",
                ")\n",
                "text_len = torch.tensor([text_tokens.shape[-1]], device=text_tokens.device)\n",
                "\n",
                "# This is the forward pass of GPT, setting \"return_latent\" to True,\n",
                "# so it returns the last layer embeddings instead of yielding\n",
                "# the speech tokens prediction.\n",
                "with torch.no_grad():\n",
                "    speech_latents = gpt(\n",
                "        text_tokens,\n",
                "        text_len,\n",
                "        speech_tokens,\n",
                "        expected_output_len,\n",
                "        cond_latents=gpt_speaker_emb,\n",
                "        return_attentions=False,\n",
                "        return_latent=True,\n",
                "    )\n",
                "\n",
                "# Let's get GPT latents corresponding to the GPT codes.\n",
                "# Like before, when we got high dimensional embeddings from text, we get high dimensional embeddings from speech GPT codes.\n",
                "# Thus, we get more information about each token in the context of the others, that we will use to upsample speech GPT codes to the waveform domain.\n",
                "print(f\"Speech tokens shape = {speech_tokens.shape}\")\n",
                "print(f\"Speech latents/embeddings shape = {speech_latents.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 6** - Now that you have the synthesised speech latents, it is time to use HiFi-GAN vocoder to obtain the final waveform, so we can listen to it. Code the forward pass of the **hifigan_decoder** and get the waveform."
            ],
            "metadata": {
                "id": "4rGXd5TtwbQl"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "with torch.no_grad():\n",
                "    # TODO: pass the speech latents and the HiFi-GAN speaker embedding. Remember to put the speaker embedding into a shape of (1, 512, 1)\n",
                "    pred_wav = hifigan_decoder(\n",
                "        ...,\n",
                "        g=...,\n",
                "    ).cpu().squeeze()\n",
                "\n",
                "print(f\"Predicted waveform shape = {pred_wav.shape}\")\n",
                "librosa.display.waveshow(pred_wav.numpy(), sr=TTS_SAMPLING_RATE)\n",
                "IPython.display.Audio(pred_wav.numpy(), rate=TTS_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## \u2753 Evaluating Synthesis\n",
                "\n",
                "Evaluating any generation task with AI is hard, because there is not a single metric that covers how good the output is. In TTS research, we normally have two proxy metrics to see if our generation is doing good:\n",
                "- We can check if the synthesis is intelligible and correct, by transcribing the output with an ASR and computing the WER.\n",
                "- We can check if the synthesised speaker identity is good, by getting the speaker embedding of the synthesised output, and comparing it against the speaker embedding used for synthesis."
            ],
            "metadata": {
                "id": "BApOuvGk0vRf"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 7** - Use what you learned about ASR in the last session. Implement an ASR call to transcribe **pred_wav** into text, and check what is the WER by comparing the synthesised text with the original ground truth text."
            ],
            "metadata": {
                "id": "X-03mqgl1rVB"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# I'm suggesting to use Whisper (small) for this, but feel free to try any other ASR.\n",
                "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
                "asr_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
                "asr = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
                "asr.config.forced_decoder_ids = None"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# If you're using Whisper, check the documentation to do the correct calls\n",
                "# for transcribing with it.\n",
                "# See https://huggingface.co/openai/whisper-small#english-to-english\n",
                "\n",
                "def transcribe_speech(pred_wav, sampling_rate):\n",
                "    # TODO: Mind that the required sampling rate for ASR is 16000!\n",
                "    input_features = asr_processor(...)\n",
                "\n",
                "    # TODO: generate token ids\n",
                "    predicted_ids = asr.generate(...)\n",
                "\n",
                "    # TODO: decode token ids to text\n",
                "    transcription = ...\n",
                "\n",
                "    return pred_text\n",
                "\n",
                "pred_text = transcribe_speech(pred_wav, sampling_rate=ASR_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Because our ground truth text and synthesised text might have punctuations, let's\n",
                "# remove them and normalize the text.\n",
                "pred_text_norm = asr_processor.tokenizer._normalize(pred_text[0])\n",
                "ground_truth_text_norm = asr_processor.tokenizer._normalize(text)\n",
                "\n",
                "print(f\"Ground truth text = {text}\")\n",
                "print(f\"Ground truth text (normalised) = {ground_truth_text_norm}\")\n",
                "print(f\"Pred text = {pred_text}\")\n",
                "print(f\"Pred text (normalised) = {pred_text_norm}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's compute the WER!\n",
                "word_error_rate = wer(ground_truth_text_norm, pred_text_norm) * 100.0\n",
                "print(f\"WER of our synthesis = {word_error_rate}%\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Finally, let's compute speaker similarity. We already have the HiFi-GAN speaker embedding of the original speaker. Now, let's use the HiFi-GAN speaker encoder to get the speaker embedding of the synthesised waveform."
            ],
            "metadata": {
                "id": "dQuK-rDm4bm9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# The speaker encoder operates with 16000 Hz waveforms, so let's resample\n",
                "# the predicted waveform to 16000 Hz.\n",
                "pred_wav_16k = torchaudio.functional.resample(pred_wav, TTS_SAMPLING_RATE, ASR_SAMPLING_RATE)\n",
                "\n",
                "# Let's get the speaker embedding of the synthesised audio.\n",
                "synth_speaker_emb = speaker_encoder.forward(pred_wav_16k.unsqueeze(0).to(text_tokens.device), l2_norm=True).unsqueeze(-1).to(text_tokens.device).view(512)\n",
                "\n",
                "print(f\"Speaker embedding shape of the original speaker = {hifigan_speaker_emb.shape}\")\n",
                "print(f\"Speaker embedding shape of the synthesised speaker = {synth_speaker_emb.shape}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 8** - Now, use cosine similarity to compare how close the original and synthesised speaker embeddings are. See the documentation here https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html"
            ],
            "metadata": {
                "id": "0r8wIWdi5S1B"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# TODO: Use cosine similarity to compare both embeddings\n",
                "speaker_sim = ...\n"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "##(Optional) Speaker Generation\n",
                "\n",
                "Until now, we've played with precomputed voices. But could you create a new voice? This is an exciting and ongoing research field.\n",
                "\n",
                "I encourage you to think on new ways to create new voices. You could mix one or more speaker embeddings of different speakers, for instance... but how? That's up to you!\n",
                "\n",
                "As we have limited time in this class, let's start with something simple. Let's simply get the HiFi-GAN speaker embedding from another speaker, and let's sum it to our current speaker embedding with a weighted sum."
            ],
            "metadata": {
                "id": "Y46ZuLbd68cx"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Our utility function to linearly combine a speaker embedding with another.\n",
                "def add_embedding_to_embedding_with_scale(speaker_emb_a, speaker_emb_b, scale):\n",
                "    return speaker_emb_a + (speaker_emb_b * scale)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Let's generate a noise vector of the same shape of our HiFi-GAN embedding\n",
                "_, hifigan_speaker_emb_b = speakers_dict[\"Daisy Studious\"].values()\n",
                "hifigan_speaker_emb_b = hifigan_speaker_emb_b.view(512)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "\u270d\ufe0f **TASK 9** - Manually modify the scale and check how does the output synthesis respond. Listen to the samples, and also keep track of the WER and speaker similarity values. What do you think about it?\n",
                "\n",
                "Remember that the larger the scale is, the more you combine the new speaker embedding into the old one."
            ],
            "metadata": {
                "id": "J5OgmY4k77bj"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "scale = 1\n",
                "\n",
                "# Re-vocode with the new speaker identity\n",
                "new_hifigan_speaker_emb = add_embedding_to_embedding_with_scale(hifigan_speaker_emb, hifigan_speaker_emb_b.to(hifigan_speaker_emb.device), scale)\n",
                "with torch.no_grad():\n",
                "    new_speaker_pred_wav = hifigan_decoder(speech_latents, g=new_hifigan_speaker_emb.unsqueeze(0).unsqueeze(-1)).cpu().squeeze()\n",
                "\n",
                "# Compute WER\n",
                "new_pred_text = transcribe_speech(new_speaker_pred_wav, sampling_rate=ASR_SAMPLING_RATE)\n",
                "new_wer = wer(ground_truth_text_norm, asr_processor.tokenizer._normalize(new_pred_text[0]))\n",
                "print(f\"Previous WER = {word_error_rate}%\")\n",
                "print(f\"New WER = {new_wer * 100}%\")\n",
                "\n",
                "# Compute speaker cosine similarity\n",
                "new_pred_wav_16k = torchaudio.functional.resample(new_speaker_pred_wav, TTS_SAMPLING_RATE, ASR_SAMPLING_RATE)\n",
                "new_synth_speaker_emb = speaker_encoder.forward(new_pred_wav_16k.unsqueeze(0).to(text_tokens.device), l2_norm=True).unsqueeze(-1).to(text_tokens.device).view(512)\n",
                "new_speaker_sim = torch.nn.functional.cosine_similarity(new_synth_speaker_emb, hifigan_speaker_emb, dim=0)\n",
                "print(f\"Previous speaker cosine similarity = {speaker_sim * 100}%\")\n",
                "print(f\"New speaker cosine similarity = {new_speaker_sim * 100}%\")\n",
                "\n",
                "# Visualize waveforms\n",
                "librosa.display.waveshow(new_speaker_pred_wav.numpy(), sr=TTS_SAMPLING_RATE)\n",
                "IPython.display.Audio(new_speaker_pred_wav.detach().numpy(), rate=TTS_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "#(Optional) \ud83e\udd29 Play time!\n",
                "\n",
                "That's all the work for today. Now that you have a better picture on how XTTS works on the inside, let me give you the function so you can run the full synthesis pipeline with a single command from the API.\n",
                "\n",
                "Feel free to play with different sentences, speakers and so on. I recommend you to try sentences that convey different emotions (\"I'm so sad...\", \"I'm really angry!!\", etc.) to see how well does the model render emotions depending on text."
            ],
            "metadata": {
                "id": "CDVrUf2KCQVz"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "input_text = \"Hello world!\"\n",
                "input_language = \"en\"\n",
                "input_speaker = \"Ana Florence\"\n",
                "\n",
                "tts.tts_to_file(\n",
                "    text=input_text,\n",
                "    file_path=\"output.wav\",\n",
                "    speaker=input_speaker,\n",
                "    language=input_language\n",
                ")\n",
                "IPython.display.Audio('output.wav', rate=TTS_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "#(Optional) \ud83d\udde3\ufe0f Use your own voice!\n",
                "\n",
                "By definition, most of the new TTS models based on large generative paradigms are able to clone a voices with a few seconds of a voice prompt.\n",
                "\n",
                "You can record yourself and pass that recording so XTTS extracts your speaker embeddings for GPT and HiFi-GAN. What do you think about it?\n",
                "\n",
                "\u26a0\ufe0f - With great power comes great responsability... Always use voice cloning for the good, and never, ever, clone someone's voice without consent."
            ],
            "metadata": {
                "id": "NrIEF4FXDBE5"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "#@title [Run this] Definition of the JS code to record audio straight from the browser\n",
                "\n",
                "RECORD = \"\"\"\n",
                "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
                "const b2text = blob => new Promise(resolve => {\n",
                "  const reader = new FileReader()\n",
                "  reader.onloadend = e => resolve(e.srcElement.result)\n",
                "  reader.readAsDataURL(blob)\n",
                "})\n",
                "var record = time => new Promise(async resolve => {\n",
                "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
                "  recorder = new MediaRecorder(stream)\n",
                "  chunks = []\n",
                "  recorder.ondataavailable = e => chunks.push(e.data)\n",
                "  recorder.start()\n",
                "  await sleep(time)\n",
                "  recorder.onstop = async ()=>{\n",
                "    blob = new Blob(chunks)\n",
                "    text = await b2text(blob)\n",
                "    resolve(text)\n",
                "  }\n",
                "  recorder.stop()\n",
                "})\n",
                "\"\"\"\n",
                "\n",
                "def record(sec=5, out_name='recorded_audio.wav'):\n",
                "  try:\n",
                "    from google.colab import output\n",
                "  except ImportError:\n",
                "    print('No possible to import output from google.colab')\n",
                "    return ''\n",
                "  else:\n",
                "    print('Recording')\n",
                "    display(Javascript(RECORD))\n",
                "    s = output.eval_js('record(%d)' % (sec*1000))\n",
                "    fname = out_name\n",
                "    print('Saving to', fname)\n",
                "    b = b64decode(s.split(',')[1])\n",
                "    with open(fname, 'wb') as f:\n",
                "      f.write(b)\n",
                "    return fname"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "EXPECTED_SAMPLE_RATE = ASR_SAMPLING_RATE\n",
                "\n",
                "def convert_audio_for_model(user_file, output_file='converted_audio_file.wav'):\n",
                "  audio = AudioSegment.from_file(user_file)\n",
                "  audio = audio.set_frame_rate(EXPECTED_SAMPLE_RATE).set_channels(1)\n",
                "  audio.export(output_file, format=\"wav\")\n",
                "  return output_file\n",
                "\n",
                "def record_utterance(file_name='my_recording_wav'):\n",
                "    file_name = record(5, file_name)\n",
                "    converted_audio_file = convert_audio_for_model(file_name)\n",
                "    input_audio, sr = torchaudio.load(converted_audio_file)\n",
                "    return input_audio"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "Run the cell below to record yourself, the recording will be automatically stopped at 5 seconds, approx."
            ],
            "metadata": {
                "id": "IHcPOpWLEcZU"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "yourself_recording_path = \"yourself.wav\"\n",
                "input_audio = record_utterance(yourself_recording_path)\n",
                "print(input_audio.shape)\n",
                "\n",
                "input_audio_24k = torchaudio.functional.resample(input_audio, ASR_SAMPLING_RATE, TTS_SAMPLING_RATE)\n",
                "torchaudio.save(yourself_recording_path, input_audio_24k, TTS_SAMPLING_RATE)\n",
                "\n",
                "librosa.display.waveshow(input_audio_24k.squeeze().numpy(), sr=TTS_SAMPLING_RATE)\n",
                "Audio(input_audio_24k.squeeze().numpy(), rate=TTS_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "input_text = \"We did it! We won the championship! I knew we could do it together!\"\n",
                "input_language = \"en\"\n",
                "output_file_path = \"output_yourself.wav\"\n",
                "\n",
                "tts.tts_to_file(\n",
                "    text=input_text,\n",
                "    file_path=output_file_path,\n",
                "    speaker_wav=yourself_recording_path,\n",
                "    language=input_language\n",
                ")\n",
                "IPython.display.Audio(output_file_path, rate=TTS_SAMPLING_RATE)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}
