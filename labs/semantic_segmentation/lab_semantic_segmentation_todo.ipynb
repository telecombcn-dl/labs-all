{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eUFzPIzjjmvy"
            },
            "source": [
                "# Semantic Segmentation\n",
                "\n",
                "**Notebook created by David Anglada Rotger for the course [Aprenentatge per Refor\u00e7 i Aprenentatge Profund](https://www.upc.edu/content/grau/guiadocent/pdf/cat/230817) in [GCED](https://dse.upc.edu/ca) (2023).** Based on the tutorial\n",
                "by Shivam Chandhok."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bsSdeUEckCvd"
            },
            "source": [
                "# Initializations and imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision\n",
                "!pip install matplotlib\n",
                "!pip install opencv-contrib-python\n",
                "!pip install imutils\n",
                "!pip install scikit-learn\n",
                "!pip install tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "32s5SVYY5WN8"
            },
            "source": [
                "For this lab, we are going to use a dataset from the repository [U-Net: Semantic segmentation with PyTorch - Custom dataset](https://github.com/ajithvallabai/UNet-Pytorch-Customdataset). We are going to clone the repository and unzip de data. Also, we need to create a folder to store the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!git clone https://github.com/ajithvallabai/UNet-Pytorch-Customdataset.git"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cd UNet-Pytorch-Customdataset/"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!unzip data.zip"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cd /content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import the necessary packages\n",
                "import torch\n",
                "import os\n",
                "from torch.nn import BCEWithLogitsLoss\n",
                "from torch.optim import Adam\n",
                "from torch.utils.data import DataLoader\n",
                "from sklearn.model_selection import train_test_split\n",
                "from torchvision import transforms\n",
                "from imutils import paths\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "# import the necessary packages\n",
                "from torch.nn import ConvTranspose2d\n",
                "from torch.nn import Conv2d\n",
                "from torch.nn import MaxPool2d\n",
                "from torch.nn import Module\n",
                "from torch.nn import ModuleList\n",
                "from torch.nn import ReLU\n",
                "import torch.nn as nn\n",
                "from torchvision.transforms import CenterCrop\n",
                "from torch.nn import functional as F\n",
                "import torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# base path of the dataset\n",
                "DATASET_PATH = os.path.join(\"/content\", \"UNet-Pytorch-Customdataset\", \"data\")\n",
                "# define the path to the images and masks dataset\n",
                "IMAGE_DATASET_PATH = os.path.join(DATASET_PATH, \"imgs\")\n",
                "MASK_DATASET_PATH = os.path.join(DATASET_PATH, \"masks\")\n",
                "# define the test split\n",
                "TEST_SPLIT = 0.15\n",
                "# determine the device to be used for training and evaluation\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "# determine if we will be pinning memory during data loading\n",
                "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
                "\n",
                "print(IMAGE_DATASET_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "6jJTisxj5wKK"
            },
            "source": [
                "# Definition of the problem\n",
                "\n",
                "The problem that we are going to face is to segment the flooded area of an image. Hence, we are facing a semantic segmentation problem with two classes: non-flooded and flooded area.\n",
                "\n",
                "To solve the problem, we are going to define a UNet model and train it from scratch.\n",
                "\n",
                "![baixa.png](data:image/png;base64,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)\n",
                "\n",
                "In the following cell, we have an example of a sample of the dataset and its correspondand ground truth mask."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "img = plt.imread(os.path.join(IMAGE_DATASET_PATH, \"0.jpg\"))\n",
                "mask = plt.imread(os.path.join(MASK_DATASET_PATH, \"0.png\"))\n",
                "print(img.shape)\n",
                "fig, axs = plt.subplots(1,2)\n",
                "axs[0].imshow(img)\n",
                "axs[1].imshow(mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "NVLYfRzK6vYD"
            },
            "source": [
                "# Global Variables definition"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# define the number of channels in the input, number of classes,\n",
                "# and number of levels in the U-Net model\n",
                "NUM_CHANNELS = 3\n",
                "NUM_CLASSES = 1\n",
                "NUM_LEVELS = 3\n",
                "# initialize learning rate, number of epochs to train for, and the\n",
                "# batch size\n",
                "INIT_LR = 0.001\n",
                "NUM_EPOCHS = 20\n",
                "BATCH_SIZE = 4\n",
                "# define the input image dimensions\n",
                "INPUT_IMAGE_WIDTH = 800\n",
                "INPUT_IMAGE_HEIGHT = 600\n",
                "# define threshold to filter weak predictions\n",
                "THRESHOLD = 0.5\n",
                "# define the path to the base output directory\n",
                "BASE_OUTPUT = \"output\"\n",
                "# define the path to the output serialized model, model training\n",
                "# plot, and testing image paths\n",
                "MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_flood_area.pth\")\n",
                "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
                "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "JmU5MKZQ7MJa"
            },
            "source": [
                "# Exercise 1: Dataset class definition\n",
                "\n",
                "In the following cell, we are going to define the class for our segmentation dataset. In this class, we define how we read the images and their masks andwe apply the preprocessing transformation. Use the function `cv2.imread()` [(documentation)](https://www.geeksforgeeks.org/python-opencv-cv2-imread-method/). The mask, has to be read in **gray scale** (use `cv2.imread(_, 0)`).\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# import the necessary packages\n",
                "from torch.utils.data import Dataset\n",
                "import cv2\n",
                "\n",
                "class SegmentationDataset(Dataset):\n",
                "    def __init__(self, imagePaths, maskPaths, transforms):\n",
                "        # store the image and mask filepaths, and augmentation\n",
                "        # transforms\n",
                "        self.imagePaths = imagePaths\n",
                "        self.maskPaths = maskPaths\n",
                "        self.transforms = transforms\n",
                "    def __len__(self):\n",
                "        # return the number of total samples contained in the dataset\n",
                "        return len(self.imagePaths)\n",
                "    def __getitem__(self, idx):\n",
                "        # TODO: grab the image path from the current index\n",
                "        imagePath = ...\n",
                "\n",
                "        # TODO: load the image from disk, swap its channels from BGR to RGB,\n",
                "        # and read the associated mask from disk in grayscale mode\n",
                "        image = ...\n",
                "        image = cv2.cvtColor(image, ...)\n",
                "        mask = ...\n",
                "\n",
                "        # check to see if we are applying any transformations\n",
                "        if self.transforms is not None:\n",
                "            # apply the transformations to both image and its mask\n",
                "            image = self.transforms(image)\n",
                "            mask = self.transforms(mask)\n",
                "        # return a tuple of the image and its mask\n",
                "        return (image, mask)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hAlpl_fR85jr"
            },
            "source": [
                "# Exercise 2: Define the Convolutional Block\n",
                "\n",
                "Now we are going to define the UNet model. First, we define the convolutional block that is going to be applied in each level. This block is composed by:\n",
                "\n",
                "- A convolutional layer with input `inChannels` and output channels `outChannels`, with a 3x3 kernel.\n",
                "- A ReLU activation function.\n",
                "- Another convolutional layer with input `inChannels` and output channels `outChannels`, with a 3x3 kernel.\n",
                "\n",
                "Complete both the layers and the `forward()` step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Block(nn.Module):\n",
                "    def __init__(self, inChannels, outChannels):\n",
                "        super(Block, self).__init__()\n",
                "        # TODO: create the convolution and RELU layers\n",
                "        self.conv1 = ...\n",
                "        self.relu = ...\n",
                "        self.conv2 = ...\n",
                "\n",
                "    def forward(self, x):\n",
                "        # TODO: apply CONV => RELU => CONV block to the inputs and return it\n",
                "        x = ...\n",
                "        x = ...\n",
                "        x = ...\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "uOZ7MGnDIC-E"
            },
            "source": [
                "# Exercise 3: Define the Encoder of the UNet.\n",
                "\n",
                "Now we are going to define the Encoder of the UNet using the Convolutional Block that we have just defined.\n",
                "\n",
                "Define a **MaxPooling Layer** with a 2x2 kernel to be applied after each Convolutional Block. Complete the `forward()` step, taking into account that, in the UNet, we have to store all the outputs of all the blocks to build the **Skip Connections** in the Decoder."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn as nn\n",
                "class Encoder(nn.Module):\n",
                "    def __init__(self, channels=(3, 16, 32, 64)):\n",
                "            super().__init__()\n",
                "            # TODO: create the encoder blocks and maxpooling layer\n",
                "            self.encBlocks = nn.ModuleList([\n",
                "               ...\n",
                "               ...\n",
                "            ])\n",
                "            self.pool = ...\n",
                "\n",
                "    def forward(self, x):\n",
                "        # initialize an empty list to store the intermediate outputs\n",
                "        blockOutputs = []\n",
                "        # TODO: loop through the encoder blocks and update the blockOutputs list\n",
                "        for block in self.encBlocks:\n",
                "            ...\n",
                "            ...\n",
                "            ...\n",
                "\n",
                "        return blockOutputs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OxaSSWgtJWjS"
            },
            "source": [
                "# Exercise 4: Define de Decoder\n",
                "\n",
                "Once we have defined the Encoder, we have to define our Decoder. We are also going to use the convolutional block previously defined, and we are going to build the Skip Connections with the output of the blocks of the encoder. Remeber that, in the UNet, before the convolutional blocks, we use Transposed Convolution to \"unpool\" the features and be able to recover the desired spatial definition.\n",
                "\n",
                "- Define the `ModuleList()` of Transposed Convolutional Layers with `ConvTransposed2d()`, with the specified `channels`, 2x2 kernel and stride = 2. Get inspired by how it's done in the Encoder.\n",
                "- Define the `ModuleList()` of Convolutional Blocks. Get inspired by how it's done in the Encoder.\n",
                "- Complete the `forward()` step, using `torch.cat()` to concatenate the Encoder Features (`encFeat`) to the output of the upsampling operation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.nn import ModuleList\n",
                "from torchvision.transforms import CenterCrop\n",
                "\n",
                "class Decoder(nn.Module):\n",
                "    def __init__(self, channels=(64, 32, 16)):\n",
                "          super().__init__()\n",
                "          # TODO: initialize the number of channels, upsampler blocks, and decoder blocks\n",
                "          self.upconvs = nn.ModuleList([\n",
                "              ...\n",
                "              ...\n",
                "          ])\n",
                "          self.dec_blocks = nn.ModuleList([\n",
                "              ...\n",
                "              ...\n",
                "          ])\n",
                "\n",
                "    def forward(self, x, encFeatures):\n",
                "      # loop through the number of channels\n",
                "        for i in range(len(self.upconvs)):\n",
                "            x = self.upconvs[i](x)\n",
                "            # TODO: crop the current features from the encoder blocks,\n",
                "            # concatenate them with the current upsampled features,\n",
                "            # and pass the concatenated output through the current\n",
                "            # decoder block\n",
                "            encFeat = ...\n",
                "            x = ...\n",
                "            x = ...\n",
                "\n",
                "        # return the final decoder output\n",
                "        return x\n",
                "\n",
                "    def crop(self, encFeatures, x):\n",
                "          # grab the dimensions of the inputs, and crop the encoder\n",
                "          # features to match the dimensions\n",
                "          (_, _, H, W) = x.shape\n",
                "          encFeatures = CenterCrop([H, W])(encFeatures)\n",
                "          # return the cropped features\n",
                "          return encFeatures"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "K1lBeBhxOedj"
            },
            "source": [
                "# Exercise 5: Define the UNet\n",
                "\n",
                "Finally, we are going to combine the Encoder and the Decoder in the final UNet model.\n",
                "\n",
                "- Define the Encoder and the Decoder using the defined modules.\n",
                "- Complete the `forward()` step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class UNet(nn.Module):\n",
                "    def __init__(self, encChannels=(3, 16, 32, 64),\n",
                "          decChannels=(64, 32, 16),\n",
                "          nbClasses=1, retainDim=True,\n",
                "          outSize=(INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)):\n",
                "\n",
                "        super().__init__()\n",
                "        # TODO: initialize the encoder and decoder\n",
                "        self.encoder = ...\n",
                "        self.decoder = ...\n",
                "\n",
                "        # initialize the regression head and store the class variables\n",
                "        self.head = nn.Conv2d(decChannels[-1], nbClasses, kernel_size=1)\n",
                "        self.retainDim = retainDim\n",
                "        self.outSize = outSize\n",
                "\n",
                "    def forward(self, x):\n",
                "        # TODO: grab the features from the encoder\n",
                "        encFeatures = ...\n",
                "\n",
                "        # TODO: pass the encoder features through decoder making sure that\n",
                "        # their dimensions are suited for concatenation\n",
                "        decFeatures = ...\n",
                "\n",
                "        # TODO: pass the decoder features through the regression head to\n",
                "        # obtain the segmentation mask\n",
                "        map = ...\n",
                "\n",
                "        # check to see if we are retaining the original output\n",
                "        # dimensions and if so, then resize the output to match them\n",
                "        if self.retainDim:\n",
                "          map = F.interpolate(map, self.outSize)\n",
                "\n",
                "        # return the segmentation map\n",
                "        return map\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "o_M052l0O4q9"
            },
            "source": [
                "# Exercise 6: Build the datasets.\n",
                "\n",
                "Here, we are defining the datasets that we are going to use to train our model.\n",
                "\n",
                "- First, use the function `train_test_split()` from sci-kit learn ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) to create the train and test partition. The test size is defined in the variable `TEST_SPLIT`. Use `random_state=42`.\n",
                "\n",
                "- Define the train and test datasets (`trainDS` and `testDS`) using the class `SegmentationDataset` defined before."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "imagePaths = sorted(list(paths.list_images(IMAGE_DATASET_PATH)))\n",
                "maskPaths = sorted(list(paths.list_images(MASK_DATASET_PATH)))\n",
                "\n",
                "# TODO: partition the data into training and testing splits using 85% of\n",
                "# the data for training and the remaining 15% for testing\n",
                "# DO NOT CHANGE THE random_state\n",
                "split = ...\n",
                "\n",
                "# unpack the data split\n",
                "(trainImages, testImages) = split[:2]\n",
                "(trainMasks, testMasks) = split[2:]\n",
                "\n",
                "# write the testing image paths to disk so that we can use then\n",
                "# when evaluating/testing our model\n",
                "print(\"[INFO] saving testing image paths...\")\n",
                "f = open(TEST_PATHS, \"w\")\n",
                "f.write(\"\\n\".join(testImages))\n",
                "f.close()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trans = transforms.Compose([transforms.ToPILImage(),\n",
                "        transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
                "        transforms.ToTensor()])\n",
                "\n",
                "# TODO: create the train and test datasets\n",
                "trainDS = ...\n",
                "testDS = ...\n",
                "\n",
                "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
                "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
                "\n",
                "# create the training and test data loaders\n",
                "trainLoader = DataLoader(trainDS, shuffle=True,\n",
                "    batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
                "    num_workers=os.cpu_count())\n",
                "testLoader = DataLoader(testDS, shuffle=False,\n",
                "    batch_size=BATCH_SIZE, pin_memory=PIN_MEMORY,\n",
                "    num_workers=os.cpu_count())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "n3jU_XnyQZLJ"
            },
            "source": [
                "# Exercise 7: Define the train loop\n",
                "\n",
                "- Initialize the model\n",
                "- Define the loss function. We are going to use the BCELoss combined with a sigmoid layer. Use the `BCEWithLogitsLoss()` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html)).\n",
                "- Define the [Adam optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate `INIT_LR`.\n",
                "- Complete the training loop."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: initialize our UNet model\n",
                "unet = ...\n",
                "unet = unet.to(DEVICE)\n",
                "\n",
                "# initialize loss function and optimizer\n",
                "lossFunc = BCEWithLogitsLoss()\n",
                "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
                "\n",
                "# calculate steps per epoch for training and test set\n",
                "trainSteps = len(trainDS) // BATCH_SIZE\n",
                "testSteps = len(testDS) // BATCH_SIZE\n",
                "\n",
                "# initialize a dictionary to store training history\n",
                "H = {\"train_loss\": [], \"test_loss\": []}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# loop over epochs\n",
                "print(\"[INFO] training the network...\")\n",
                "startTime = time.time()\n",
                "for e in tqdm(range(NUM_EPOCHS)):\n",
                "    # set the model in training mode\n",
                "    unet.train()\n",
                "    # initialize the total training and validation loss\n",
                "    totalTrainLoss = 0\n",
                "    totalTestLoss = 0\n",
                "    # loop over the training set\n",
                "    for (i, (x, y)) in enumerate(trainLoader):\n",
                "        # send the input to the device\n",
                "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
                "\n",
                "        # TODO: perform a forward pass and calculate the training loss\n",
                "        pred = ...\n",
                "        loss = ...\n",
                "\n",
                "        # first, zero out any previously accumulated gradients, then\n",
                "        # perform backpropagation, and then update model parameters\n",
                "        opt.zero_grad()\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "\n",
                "        # Add the loss to the total training loss\n",
                "        totalTrainLoss += loss\n",
                "\n",
                "    with torch.no_grad():\n",
                "        # set the model in evaluation mode\n",
                "        unet.eval()\n",
                "        # loop over the validation set\n",
                "        for (x, y) in testLoader:\n",
                "              # send the input to the device\n",
                "              (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
                "              # make the predictions and calculate the validation loss\n",
                "              pred = unet(x)\n",
                "              totalTestLoss += lossFunc(pred, y)\n",
                "        # calculate the average training and validation loss\n",
                "        avgTrainLoss = totalTrainLoss / trainSteps\n",
                "        avgTestLoss = totalTestLoss / testSteps\n",
                "        # update our training history\n",
                "        H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
                "        H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
                "        # print the model training and validation information\n",
                "        print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
                "        print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
                "          avgTrainLoss, avgTestLoss))\n",
                "        # display the total time needed to perform the training\n",
                "        endTime = time.time()\n",
                "        print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
                "        endTime - startTime))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "avHq_PZoSGdR"
            },
            "source": [
                "Here we plot the training loss curves for the training set and the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# plot the training loss\n",
                "plt.style.use(\"ggplot\")\n",
                "plt.figure()\n",
                "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
                "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
                "plt.title(\"Training Loss on Dataset\")\n",
                "plt.xlabel(\"Epoch #\")\n",
                "plt.ylabel(\"Loss\")\n",
                "plt.legend(loc=\"lower left\")\n",
                "plt.savefig(PLOT_PATH)\n",
                "# serialize the model to disk\n",
                "torch.save(unet, MODEL_PATH)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bhiXo5OVSUKu"
            },
            "source": [
                "# Exercise 8: Enjoy your results!\n",
                "\n",
                "Here, we apply the trained model over the test set and we plot the results versus the ground truth mask.\n",
                "\n",
                "You do not have to do anything, just enjoy!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import torch\n",
                "import cv2\n",
                "import os\n",
                "def prepare_plot(origImage, origMask, predMask):\n",
                "    # initialize our figure\n",
                "    figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
                "    # plot the original image, its mask, and the predicted mask\n",
                "    ax[0].imshow(origImage)\n",
                "    ax[1].imshow(origMask)\n",
                "    ax[2].imshow(predMask)\n",
                "    # set the titles of the subplots\n",
                "    ax[0].set_title(\"Image\")\n",
                "    ax[1].set_title(\"Original Mask\")\n",
                "    ax[2].set_title(\"Predicted Mask\")\n",
                "    # set the layout of the figure and display it\n",
                "    figure.tight_layout()\n",
                "    figure.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_predictions(model, imagePath):\n",
                "    # set model to evaluation mode\n",
                "    model.eval()\n",
                "    # turn off gradient tracking\n",
                "    with torch.no_grad():\n",
                "        # load the image from disk, swap its color channels, cast it\n",
                "        # to float data type, and scale its pixel values\n",
                "        image = cv2.imread(imagePath)\n",
                "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "        image = image.astype(\"float32\") / 255.0\n",
                "        # resize the image and make a copy of it for visualization\n",
                "        image = cv2.resize(image, (800, 600))\n",
                "        orig = image.copy()\n",
                "        # find the filename and generate the path to ground truth\n",
                "        # mask\n",
                "        filename = imagePath.split(os.path.sep)[-1]\n",
                "        filename_mask = filename.split(\".jpg\")[0] + '.png'\n",
                "        groundTruthPath = os.path.join(MASK_DATASET_PATH,\n",
                "          filename_mask)\n",
                "        # load the ground-truth segmentation mask in grayscale mode\n",
                "        # and resize it\n",
                "        gtMask = cv2.imread(groundTruthPath, 0)\n",
                "        gtMask = cv2.resize(gtMask, (INPUT_IMAGE_WIDTH,\n",
                "           INPUT_IMAGE_HEIGHT))\n",
                "\n",
                "        # make the channel axis to be the leading one, add a batch\n",
                "        # dimension, create a PyTorch tensor, and flash it to the\n",
                "        # current device\n",
                "        image = np.transpose(image, (2, 0, 1))\n",
                "        image = np.expand_dims(image, 0)\n",
                "        image = torch.from_numpy(image).to(DEVICE)\n",
                "        # make the prediction, pass the results through the sigmoid\n",
                "        # function, and convert the result to a NumPy array\n",
                "        predMask = model(image).squeeze()\n",
                "        predMask = torch.sigmoid(predMask)\n",
                "        predMask = predMask.cpu().numpy()\n",
                "        # filter out the weak predictions and convert them to integers\n",
                "        predMask = (predMask > THRESHOLD) * 255\n",
                "        predMask = predMask.astype(np.uint8)\n",
                "        # prepare a plot for visualization\n",
                "        prepare_plot(orig, gtMask, predMask)\n",
                "\n",
                "\n",
                "# load the image paths in our testing file and randomly select 10\n",
                "# image paths\n",
                "print(\"[INFO] loading up test image paths...\")\n",
                "imagePaths = open(TEST_PATHS).read().strip().split(\"\\n\")\n",
                "imagePaths = np.random.choice(imagePaths, size=10)\n",
                "# load our model from disk and flash it to the current device\n",
                "print(\"[INFO] load up model...\")\n",
                "unet = torch.load(MODEL_PATH).to(DEVICE)\n",
                "# iterate over the randomly selected test image paths\n",
                "for path in imagePaths:\n",
                "    # make predictions and visualize the results\n",
                "    make_predictions(unet, path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9lDUJImkSw7b"
            },
            "source": [
                "# EXCERCISE 9: Evaluate your results\n",
                "\n",
                "Now it's time to evaluate your results! We are going to compute the Intersection over Union (IoU) and the Dice Index.\n",
                "\n",
                "## Intersection over Union (IoU)\n",
                "\n",
                "The IoU metric, also known as the Jaccard index, measures the overlap between the predicted segmentation and the ground truth segmentation. It's computed as:\n",
                "\n",
                "$$\\text{IoU} = \\frac{|\\text{Prediction} \\cap \\text{Ground Truth}|}{|\\text{Prediction} \\cup \\text{Ground Truth}|}$$\n",
                "\n",
                "## Dice Index\n",
                "\n",
                "The Dice coefficient, or Dice similarity coefficient (DSC), is similar to IoU but gives a slightly different perspective:\n",
                "\n",
                "$$\\text{Dice} = \\frac{2 |\\text{Prediction} \\cap \\text{Ground Truth}|}{|\\text{Prediction}| + |\\text{Ground Truth}|}$$\n",
                "\n",
                "Here you have the code to compute them and an example of it's usage. Write your own code to compute this metrics for your train dataset and your test dataset. Try to modify the functions `compute_iou` and `compute_dice` as little as possible.\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions for computing IoU and Dice scores\n",
                "def compute_iou(pred, target, threshold=0.5, smooth=1e-6):\n",
                "    \"\"\"\n",
                "    Compute the Intersection over Union (IoU) score.\n",
                "\n",
                "    Args:\n",
                "    pred (torch.Tensor): Predicted segmentation (probabilities or logits).\n",
                "    target (torch.Tensor): Ground truth segmentation (binary).\n",
                "    threshold (float): Threshold to binarize the predictions.\n",
                "    smooth (float): Small value to avoid division by zero.\n",
                "\n",
                "    Returns:\n",
                "    float: IoU score.\n",
                "    \"\"\"\n",
                "    pred = (pred > threshold).float()\n",
                "    intersection = (pred * target).sum((1, 2))\n",
                "    union = pred.sum((1, 2)) + target.sum((1, 2)) - intersection\n",
                "    iou = (intersection + smooth) / (union + smooth)\n",
                "\n",
                "    return iou.mean().item()\n",
                "\n",
                "def compute_dice(pred, target, threshold=0.5, smooth=1e-6):\n",
                "    \"\"\"\n",
                "    Compute the Dice coefficient.\n",
                "\n",
                "    Args:\n",
                "    pred (torch.Tensor): Predicted segmentation (probabilities or logits).\n",
                "    target (torch.Tensor): Ground truth segmentation (binary).\n",
                "    threshold (float): Threshold to binarize the predictions.\n",
                "    smooth (float): Small value to avoid division by zero.\n",
                "\n",
                "    Returns:\n",
                "    float: Dice coefficient.\n",
                "    \"\"\"\n",
                "    pred = (pred > threshold).float()\n",
                "    intersection = (pred * target).sum((1, 2))\n",
                "    dice = (2 * intersection + smooth) / (pred.sum((1, 2)) + target.sum((1, 2)) + smooth)\n",
                "\n",
                "    return dice.mean().item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dummy tensors for demonstration\n",
                "predictions = torch.randn(4, 256, 256)  # Example prediction tensor\n",
                "ground_truths = torch.randint(0, 2, (4, 256, 256)).float()  # Example ground truth tensor\n",
                "\n",
                "iou_score = compute_iou(predictions, ground_truths)\n",
                "dice_score = compute_dice(predictions, ground_truths)\n",
                "\n",
                "print(f\"IoU Score: {iou_score}\")\n",
                "print(f\"Dice Score: {dice_score}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the image paths\n",
                "imagePaths = open(TEST_PATHS).read().strip().split(\"\\n\")\n",
                "\n",
                "# Load the model\n",
                "model = torch.load(MODEL_PATH, map_location=DEVICE)\n",
                "model.eval()\n",
                "\n",
                "# Turn off gradient tracking\n",
                "all_pred_masks = []\n",
                "all_gt_masks = []\n",
                "\n",
                "with torch.no_grad():\n",
                "    for imagePath in imagePaths:\n",
                "        # Load the image from disk, swap its color channels, cast it to float data type, and scale its pixel values\n",
                "        image = cv2.imread(imagePath)\n",
                "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "        image = image.astype(\"float32\") / 255.0\n",
                "\n",
                "        # Resize the image\n",
                "        image = cv2.resize(image, (INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT))\n",
                "\n",
                "        # Find the filename and generate the path to ground truth mask\n",
                "        filename = os.path.basename(imagePath)\n",
                "        filename_mask = filename.split(\".\")[0] + '.png'\n",
                "        groundTruthPath = os.path.join(MASK_DATASET_PATH, filename_mask)\n",
                "\n",
                "        # Load the ground-truth segmentation mask in grayscale mode and resize it\n",
                "        gtMask = cv2.imread(groundTruthPath, cv2.IMREAD_GRAYSCALE)\n",
                "        gtMask = cv2.resize(gtMask, (INPUT_IMAGE_WIDTH, INPUT_IMAGE_HEIGHT))\n",
                "\n",
                "        # Preprocess image for model input\n",
                "        image = np.transpose(image, (2, 0, 1))\n",
                "        image = np.expand_dims(image, 0)\n",
                "        image = torch.from_numpy(image).to(DEVICE)\n",
                "\n",
                "        # Make the prediction, pass the results through the sigmoid function, and convert the result to a NumPy array\n",
                "        predMask = model(image).squeeze()\n",
                "        predMask = torch.sigmoid(predMask)\n",
                "        predMask = predMask.cpu().numpy()\n",
                "\n",
                "        # Filter out the weak predictions and convert them to integers\n",
                "        predMask = (predMask > THRESHOLD).astype(np.uint8) * 255\n",
                "\n",
                "        # Append the masks to the lists\n",
                "        all_pred_masks.append(predMask)\n",
                "        all_gt_masks.append(gtMask)\n",
                "\n",
                "# Convert lists to tensors\n",
                "all_pred_masks = torch.tensor(all_pred_masks).float() / 255.0\n",
                "all_gt_masks = torch.tensor(all_gt_masks).float() / 255.0\n",
                "\n",
                "# Compute overall IoU and Dice scores\n",
                "iou_score = compute_iou(all_pred_masks, all_gt_masks)\n",
                "dice_score = compute_dice(all_pred_masks, all_gt_masks)\n",
                "\n",
                "print(f\"Overall IoU Score: {iou_score:.4f}\")\n",
                "print(f\"Overall Dice Score: {dice_score:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
